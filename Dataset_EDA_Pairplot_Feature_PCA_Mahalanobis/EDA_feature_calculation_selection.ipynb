{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis (EDA): feature engineering and feature selection\n",
    "\n",
    "**Author:** Y.X. Wu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.FeatureCalculator import FeatureCalculator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# display the current working directory\n",
    "display(\"Current working directory: {0}\".format(os.getcwd()))\n",
    "\n",
    "data_path = '../Dataset_Cleaned/'\n",
    "display(os.path.isfile(data_path+'LiteratureDataset_Corrosion_YW_v3.xlsx'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "### Feature Calculation for Alloy Components\n",
    "\n",
    "Prepares and processes data about alloy compositions, specifically it creates a `FeatureCalculator` object from the defined compositions, then calculates and prints the corresponding alloy features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of component elements and their corresponding fractions\n",
    "compo_elem = [\"Ni\", \"Cr\", \"Mo\", \"Ti\", \"Fe\"]\n",
    "ele_frac = np.array([43.8, 38.3, 2.44, 1.04, 0])\n",
    "\n",
    "# Create a dictionary mapping each element to its corresponding fraction,\n",
    "ele_frac_dict = {elem: frac for elem, frac in zip(compo_elem, ele_frac)}\n",
    "\n",
    "# Prepare data in the format required for FeatureCalculator - a list of tuples,\n",
    "# where each tuple contains a list of elements and their corresponding fractions\n",
    "compositions = [(list(ele_frac_dict.keys()), list(ele_frac_dict.values()))]\n",
    "\n",
    "print(compositions)\n",
    "\n",
    "# Create a FeatureCalculator object with the prepared compositions\n",
    "calculator = FeatureCalculator(compositions)\n",
    "\n",
    "# Calculate the features using the FeatureCalculator object\n",
    "features = calculator.calculate_features()\n",
    "\n",
    "\n",
    "feature_names = [\"a\", \"delta_a\", \"Tm\", \"sigma_Tm\", \"Hmix\", \"sigma_Hmix\", \"ideal_S\",\n",
    "                 \"elec_nega\", \"sigma_elec_nega\", \"VEC\", \"sigma_VEC\", \"bulk_modulus\", \"sigma_bulk_modulus\"]\n",
    "# tabulate the features the feature under feature_names\n",
    "df = pd.DataFrame(features, columns=feature_names)\n",
    "display(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading, Feature Calculation, and Extraction\n",
    "\n",
    "Reads various datasets from Excel files, calculates specific features for each material composition in these datasets using a custom `FeatureCalculator` class, extracts relevant data from the corrosion and hardness datasets, and displays the first rows of the extracted data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils.FeatureCalculator import FeatureCalculator\n",
    "\n",
    "# Initialization of the constants and the data to be loaded\n",
    "feature_names = [\"a\", \"delta_a\", \"Tm\", \"sigma_Tm\", \"Hmix\", \"sigma_Hmix\", \"ideal_S\",\n",
    "                 \"elec_nega\", \"sigma_elec_nega\", \"VEC\", \"sigma_VEC\", \"bulk_modulus\", \"sigma_bulk_modulus\"]\n",
    "\n",
    "# Lists of filenames, elements for each file, and the header rows for the data in each file\n",
    "data_file_names = [\"LiteratureDataset_Corrosion_YW_v3.xlsx\",\n",
    "                   \"LiteratureDataset_Hardness_YW_v3.xlsx\",\n",
    "                   \"MultiTaskModel_NiCrCoVFe_KW99_at_pct.xlsx\",\n",
    "                   \"MultiTaskModel_NiCrCoVFe_KW99_wt_pct.xlsx\",\n",
    "                   \"MultiTaskModel_NiCrMoTiFe_KW131_at_pct.xlsx\",\n",
    "                   \"MultiTaskModel_NiCrMoTiFe_KW131_wt_pct.xlsx\"]\n",
    "element_columns = [['Fe', 'Cr', 'Ni', 'Mo', 'W', 'N', 'Nb', 'C', 'Si', 'Mn',\n",
    "                    'Cu', 'Al', 'V', 'Ta', 'Ti', 'Co', 'Mg', 'Y'],\n",
    "                   ['Fe', 'Cr', 'Ni', 'Mo', 'W', 'N', 'Nb', 'C', 'Si', 'Mn',\n",
    "                    'Cu', 'Al', 'V', 'Ta', 'Ti', 'Co', 'Mg', 'Y', 'Zr', 'Hf'],\n",
    "                   ['Ni', 'Cr', 'Co', 'V', 'Fe'],\n",
    "                   ['Ni', 'Cr', 'Co', 'V', 'Fe'],\n",
    "                   ['Ni', 'Cr', 'Mo', 'Ti', 'Fe'],\n",
    "                   ['Ni', 'Cr', 'Mo', 'Ti', 'Fe']]\n",
    "\n",
    "df_header_list = [2, 2, 0, 0, 0, 0, 0, 0]\n",
    "df_compo = pd.DataFrame(columns=['Fe', 'Cr', 'Ni', 'Mo', 'W', 'N', 'Nb', 'C', 'Si', 'Mn',\n",
    "                                 'Cu', 'Al', 'V', 'Ta', 'Ti', 'Co', 'Mg', 'Y', 'Zr', 'Hf'])\n",
    "features_dfs = []\n",
    "\n",
    "# Processing each data file along with the corresponding elements\n",
    "for i in range(len(data_file_names)):\n",
    "    # Load and preprocess data from each excel file\n",
    "    data_df = pd.read_excel(\n",
    "        data_path + data_file_names[i], header=df_header_list[i])\n",
    "    # print(data_df.columns)\n",
    "    element_fractions = data_df[element_columns[i]].fillna(0)\n",
    "\n",
    "    # Calculate features for each composition\n",
    "    compositions = [(element_columns[i], element_fraction)\n",
    "                    for element_fraction in element_fractions.values]\n",
    "    feature_calculator = FeatureCalculator(compositions)\n",
    "    calculated_features = feature_calculator.calculate_features()\n",
    "\n",
    "    # Store the calculated features in a DataFrame\n",
    "    features_df = pd.DataFrame(calculated_features, columns=feature_names)\n",
    "    features_dfs.append(features_df)\n",
    "\n",
    "    # Extract and store specific data and features for the corrosion and hardness datasets\n",
    "    if i == 0:  # Corrosion dataset\n",
    "        df_C_compo, df_C_specific_testing, df_C_specific_features, df_C_output = element_fractions, data_df[[\n",
    "            'TestTemperature_C', 'ChlorideIonConcentration', 'pH', 'ScanRate_mVs']], features_df, data_df[['AvgPittingPotential_mV']]\n",
    "\n",
    "        # now I want to make df_C_compo have the same columns as df_compo\n",
    "        df_C_compo = pd.concat([df_C_compo, df_compo],\n",
    "                               axis=0, ignore_index=True).fillna(0)\n",
    "        display(df_C_compo.head(1))\n",
    "\n",
    "    if i == 1:  # Hardness dataset\n",
    "        df_H_compo, df_H_specific_features, df_H_output = element_fractions, features_df, data_df[[\n",
    "            'converted HV']]\n",
    "        df_H_compo = pd.concat([df_H_compo, df_compo],\n",
    "                               axis=0, ignore_index=True).fillna(0)\n",
    "        display(df_H_compo.head(1))\n",
    "\n",
    "\n",
    "# Display the first row of the specific data, features, and output for the corrosion and hardness datasets\n",
    "display(df_C_compo.head(1), df_C_specific_testing.head(1),\n",
    "        df_C_specific_features.head(1), df_C_output.head(1))\n",
    "display(df_H_compo.head(1), df_H_specific_features.head(1), df_H_output.head(1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Correlation Matrix and Correlation with the Target Variable\n",
    "\n",
    "Generates two visuals: a heatmap showing the correlation between all features in the data set, and a bar chart indicating the correlation of each feature with the target variable, 'AvgPittingPotential_mV'. These visuals help in identifying the relationships between different features, and how each one impacts the target variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Concatenate DataFrames\n",
    "df_C = pd.concat([df_C_compo, df_C_specific_testing,\n",
    "                 df_C_specific_features, df_C_output], axis=1)\n",
    "display(df_C.head(1))\n",
    "\n",
    "# Compute absolute correlation matrix\n",
    "corr_matrix = df_C.corr().abs()\n",
    "\n",
    "# Remove NaN columns and rows\n",
    "corr_matrix = corr_matrix.loc[:, ~corr_matrix.isna().all(axis=0)]\n",
    "corr_matrix = corr_matrix.loc[~corr_matrix.isna().all(axis=1), :]\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(15, 15), dpi=150)\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".1f\", cmap='RdGy_r',\n",
    "            cbar_kws={'label': 'Correlation Coefficient'}, annot_kws={\"size\": 8})\n",
    "plt.title(\"Correlation matrix of the features\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute and plot correlations with the target variable\n",
    "plt.figure(figsize=(15, 5), dpi=150)\n",
    "df_C.drop('AvgPittingPotential_mV', axis=1).apply(lambda x: x.corr(\n",
    "    df_C['AvgPittingPotential_mV'])).abs().plot(kind='bar')\n",
    "plt.title(\"Correlation of the features with the AvgPittingPotential_mV\")\n",
    "plt.ylabel(\"Correlation\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection\n",
    "\n",
    "### Data Normalization\n",
    "\n",
    "performs MinMax scaling on multiple datasets to prepare them as inputs for a Neural Network, concatenates certain scaled datasets for model training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Prepare data for NN\n",
    "dfs = [df_H_compo, df_H_specific_features, df_H_output,\n",
    "       df_C_compo, df_C_specific_testing, df_C_specific_features, df_C_output]\n",
    "\n",
    "# Convert DataFrames to numpy arrays\n",
    "inputs_outputs = [np.asarray(df.values) for df in dfs]\n",
    "\n",
    "# Define each variable\n",
    "X1, Y1, H1, X2, Z2, W2, C2 = inputs_outputs\n",
    "\n",
    "# Initialize MinMaxScalers for each data set\n",
    "scalers = {\n",
    "    \"compo\": MinMaxScaler(),\n",
    "    \"H_specific_features\": MinMaxScaler(),\n",
    "    \"H_output\": MinMaxScaler(),\n",
    "    \"C_specific_testing\": MinMaxScaler(),\n",
    "    \"C_specific_features\": MinMaxScaler(),\n",
    "    \"C_output\": MinMaxScaler()\n",
    "}\n",
    "\n",
    "# Fit scalers to appropriate data\n",
    "scalers[\"compo\"].fit(np.concatenate((X1, X2)))\n",
    "scalers[\"H_specific_features\"].fit(Y1)\n",
    "scalers[\"H_output\"].fit(H1.reshape((-1, 1)))\n",
    "scalers[\"C_specific_testing\"].fit(Z2)\n",
    "scalers[\"C_specific_features\"].fit(W2)\n",
    "scalers[\"C_output\"].fit(C2.reshape((-1, 1)))\n",
    "\n",
    "# Apply transformations\n",
    "X1_norm = scalers[\"compo\"].transform(X1)\n",
    "Y1_norm = scalers[\"H_specific_features\"].transform(Y1)\n",
    "H1_norm = scalers[\"H_output\"].transform(H1.reshape((-1, 1)))\n",
    "X2_norm = scalers[\"compo\"].transform(X2)\n",
    "Z2_norm = scalers[\"C_specific_testing\"].transform(Z2)\n",
    "W2_norm = scalers[\"C_specific_features\"].transform(W2)\n",
    "C2_norm = scalers[\"C_output\"].transform(C2.reshape((-1, 1)))\n",
    "\n",
    "# Prepare final input data for model training\n",
    "X_H_norm = np.concatenate((X1_norm, Y1_norm), axis=1)\n",
    "X_C_norm = np.concatenate((X2_norm, Z2_norm, W2_norm), axis=1)\n",
    "\n",
    "# Plot distribution of target variables\n",
    "plt.figure(figsize=(15, 5), dpi=150)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(W2_norm[:, 2], bins=50)  # Distribution of one of the features\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(C2_norm, bins=50)  # Distribution of target variable\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and Evaluating Random Forest Regression Models\n",
    "\n",
    "Builds two Random Forest Regression models for different datasets, fits the models to the data, and then evaluates their performance by printing the R^2 score.\n",
    "\n",
    "The goal is to train the Random Forest Regression Models with well-tuned hyperparameters before feature selection, so that the feature selection process can be more accurate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Create a Random Forest model\n",
    "forestmodel_H = RandomForestRegressor(random_state=0,\n",
    "                                      n_estimators=300,\n",
    "                                      max_features=20,\n",
    "                                      max_depth=10,\n",
    "                                      min_samples_split=2,\n",
    "                                      min_samples_leaf=4,\n",
    "                                      bootstrap=True)\n",
    "# print(forestmodel_H)\n",
    "\n",
    "forestmodel_H.fit(X_H_norm, H1_norm.ravel())\n",
    "\n",
    "# Perform cross-validation on model H\n",
    "cv_scores_H = cross_val_score(forestmodel_H, X_H_norm, H1_norm.ravel(), cv=6)\n",
    "print(\"CV scores for H:\", cv_scores_H)\n",
    "print(\"Mean CV score for H:\", np.mean(cv_scores_H))\n",
    "\n",
    "forestmodel_C = RandomForestRegressor(random_state=2,\n",
    "                                      n_estimators=300,\n",
    "                                      max_features=20,\n",
    "                                      max_depth=10,\n",
    "                                      min_samples_split=2,\n",
    "                                      min_samples_leaf=4,\n",
    "                                      bootstrap=True)\n",
    "forestmodel_C.fit(X_C_norm, C2_norm.ravel())\n",
    "\n",
    "# Perform cross-validation on model C\n",
    "cv_scores_C = cross_val_score(forestmodel_C, X_C_norm, C2_norm.ravel(), cv=6)\n",
    "print(\"CV scores for C:\", cv_scores_C)\n",
    "print(\"Mean CV score for C:\", np.mean(cv_scores_C))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Hyperparameter Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39m# Perform randomized search for model H\u001b[39;00m\n\u001b[1;32m     17\u001b[0m random_search_H \u001b[39m=\u001b[39m RandomizedSearchCV(estimator\u001b[39m=\u001b[39mforestmodel_H, param_distributions\u001b[39m=\u001b[39mrf_params,\n\u001b[1;32m     18\u001b[0m                                      n_iter\u001b[39m=\u001b[39m\u001b[39m200\u001b[39m, cv\u001b[39m=\u001b[39m\u001b[39m6\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m random_search_H\u001b[39m.\u001b[39;49mfit(X_H_norm, H1_norm\u001b[39m.\u001b[39;49mravel())\n\u001b[1;32m     21\u001b[0m \u001b[39m# Access the best parameters and best score for model H\u001b[39;00m\n\u001b[1;32m     22\u001b[0m best_params_H \u001b[39m=\u001b[39m random_search_H\u001b[39m.\u001b[39mbest_params_\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-env/lib/python3.9/site-packages/sklearn/model_selection/_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    868\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[1;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    870\u001b[0m     )\n\u001b[1;32m    872\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m--> 874\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[1;32m    876\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    878\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-env/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1768\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1766\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1767\u001b[0m     \u001b[39m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1768\u001b[0m     evaluate_candidates(\n\u001b[1;32m   1769\u001b[0m         ParameterSampler(\n\u001b[1;32m   1770\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_distributions, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_iter, random_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state\n\u001b[1;32m   1771\u001b[0m         )\n\u001b[1;32m   1772\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-env/lib/python3.9/site-packages/sklearn/model_selection/_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    814\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m    815\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[1;32m    818\u001b[0m         )\n\u001b[1;32m    819\u001b[0m     )\n\u001b[0;32m--> 821\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    822\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m         clone(base_estimator),\n\u001b[1;32m    824\u001b[0m         X,\n\u001b[1;32m    825\u001b[0m         y,\n\u001b[1;32m    826\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[1;32m    827\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[1;32m    828\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[1;32m    829\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[1;32m    830\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[1;32m    831\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[1;32m    832\u001b[0m     )\n\u001b[1;32m    833\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[1;32m    834\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[1;32m    835\u001b[0m     )\n\u001b[1;32m    836\u001b[0m )\n\u001b[1;32m    838\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    839\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-env/lib/python3.9/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-env/lib/python3.9/site-packages/joblib/parallel.py:1061\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1058\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1061\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[1;32m   1062\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-env/lib/python3.9/site-packages/joblib/parallel.py:938\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    936\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    937\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 938\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[1;32m    939\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    940\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-env/lib/python3.9/site-packages/joblib/_parallel_backends.py:542\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[39mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 542\u001b[0m     \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    543\u001b[0m \u001b[39mexcept\u001b[39;00m CfTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    544\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-env/lib/python3.9/concurrent/futures/_base.py:441\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m    439\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[0;32m--> 441\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_condition\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    443\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    444\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-env/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    313\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "rf_params = {\n",
    "    'n_estimators': [50, 100, 200, 300, 400, 500],\n",
    "    'max_features': [1, 'sqrt'],\n",
    "    'max_depth': [10, 20, 30, 40, 50, 60, 70],\n",
    "    'min_samples_split': [2, 5, 10, 15, 20],\n",
    "    'min_samples_leaf': [1, 2, 3, 4, 5]\n",
    "}\n",
    "\n",
    "# Create a Random Forest model\n",
    "forestmodel_H = RandomForestRegressor(random_state=1)\n",
    "\n",
    "# Perform randomized search for model H\n",
    "random_search_H = RandomizedSearchCV(estimator=forestmodel_H, param_distributions=rf_params,\n",
    "                                     n_iter=200, cv=6, verbose=0, random_state=2, n_jobs=-1)\n",
    "random_search_H.fit(X_H_norm, H1_norm.ravel())\n",
    "\n",
    "# Access the best parameters and best score for model H\n",
    "best_params_H = random_search_H.best_params_\n",
    "best_score_H = random_search_H.best_score_\n",
    "\n",
    "# Repeat for model C\n",
    "forestmodel_C = RandomForestRegressor(random_state=3)\n",
    "random_search_C = RandomizedSearchCV(estimator=forestmodel_C, param_distributions=rf_params,\n",
    "                                     n_iter=200, cv=6, verbose=0, random_state=4, n_jobs=-1)\n",
    "random_search_C.fit(X_C_norm, C2_norm.ravel())\n",
    "\n",
    "# Access the best parameters and best score for model C\n",
    "best_params_C = random_search_C.best_params_\n",
    "best_score_C = random_search_C.best_score_\n",
    "\n",
    "# Perform cross-validation for model H\n",
    "cv_scores_H = cross_val_score(\n",
    "    random_search_H.best_estimator_, X_H_norm, H1_norm.ravel(), cv=10)\n",
    "mean_cv_score_H = np.mean(cv_scores_H)\n",
    "\n",
    "# Perform cross-validation for model C\n",
    "cv_scores_C = cross_val_score(\n",
    "    random_search_C.best_estimator_, X_C_norm, C2_norm.ravel(), cv=10)\n",
    "mean_cv_score_C = np.mean(cv_scores_C)\n",
    "\n",
    "# Print the cross-validation results and best scores\n",
    "print(\"Mean CV score for H:\", mean_cv_score_H)\n",
    "print(\"Mean CV score for C:\", mean_cv_score_C)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best parameters for model H:\", best_params_H)\n",
    "print(\"Best parameters for model C:\", best_params_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Refit model H with best parameters\n",
    "best_params_H = rf_random_H.best_params_\n",
    "forestmodel_H = RandomForestRegressor(random_state=0, **best_params_H)\n",
    "forestmodel_H.fit(X_H_norm, H1_norm.ravel())\n",
    "\n",
    "# Perform cross-validation on model H\n",
    "cv_scores_H = cross_val_score(forestmodel_H, X_H_norm, H1_norm.ravel(), cv=6)\n",
    "print(\"CV scores for H:\", cv_scores_H)\n",
    "print(\"Mean CV score for H:\", np.mean(cv_scores_H))\n",
    "\n",
    "# Refit model C with best parameters\n",
    "best_params_C = rf_random_C.best_params_\n",
    "forestmodel_C = RandomForestRegressor(random_state=0, **best_params_C)\n",
    "forestmodel_C.fit(X_C_norm, C2_norm.ravel())\n",
    "\n",
    "# Perform cross-validation on model C\n",
    "cv_scores_C = cross_val_score(forestmodel_C, X_C_norm, C2_norm.ravel(), cv=6)\n",
    "print(\"CV scores for C:\", cv_scores_C)\n",
    "print(\"Mean CV score for C:\", np.mean(cv_scores_C))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation feature importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_feature_importance(model, X, y):\n",
    "    result = permutation_importance(model, X, y, n_repeats=10, random_state=42)\n",
    "    return result.importances_mean, result.importances_std\n",
    "\n",
    "\n",
    "# Calculate feature importances and their standard deviations\n",
    "importances_H, std_H = evaluate_feature_importance(\n",
    "    forestmodel_H, X_H_norm, H1_norm.ravel())\n",
    "importances_C, std_C = evaluate_feature_importance(\n",
    "    forestmodel_C, X_C_norm, C2_norm.ravel())\n",
    "\n",
    "feature_names_H = df_H_compo.columns.append(\n",
    "    df_H_specific_features.columns).tolist()\n",
    "feature_names_C = df_C_compo.columns.append(\n",
    "    df_C_specific_testing.columns).append(df_C_specific_features.columns).tolist()\n",
    "\n",
    "# now create a dataframe of feature importance and std with the column names of feature names\n",
    "# Create a dataframe for C\n",
    "df_feature_importance_C = pd.DataFrame(\n",
    "    importances_C.reshape(1, -1), columns=feature_names_C)\n",
    "df_feature_importance_C = pd.concat([df_feature_importance_C, pd.DataFrame(\n",
    "    std_C.reshape(1, -1), columns=feature_names_C)], axis=0)\n",
    "df_feature_importance_C.index = ['importance', 'std_dev']\n",
    "display(df_feature_importance_C)\n",
    "\n",
    "# Create a dataframe for H\n",
    "df_feature_importance_H = pd.DataFrame(\n",
    "    importances_H.reshape(1, -1), columns=feature_names_H)\n",
    "df_feature_importance_H = pd.concat([df_feature_importance_H, pd.DataFrame(\n",
    "    std_H.reshape(1, -1), columns=feature_names_H)], axis=0)\n",
    "df_feature_importance_H.index = ['importance', 'std_dev']\n",
    "display(df_feature_importance_H)\n",
    "\n",
    "# match the feature importance with the feature names\n",
    "df_feature_importance_H_full = pd.DataFrame(columns=feature_names_C)\n",
    "df_feature_importance_H_full = pd.concat(\n",
    "    [df_feature_importance_H_full, df_feature_importance_H], axis=0)\n",
    "df_feature_importance_H_full.index = df_feature_importance_H.index\n",
    "display(df_feature_importance_H_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "# Define bar width\n",
    "bar_width = 0.35\n",
    "index = np.arange(len(df_feature_importance_C.columns))\n",
    "\n",
    "bar1 = ax.barh(index - bar_width / 2,\n",
    "               df_feature_importance_C.loc['importance', :],\n",
    "               bar_width,\n",
    "               xerr=df_feature_importance_C.loc['std_dev', :],\n",
    "               label='hardness dataset')\n",
    "bar2 = ax.barh(index + bar_width / 2,\n",
    "               df_feature_importance_H_full.loc['importance', :],\n",
    "               bar_width,\n",
    "               xerr=df_feature_importance_H_full.loc['std_dev', :],\n",
    "               label='corrosion dataset')\n",
    "ax.set_xlabel('Importance')\n",
    "ax.set_ylabel('Features')\n",
    "ax.set_title('Feature Importance for H and C')\n",
    "ax.set_yticks(index)\n",
    "ax.set_yticklabels(df_feature_importance_C.columns)\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimination (RFE) - a customised version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import RFE\n",
    "\n",
    "# # Let's say these are your full features and you have a list defining the group of features\n",
    "# # you're interested in, like this:\n",
    "\n",
    "# all_features_C = np.array(df_C_compo.columns.to_list(\n",
    "# ) + df_C_specific_testing.columns.to_list() + df_C_specific_features.columns.to_list())  # all features\n",
    "# # the specific group of features\n",
    "# group_features_C = np.array(df_C_specific_features.columns.to_list())\n",
    "\n",
    "# # Get the indices of the group features in the full feature list\n",
    "# group_indices_C = np.where(np.isin(all_features_C, group_features_C))[0]\n",
    "\n",
    "# # Now extract the subset of X corresponding to group features\n",
    "# X_C_norm_subset = X_C_norm[:, group_indices_C]\n",
    "\n",
    "# # Now you can run RFE or any other feature selection method on this subset\n",
    "# rfe_C = RFE(estimator=forestmodel_C, n_features_to_select=5)\n",
    "# rfe_C = rfe_C.fit(X_C_norm_subset, C2_norm.ravel())\n",
    "# X_C_norm_subset_rfe = rfe_C.transform(X_C_norm_subset)\n",
    "\n",
    "# # Get a mask, or integer index, of the features selected\n",
    "# selected_features_C = rfe_C.support_\n",
    "\n",
    "# # Get a list of the feature names selected\n",
    "# selected_feature_names = group_features_C[selected_features_C]\n",
    "# print(selected_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.feature_selection import RFE\n",
    "# import numpy as np\n",
    "\n",
    "# # Assuming X_train is your feature matrix and y_train are your labels\n",
    "# # always_include is a list of indices for the features you always want to include\n",
    "\n",
    "# # replace these with the indices of your features\n",
    "# always_include_indices = [0, 2, 5]\n",
    "# include_X_train = X_train[:, always_include_indices]\n",
    "\n",
    "# # Remaining features for RFE\n",
    "# rfe_indices = [idx for idx in range(\n",
    "#     X_train.shape[1]) if idx not in always_include_indices]\n",
    "# rfe_X_train = X_train[:, rfe_indices]\n",
    "\n",
    "# # Set up a classifier to use with RFE\n",
    "# clf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# # Perform RFE on remaining features\n",
    "# selector = RFE(clf, n_features_to_select=10, step=1)  # choose your parameters\n",
    "# selector = selector.fit(rfe_X_train, y_train)\n",
    "\n",
    "# # Now, we join the always included features with the selected features from RFE\n",
    "# mask = selector.support_\n",
    "# selected_rfe_indices = np.array(rfe_indices)[mask]\n",
    "# selected_indices = np.concatenate(\n",
    "#     [always_include_indices, selected_rfe_indices])\n",
    "\n",
    "# # Now you can fit your final model on the selected features\n",
    "# final_X_train = X_train[:, selected_indices]\n",
    "# final_clf = RandomForestClassifier(n_estimators=100)\n",
    "# final_clf.fit(final_X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first let's use an example to understand how the RFE works\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate a dummy dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=6,\n",
    "                       noise=0.1, random_state=42)\n",
    "df = pd.DataFrame(X, columns=['A', 'B', 'C', 'D', 'E', 'F'])\n",
    "df['target'] = y\n",
    "\n",
    "# Define the fixed features and the features to be eliminated\n",
    "fixed_features = ['A', 'B']\n",
    "elimination_features = ['C', 'D', 'E', 'F']\n",
    "\n",
    "# Define a pipeline for the elimination features\n",
    "elimination_pipeline = Pipeline(steps=[\n",
    "    ('scale', StandardScaler()),\n",
    "    ('rfecv', RFECV(estimator=LinearRegression(), cv=5, scoring='r2'))\n",
    "])\n",
    "\n",
    "# Define a preprocessor that applies the elimination pipeline to the elimination features,\n",
    "# and applies scaling to the fixed features\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('elim', elimination_pipeline, elimination_features),\n",
    "    ('fix', StandardScaler(), fixed_features)\n",
    "])\n",
    "\n",
    "# Define the final pipeline that includes preprocessing and model training\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('pre', preprocessor),\n",
    "    ('reg', LinearRegression())\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the all data\n",
    "X_all = df[fixed_features + elimination_features]\n",
    "y_all = df['target']\n",
    "\n",
    "pipeline.fit(X_all, y_all)\n",
    "\n",
    "# Access the selected features\n",
    "selected_features_mask = pipeline.named_steps['pre'].transformers_[\n",
    "    0][1].named_steps['rfecv'].support_\n",
    "print(\n",
    "    f'Selected features: {np.array(elimination_features)[selected_features_mask]}')\n",
    "\n",
    "# Plot the R^2 score as a function of the number of selected features\n",
    "num_features = range(1, len(pipeline.named_steps['pre'].transformers_[\n",
    "                     0][1].named_steps['rfecv'].cv_results_['mean_test_score']) + 1)\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (r2)\")\n",
    "plt.errorbar(num_features,\n",
    "             pipeline.named_steps['pre'].transformers_[\n",
    "                 0][1].named_steps['rfecv'].cv_results_['mean_test_score'],\n",
    "             yerr=pipeline.named_steps['pre'].transformers_[\n",
    "                 0][1].named_steps['rfecv'].cv_results_['std_test_score'],\n",
    "             fmt='o-', color='black', ecolor='lightgray', elinewidth=3, capsize=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Use the same features and subset\n",
    "all_features_C = np.array(df_C_compo.columns.to_list(\n",
    ") + df_C_specific_testing.columns.to_list() + df_C_specific_features.columns.to_list())\n",
    "group_features_C = np.array(df_C_specific_features.columns.to_list())\n",
    "\n",
    "group_indices_C = np.where(np.isin(all_features_C, group_features_C))[0]\n",
    "X_C_norm_subset = X_C_norm[:, group_indices_C]\n",
    "\n",
    "# Create the RFECV object and compute a cross-validated score.\n",
    "# The \"accuracy\" scoring is proportional to the number of correct classifications\n",
    "rfecv_C = RFECV(estimator=forestmodel_C, step=1, cv=KFold(6),\n",
    "                scoring='neg_mean_squared_error')\n",
    "\n",
    "rfecv_C.fit(X_C_norm_subset, C2_norm.ravel())\n",
    "\n",
    "print(\"Optimal number of features : %d\" % rfecv_C.n_features_)\n",
    "\n",
    "# Get a mask, or integer index, of the features selected\n",
    "selected_features_C = rfecv_C.support_\n",
    "\n",
    "# Get a list of the feature names selected\n",
    "selected_feature_names = group_features_C[selected_features_C]\n",
    "print(selected_feature_names)\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (neg mean squared error)\")\n",
    "plt.plot(range(1, len(rfecv_C.grid_scores_) + 1), rfecv_C.grid_scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore the number of selected features for RFE\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# get the dataset\n",
    "\n",
    "\n",
    "def get_dataset():\n",
    "    X, y = make_classification(\n",
    "        n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n",
    "    return X, y\n",
    "\n",
    "# get a list of models to evaluate\n",
    "\n",
    "\n",
    "def get_models():\n",
    "    models = dict()\n",
    "    for i in range(2, 10):\n",
    "        rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=i)\n",
    "        model = DecisionTreeClassifier()\n",
    "        models[str(i)] = Pipeline(steps=[('s', rfe), ('m', model)])\n",
    "    return models\n",
    "\n",
    "# evaluate a give model using cross-validation\n",
    "\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(\n",
    "        model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    return scores\n",
    "\n",
    "\n",
    "# define dataset\n",
    "X, y = get_dataset()\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "    scores = evaluate_model(model, X, y)\n",
    "    results.append(scores)\n",
    "    names.append(name)\n",
    "    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
