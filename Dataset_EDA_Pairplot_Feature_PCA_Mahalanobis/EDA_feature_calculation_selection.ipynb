{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis (EDA): feature engineering and feature selection\n",
    "\n",
    "**Author:** Y.X. Wu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.FeatureCalculator import FeatureCalculator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# display the current working directory\n",
    "display(\"Current working directory: {0}\".format(os.getcwd()))\n",
    "\n",
    "data_path = '../Dataset_Cleaned/'\n",
    "display(os.path.isfile(data_path+'LiteratureDataset_Corrosion_YW_v3.xlsx'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "### Feature Calculation for Alloy Components\n",
    "\n",
    "Prepares and processes data about alloy compositions, specifically it creates a `FeatureCalculator` object from the defined compositions, then calculates and prints the corresponding alloy features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of component elements and their corresponding fractions\n",
    "compo_elem = [\"Ni\", \"Cr\", \"Mo\", \"Ti\", \"Fe\"]\n",
    "ele_frac = np.array([43.8, 38.3, 2.44, 1.04, 0])\n",
    "\n",
    "# Create a dictionary mapping each element to its corresponding fraction,\n",
    "ele_frac_dict = {elem: frac for elem, frac in zip(compo_elem, ele_frac)}\n",
    "\n",
    "# Prepare data in the format required for FeatureCalculator - a list of tuples,\n",
    "# where each tuple contains a list of elements and their corresponding fractions\n",
    "compositions = [(list(ele_frac_dict.keys()), list(ele_frac_dict.values()))]\n",
    "\n",
    "print(compositions)\n",
    "\n",
    "# Create a FeatureCalculator object with the prepared compositions\n",
    "calculator = FeatureCalculator(compositions)\n",
    "\n",
    "# Calculate the features using the FeatureCalculator object\n",
    "features = calculator.calculate_features()\n",
    "\n",
    "\n",
    "feature_names = [\"a\", \"delta_a\", \"Tm\", \"sigma_Tm\", \"Hmix\", \"sigma_Hmix\", \"ideal_S\",\n",
    "                 \"elec_nega\", \"sigma_elec_nega\", \"VEC\", \"sigma_VEC\", \"bulk_modulus\", \"sigma_bulk_modulus\"]\n",
    "# tabulate the features the feature under feature_names\n",
    "df = pd.DataFrame(features, columns=feature_names)\n",
    "display(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading, Feature Calculation, and Extraction\n",
    "\n",
    "Reads various datasets from Excel files, calculates specific features for each material composition in these datasets using a custom `FeatureCalculator` class, extracts relevant data from the corrosion and hardness datasets, and displays the first rows of the extracted data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils.FeatureCalculator import FeatureCalculator\n",
    "\n",
    "# Initialization of the constants and the data to be loaded\n",
    "feature_names = [\"a\", \"delta_a\", \"Tm\", \"sigma_Tm\", \"Hmix\", \"sigma_Hmix\", \"ideal_S\",\n",
    "                 \"elec_nega\", \"sigma_elec_nega\", \"VEC\", \"sigma_VEC\", \"bulk_modulus\", \"sigma_bulk_modulus\"]\n",
    "\n",
    "# Lists of filenames, elements for each file, and the header rows for the data in each file\n",
    "data_file_names = [\"LiteratureDataset_Corrosion_YW_v3.xlsx\",\n",
    "                   \"LiteratureDataset_Hardness_YW_v3.xlsx\",\n",
    "                   \"MultiTaskModel_NiCrCoVFe_KW99_at_pct.xlsx\",\n",
    "                   \"MultiTaskModel_NiCrCoVFe_KW99_wt_pct.xlsx\",\n",
    "                   \"MultiTaskModel_NiCrMoTiFe_KW131_at_pct.xlsx\",\n",
    "                   \"MultiTaskModel_NiCrMoTiFe_KW131_wt_pct.xlsx\"]\n",
    "element_columns = [['Fe', 'Cr', 'Ni', 'Mo', 'W', 'N', 'Nb', 'C', 'Si', 'Mn',\n",
    "                    'Cu', 'Al', 'V', 'Ta', 'Ti', 'Co', 'Mg', 'Y'],\n",
    "                   ['Fe', 'Cr', 'Ni', 'Mo', 'W', 'N', 'Nb', 'C', 'Si', 'Mn',\n",
    "                    'Cu', 'Al', 'V', 'Ta', 'Ti', 'Co', 'Mg', 'Y', 'Zr', 'Hf'],\n",
    "                   ['Ni', 'Cr', 'Co', 'V', 'Fe'],\n",
    "                   ['Ni', 'Cr', 'Co', 'V', 'Fe'],\n",
    "                   ['Ni', 'Cr', 'Mo', 'Ti', 'Fe'],\n",
    "                   ['Ni', 'Cr', 'Mo', 'Ti', 'Fe']]\n",
    "\n",
    "df_header_list = [2, 2, 0, 0, 0, 0, 0, 0]\n",
    "df_compo = pd.DataFrame(columns=['Fe', 'Cr', 'Ni', 'Mo', 'W', 'N', 'Nb', 'C', 'Si', 'Mn',\n",
    "                                 'Cu', 'Al', 'V', 'Ta', 'Ti', 'Co', 'Mg', 'Y', 'Zr', 'Hf'])\n",
    "features_dfs = []\n",
    "\n",
    "# Processing each data file along with the corresponding elements\n",
    "for i in range(len(data_file_names)):\n",
    "    # Load and preprocess data from each excel file\n",
    "    data_df = pd.read_excel(\n",
    "        data_path + data_file_names[i], header=df_header_list[i])\n",
    "    # print(data_df.columns)\n",
    "    element_fractions = data_df[element_columns[i]].fillna(0)\n",
    "\n",
    "    # Calculate features for each composition\n",
    "    compositions = [(element_columns[i], element_fraction)\n",
    "                    for element_fraction in element_fractions.values]\n",
    "    feature_calculator = FeatureCalculator(compositions)\n",
    "    calculated_features = feature_calculator.calculate_features()\n",
    "\n",
    "    # Store the calculated features in a DataFrame\n",
    "    features_df = pd.DataFrame(calculated_features, columns=feature_names)\n",
    "    features_dfs.append(features_df)\n",
    "\n",
    "    # Extract and store specific data and features for the corrosion and hardness datasets\n",
    "    if i == 0:  # Corrosion dataset\n",
    "        df_C_compo, df_C_specific_testing, df_C_specific_features, df_C_output = element_fractions, data_df[[\n",
    "            'TestTemperature_C', 'ChlorideIonConcentration', 'pH', 'ScanRate_mVs']], features_df, data_df[['AvgPittingPotential_mV']]\n",
    "\n",
    "        # now I want to make df_C_compo have the same columns as df_compo\n",
    "        df_C_compo = pd.concat([df_C_compo, df_compo],\n",
    "                               axis=0, ignore_index=True).fillna(0)\n",
    "        display(df_C_compo.head(1))\n",
    "\n",
    "    if i == 1:  # Hardness dataset\n",
    "        df_H_compo, df_H_specific_features, df_H_output = element_fractions, features_df, data_df[[\n",
    "            'converted HV']]\n",
    "        df_H_compo = pd.concat([df_H_compo, df_compo],\n",
    "                               axis=0, ignore_index=True).fillna(0)\n",
    "        display(df_H_compo.head(1))\n",
    "\n",
    "\n",
    "# Display the first row of the specific data, features, and output for the corrosion and hardness datasets\n",
    "display(df_C_compo.head(1), df_C_specific_testing.head(1),\n",
    "        df_C_specific_features.head(1), df_C_output.head(1))\n",
    "display(df_H_compo.head(1), df_H_specific_features.head(1), df_H_output.head(1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Correlation Matrix and Correlation with the Target Variable\n",
    "\n",
    "Generates two visuals: a heatmap showing the correlation between all features in the data set, and a bar chart indicating the correlation of each feature with the target variable, 'AvgPittingPotential_mV'. These visuals help in identifying the relationships between different features, and how each one impacts the target variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Concatenate DataFrames\n",
    "df_C = pd.concat([df_C_compo, df_C_specific_testing,\n",
    "                 df_C_specific_features, df_C_output], axis=1)\n",
    "display(df_C.head(1))\n",
    "\n",
    "# Compute absolute correlation matrix\n",
    "corr_matrix_C = df_C.corr().abs()\n",
    "\n",
    "# Remove NaN columns and rows\n",
    "corr_matrix_C = corr_matrix_C.loc[:, ~corr_matrix_C.isna().all(axis=0)]\n",
    "corr_matrix_C = corr_matrix_C.loc[~corr_matrix_C.isna().all(axis=1), :]\n",
    "\n",
    "# Create a mask with True in all the cells. We'll only set the diagonal to False in the next step.\n",
    "mask = np.triu(np.ones_like(corr_matrix_C, dtype=bool))\n",
    "\n",
    "# Set the diagonal to False (these are the cells we want to keep)\n",
    "np.fill_diagonal(mask, False)\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(15, 15), dpi=150)\n",
    "sns.heatmap(corr_matrix_C, mask = mask, annot=True, fmt=\".1f\", cmap='RdGy_r',\n",
    "            cbar_kws={'label': 'Correlation Coefficient'}, annot_kws={\"size\": 8})\n",
    "plt.title(\"Correlation matrix of the features\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute and plot correlations with the target variable\n",
    "plt.figure(figsize=(15, 5), dpi=150)\n",
    "df_C.drop('AvgPittingPotential_mV', axis=1).apply(lambda x: x.corr(\n",
    "    df_C['AvgPittingPotential_mV'])).abs().plot(kind='bar')\n",
    "plt.title(\"Correlation of the features with the AvgPittingPotential_mV\")\n",
    "plt.ylabel(\"Correlation\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Concatenate DataFrames\n",
    "df_H = pd.concat([df_H_compo, df_H_specific_features, df_H_output], axis=1)\n",
    "\n",
    "# Compute absolute correlation matrix\n",
    "corr_matrix_H = df_H.corr().abs()\n",
    "\n",
    "# Remove NaN columns and rows\n",
    "corr_matrix_H = corr_matrix_H.loc[:, ~corr_matrix_H.isna().all(axis=0)]\n",
    "corr_matrix_H = corr_matrix_H.loc[~corr_matrix_H.isna().all(axis=1), :]\n",
    "\n",
    "# Create a mask with True in all the cells. We'll only set the diagonal to False in the next step.\n",
    "mask = np.triu(np.ones_like(corr_matrix_H, dtype=bool))\n",
    "\n",
    "# Set the diagonal to False (these are the cells we want to keep)\n",
    "np.fill_diagonal(mask, False)\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(15, 15), dpi=150)\n",
    "\n",
    "# Use the mask in the heatmap\n",
    "sns.heatmap(corr_matrix_H, mask=mask, annot=True, fmt=\".1f\", cmap='RdGy_r',\n",
    "            cbar_kws={'label': 'Correlation Coefficient'}, annot_kws={\"size\": 8})\n",
    "\n",
    "plt.title(\"Correlation matrix of the features\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute and plot correlations with the target variable\n",
    "plt.figure(figsize=(15, 5), dpi=150)\n",
    "df_H.drop('converted HV', axis=1).apply(lambda x: x.corr(\n",
    "    df_H['converted HV'])).abs().plot(kind='bar')\n",
    "plt.title(\"Correlation of the features with the converted HV\")\n",
    "plt.ylabel(\"Correlation\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection\n",
    "\n",
    "### Data Normalization\n",
    "\n",
    "performs MinMax scaling on multiple datasets to prepare them as inputs for a Neural Network, concatenates certain scaled datasets for model training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Prepare data for NN\n",
    "dfs = [df_H_compo, df_H_specific_features, df_H_output,\n",
    "       df_C_compo, df_C_specific_testing, df_C_specific_features, df_C_output]\n",
    "\n",
    "# Convert DataFrames to numpy arrays\n",
    "inputs_outputs = [np.asarray(df.values) for df in dfs]\n",
    "\n",
    "# Define each variable\n",
    "X1, Y1, H1, X2, Z2, W2, C2 = inputs_outputs\n",
    "\n",
    "# Initialize MinMaxScalers for each data set\n",
    "scalers = {\n",
    "    \"compo\": MinMaxScaler(),\n",
    "    \"H_specific_features\": MinMaxScaler(),\n",
    "    \"H_output\": MinMaxScaler(),\n",
    "    \"C_specific_testing\": MinMaxScaler(),\n",
    "    \"C_specific_features\": MinMaxScaler(),\n",
    "    \"C_output\": MinMaxScaler()\n",
    "}\n",
    "\n",
    "# Fit scalers to appropriate data\n",
    "scalers[\"compo\"].fit(np.concatenate((X1, X2)))\n",
    "scalers[\"H_specific_features\"].fit(Y1)\n",
    "scalers[\"H_output\"].fit(H1.reshape((-1, 1)))\n",
    "scalers[\"C_specific_testing\"].fit(Z2)\n",
    "scalers[\"C_specific_features\"].fit(W2)\n",
    "scalers[\"C_output\"].fit(C2.reshape((-1, 1)))\n",
    "\n",
    "# Apply transformations\n",
    "X1_norm = scalers[\"compo\"].transform(X1)\n",
    "Y1_norm = scalers[\"H_specific_features\"].transform(Y1)\n",
    "H1_norm = scalers[\"H_output\"].transform(H1.reshape((-1, 1)))\n",
    "\n",
    "X2_norm = scalers[\"compo\"].transform(X2)\n",
    "Z2_norm = scalers[\"C_specific_testing\"].transform(Z2)\n",
    "W2_norm = scalers[\"C_specific_features\"].transform(W2)\n",
    "C2_norm = scalers[\"C_output\"].transform(C2.reshape((-1, 1)))\n",
    "\n",
    "# Prepare final input data for model training\n",
    "X_H_norm = np.concatenate((X1_norm, Y1_norm), axis=1)\n",
    "X_C_norm = np.concatenate((X2_norm, Z2_norm, W2_norm), axis=1)\n",
    "\n",
    "# Plot distribution of target variables\n",
    "plt.figure(figsize=(15, 5), dpi=150)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(W2_norm[:, 2], bins=50)  # Distribution of one of the features\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(C2_norm, bins=50)  # Distribution of target variable\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and Evaluating Random Forest Regression Models\n",
    "\n",
    "trains and evaluates a Random Forest Regression model using 6-fold cross-validation on two sets of normalized data ('H' and 'C'), outputting the R^2 scores for each fold and their means.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "kf = KFold(n_splits=6, random_state=0, shuffle=True)\n",
    "\n",
    "def train_and_evaluate(X, y, model_name):\n",
    "    models, scores = [], []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test, y_train, y_test = X[train_index], X[test_index], y.ravel()[\n",
    "            train_index], y.ravel()[test_index]\n",
    "\n",
    "        model = RandomForestRegressor(random_state=0,\n",
    "                                      n_estimators=300,\n",
    "                                      max_features=20,\n",
    "                                      max_depth=10,\n",
    "                                      min_samples_split=2,\n",
    "                                      min_samples_leaf=4,\n",
    "                                      bootstrap=True)\n",
    "        model.fit(X_train, y_train)\n",
    "        models.append(model)\n",
    "        scores.append(r2_score(y_test, model.predict(X_test)))\n",
    "\n",
    "\n",
    "    # print the model performance mean and std\n",
    "    print(f\"{model_name} R^2 scores Mean: {np.mean(scores)} Std: {np.std(scores)}\")\n",
    "    return models\n",
    "\n",
    "\n",
    "models_H = train_and_evaluate(X_H_norm, H1_norm, 'H')\n",
    "models_C = train_and_evaluate(X_C_norm, C2_norm, 'C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(models_H[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Hyperparameter Tuning\n",
    "\n",
    "performs hyperparameter tuning and training of Random Forest Regressor models using K-fold cross-validation, evaluates model performance with R^2 scores, and calculates permutation feature importance for two target variables ('H' and 'C') with the same input features.\n",
    "\n",
    "for the small dataset and I will only split into train and test data based on cross-validation: model score + feature importance are evaluated based on the test data.\n",
    "\n",
    "be careful with overfitting: comparing the model score of the training data and the test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Set up the k-fold cross-validation\n",
    "kf = KFold(n_splits=6, random_state=0, shuffle=True)\n",
    "\n",
    "# Define a function to create, train, and evaluate a Random Forest model\n",
    "\n",
    "\n",
    "def hyperevaluate_train_model(X, y, model_name):\n",
    "    (models, scores_train, scores_test, \n",
    "     permu_importances_train, permu_importances_test) = [], [], [], [], []\n",
    "\n",
    "    param_distributions = {\n",
    "        'n_estimators': [50, 100, 150], # less trees to reduce overfitting\n",
    "        'max_features': [1, 'log2', 'sqrt'], \n",
    "        'max_depth': [4, 6, 8], # smaller to reduce overfitting\n",
    "        'min_samples_split': [5, 10, 15], # larger to reduce overfitting \n",
    "        'min_samples_leaf': [6, 8, 10], # larger to reduce overfitting\n",
    "        # 'bootstrap': [True, False]\n",
    "    }\n",
    "\n",
    "    rf = RandomForestRegressor(random_state=0)\n",
    "    random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_distributions,\n",
    "                                       n_iter=100, cv=kf, scoring='r2', verbose=0, random_state=0, n_jobs=-1)\n",
    "    # the model uses the KFold to estimate the average r2 score by testing dataset\n",
    "    random_search.fit(X, y.ravel())\n",
    "\n",
    "    print(f\"Best parameters for {model_name}:\", random_search.best_params_)\n",
    "    print(f\"Best score for {model_name}:\", random_search.best_score_)\n",
    "\n",
    "    # Now with best parameters, train and evaluate\n",
    "    for train_index, test_index in kf.split(X):\n",
    "\n",
    "        X_train, X_test, y_train, y_test = X[train_index], X[test_index], y.ravel()[\n",
    "            train_index], y.ravel()[test_index]\n",
    "\n",
    "        # Now build model with best parameters\n",
    "        model_best = RandomForestRegressor(\n",
    "            **random_search.best_params_, random_state=0)\n",
    "        model_best.fit(X_train, y_train)\n",
    "\n",
    "        models.append(model_best)\n",
    "        scores_train.append(r2_score(y_train, model_best.predict(X_train)))\n",
    "        scores_test.append(r2_score(y_test, model_best.predict(X_test)))\n",
    "\n",
    "        # Calculate permutation feature importance\n",
    "        permu_importance_train = permutation_importance(\n",
    "            model_best, X_train, y_train, n_repeats=50, random_state=42, n_jobs=-1)\n",
    "        permu_importances_train.append(permu_importance_train)\n",
    "        \n",
    "        permu_importance_test = permutation_importance(\n",
    "            model_best, X_test, y_test, n_repeats=50, random_state=42, n_jobs=-1)\n",
    "        permu_importances_test.append(permu_importance_test)\n",
    "\n",
    "\n",
    "    print(f\"{model_name} R^2 scores_train Mean: {np.mean(scores_train)}, Std: {np.std(scores_train)}\")\n",
    "    print(f\"{model_name} R^2 scores_test Mean: {np.mean(scores_test)}, Std: {np.std(scores_test)}\")\n",
    "    print(f\"--------------------------------------------------\")\n",
    "    return models, random_search.best_params_, permu_importances_train, permu_importances_test\n",
    "\n",
    "\n",
    "# Use the function to create, evaluate both models, and calculate importances\n",
    "(models_H, models_H_best_params, permu_importances_train_H, permu_importances_test_H) = hyperevaluate_train_model(X_H_norm, H1_norm, 'H')\n",
    "(models_C, models_C_best_params, permu_importances_train_C, permu_importances_test_C) = hyperevaluate_train_model(X_C_norm, C2_norm, 'C')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a set of four subplots displaying box plots overlapped with error bars, showing permutation feature importances across different datasets (Train H, Test H, Train C, Test C), each derived from 6-fold cross-validation.\n",
    "\n",
    "Compare the permutation feature importances between training data and testing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(10, 12), sharex=True)\n",
    "\n",
    "# create a color palette of 6 colors using seaborn\n",
    "colors = sns.color_palette(\"Set2\", 6)\n",
    "\n",
    "# Data for subplots\n",
    "datasets = [\n",
    "    (permu_importances_train_H, 'Train H'),\n",
    "    (permu_importances_test_H, 'Test H'),\n",
    "    (permu_importances_train_C, 'Train C'),\n",
    "    (permu_importances_test_C, 'Test C')\n",
    "]\n",
    "\n",
    "feature_names_list = [\n",
    "    df_H_compo.columns.append(df_H_specific_features.columns).tolist(),\n",
    "    df_H_compo.columns.append(df_H_specific_features.columns).tolist(),\n",
    "    df_C_compo.columns.append(df_C_specific_testing.columns).append(df_C_specific_features.columns).tolist(),\n",
    "    df_C_compo.columns.append(df_C_specific_testing.columns).append(df_C_specific_features.columns).tolist()\n",
    "]\n",
    "\n",
    "# Create each subplot\n",
    "for ax, (permu_importances, data_label), feature_names in zip(axs.ravel(), datasets, feature_names_list):\n",
    "    \n",
    "    importances_df_Kfold = pd.DataFrame()\n",
    "\n",
    "    for i in range(0, 6):\n",
    "        importances_df = pd.DataFrame(permu_importances[i].importances.T, columns=feature_names)\n",
    "        # display(importances_df)\n",
    "        \n",
    "        importances_df_Kfold = pd.concat([importances_df_Kfold, importances_df], axis=0)\n",
    "\n",
    "        means = importances_df.mean()\n",
    "        errors = importances_df.std()\n",
    "        for j, mean_val in enumerate(means):\n",
    "            ax.errorbar(mean_val, j, xerr=errors[j], marker='o', color=colors[i], zorder=2)\n",
    "    \n",
    "    # print(importances_df_Kfold.shape)\n",
    "    \n",
    "    # now overlap the boxplot from the importances_df_Kfold \n",
    "    ax.boxplot(importances_df_Kfold, vert=False, whis=[5, 95], positions=range(len(importances_df_Kfold.columns)), showfliers=False, widths=1, zorder=1)\n",
    "    ax.axvline(x=0, linestyle='--', color='grey', zorder=1)\n",
    "    ax.set_yticks(range(len(importances_df_Kfold.columns)))\n",
    "    ax.set_yticklabels(importances_df_Kfold.columns)\n",
    "    ax.set_xlabel('Importance')\n",
    "    ax.set_title(f'Permutation Importances on {data_label} Data')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimination to select features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;elim&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;rfecv&#x27;,\n",
       "                                                                   RFECV(cv=6,\n",
       "                                                                         estimator=RandomForestRegressor(max_depth=8,\n",
       "                                                                                                         max_features=&#x27;sqrt&#x27;,\n",
       "                                                                                                         min_samples_leaf=6,\n",
       "                                                                                                         min_samples_split=5,\n",
       "                                                                                                         random_state=0),\n",
       "                                                                         scoring=&#x27;r2&#x27;,\n",
       "                                                                         verbose=True))]),\n",
       "                                                  [&#x27;a&#x27;, &#x27;delta_a&#x27;, &#x27;Tm&#x27;,\n",
       "                                                   &#x27;sigma_Tm&#x27;, &#x27;Hmix&#x27;,\n",
       "                                                   &#x27;sigma_Hmix&#x27;, &#x27;ideal_S&#x27;,\n",
       "                                                   &#x27;elec_nega&#x27;,\n",
       "                                                   &#x27;sigma_elec_nega&#x27;, &#x27;VEC&#x27;,\n",
       "                                                   &#x27;sigma_VEC&#x27;, &#x27;bulk_modulus&#x27;,\n",
       "                                                   &#x27;sigma_bulk_modulus&#x27;]),\n",
       "                                                 (&#x27;fix&#x27;, &#x27;passthrough&#x27;,\n",
       "                                                  [&#x27;Fe&#x27;, &#x27;Cr&#x27;, &#x27;Ni&#x27;, &#x27;Mo&#x27;, &#x27;W&#x27;,\n",
       "                                                   &#x27;N&#x27;, &#x27;Nb&#x27;, &#x27;C&#x27;, &#x27;Si&#x27;, &#x27;Mn&#x27;,\n",
       "                                                   &#x27;Cu&#x27;, &#x27;Al&#x27;, &#x27;V&#x27;, &#x27;Ta&#x27;, &#x27;Ti&#x27;,\n",
       "                                                   &#x27;Co&#x27;, &#x27;Mg&#x27;, &#x27;Y&#x27;, &#x27;Zr&#x27;,\n",
       "                                                   &#x27;Hf&#x27;])])),\n",
       "                (&#x27;reg&#x27;,\n",
       "                 RandomForestRegressor(max_depth=8, max_features=&#x27;sqrt&#x27;,\n",
       "                                       min_samples_leaf=6, min_samples_split=5,\n",
       "                                       random_state=0))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;elim&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;rfecv&#x27;,\n",
       "                                                                   RFECV(cv=6,\n",
       "                                                                         estimator=RandomForestRegressor(max_depth=8,\n",
       "                                                                                                         max_features=&#x27;sqrt&#x27;,\n",
       "                                                                                                         min_samples_leaf=6,\n",
       "                                                                                                         min_samples_split=5,\n",
       "                                                                                                         random_state=0),\n",
       "                                                                         scoring=&#x27;r2&#x27;,\n",
       "                                                                         verbose=True))]),\n",
       "                                                  [&#x27;a&#x27;, &#x27;delta_a&#x27;, &#x27;Tm&#x27;,\n",
       "                                                   &#x27;sigma_Tm&#x27;, &#x27;Hmix&#x27;,\n",
       "                                                   &#x27;sigma_Hmix&#x27;, &#x27;ideal_S&#x27;,\n",
       "                                                   &#x27;elec_nega&#x27;,\n",
       "                                                   &#x27;sigma_elec_nega&#x27;, &#x27;VEC&#x27;,\n",
       "                                                   &#x27;sigma_VEC&#x27;, &#x27;bulk_modulus&#x27;,\n",
       "                                                   &#x27;sigma_bulk_modulus&#x27;]),\n",
       "                                                 (&#x27;fix&#x27;, &#x27;passthrough&#x27;,\n",
       "                                                  [&#x27;Fe&#x27;, &#x27;Cr&#x27;, &#x27;Ni&#x27;, &#x27;Mo&#x27;, &#x27;W&#x27;,\n",
       "                                                   &#x27;N&#x27;, &#x27;Nb&#x27;, &#x27;C&#x27;, &#x27;Si&#x27;, &#x27;Mn&#x27;,\n",
       "                                                   &#x27;Cu&#x27;, &#x27;Al&#x27;, &#x27;V&#x27;, &#x27;Ta&#x27;, &#x27;Ti&#x27;,\n",
       "                                                   &#x27;Co&#x27;, &#x27;Mg&#x27;, &#x27;Y&#x27;, &#x27;Zr&#x27;,\n",
       "                                                   &#x27;Hf&#x27;])])),\n",
       "                (&#x27;reg&#x27;,\n",
       "                 RandomForestRegressor(max_depth=8, max_features=&#x27;sqrt&#x27;,\n",
       "                                       min_samples_leaf=6, min_samples_split=5,\n",
       "                                       random_state=0))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocessor: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;elim&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;rfecv&#x27;,\n",
       "                                                  RFECV(cv=6,\n",
       "                                                        estimator=RandomForestRegressor(max_depth=8,\n",
       "                                                                                        max_features=&#x27;sqrt&#x27;,\n",
       "                                                                                        min_samples_leaf=6,\n",
       "                                                                                        min_samples_split=5,\n",
       "                                                                                        random_state=0),\n",
       "                                                        scoring=&#x27;r2&#x27;,\n",
       "                                                        verbose=True))]),\n",
       "                                 [&#x27;a&#x27;, &#x27;delta_a&#x27;, &#x27;Tm&#x27;, &#x27;sigma_Tm&#x27;, &#x27;Hmix&#x27;,\n",
       "                                  &#x27;sigma_Hmix&#x27;, &#x27;ideal_S&#x27;, &#x27;elec_nega&#x27;,\n",
       "                                  &#x27;sigma_elec_nega&#x27;, &#x27;VEC&#x27;, &#x27;sigma_VEC&#x27;,\n",
       "                                  &#x27;bulk_modulus&#x27;, &#x27;sigma_bulk_modulus&#x27;]),\n",
       "                                (&#x27;fix&#x27;, &#x27;passthrough&#x27;,\n",
       "                                 [&#x27;Fe&#x27;, &#x27;Cr&#x27;, &#x27;Ni&#x27;, &#x27;Mo&#x27;, &#x27;W&#x27;, &#x27;N&#x27;, &#x27;Nb&#x27;, &#x27;C&#x27;,\n",
       "                                  &#x27;Si&#x27;, &#x27;Mn&#x27;, &#x27;Cu&#x27;, &#x27;Al&#x27;, &#x27;V&#x27;, &#x27;Ta&#x27;, &#x27;Ti&#x27;, &#x27;Co&#x27;,\n",
       "                                  &#x27;Mg&#x27;, &#x27;Y&#x27;, &#x27;Zr&#x27;, &#x27;Hf&#x27;])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">elim</label><div class=\"sk-toggleable__content\"><pre>[&#x27;a&#x27;, &#x27;delta_a&#x27;, &#x27;Tm&#x27;, &#x27;sigma_Tm&#x27;, &#x27;Hmix&#x27;, &#x27;sigma_Hmix&#x27;, &#x27;ideal_S&#x27;, &#x27;elec_nega&#x27;, &#x27;sigma_elec_nega&#x27;, &#x27;VEC&#x27;, &#x27;sigma_VEC&#x27;, &#x27;bulk_modulus&#x27;, &#x27;sigma_bulk_modulus&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">rfecv: RFECV</label><div class=\"sk-toggleable__content\"><pre>RFECV(cv=6,\n",
       "      estimator=RandomForestRegressor(max_depth=8, max_features=&#x27;sqrt&#x27;,\n",
       "                                      min_samples_leaf=6, min_samples_split=5,\n",
       "                                      random_state=0),\n",
       "      scoring=&#x27;r2&#x27;, verbose=True)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=8, max_features=&#x27;sqrt&#x27;, min_samples_leaf=6,\n",
       "                      min_samples_split=5, random_state=0)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=8, max_features=&#x27;sqrt&#x27;, min_samples_leaf=6,\n",
       "                      min_samples_split=5, random_state=0)</pre></div></div></div></div></div></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">fix</label><div class=\"sk-toggleable__content\"><pre>[&#x27;Fe&#x27;, &#x27;Cr&#x27;, &#x27;Ni&#x27;, &#x27;Mo&#x27;, &#x27;W&#x27;, &#x27;N&#x27;, &#x27;Nb&#x27;, &#x27;C&#x27;, &#x27;Si&#x27;, &#x27;Mn&#x27;, &#x27;Cu&#x27;, &#x27;Al&#x27;, &#x27;V&#x27;, &#x27;Ta&#x27;, &#x27;Ti&#x27;, &#x27;Co&#x27;, &#x27;Mg&#x27;, &#x27;Y&#x27;, &#x27;Zr&#x27;, &#x27;Hf&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">passthrough</label><div class=\"sk-toggleable__content\"><pre>passthrough</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=8, max_features=&#x27;sqrt&#x27;, min_samples_leaf=6,\n",
       "                      min_samples_split=5, random_state=0)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('elim',\n",
       "                                                  Pipeline(steps=[('rfecv',\n",
       "                                                                   RFECV(cv=6,\n",
       "                                                                         estimator=RandomForestRegressor(max_depth=8,\n",
       "                                                                                                         max_features='sqrt',\n",
       "                                                                                                         min_samples_leaf=6,\n",
       "                                                                                                         min_samples_split=5,\n",
       "                                                                                                         random_state=0),\n",
       "                                                                         scoring='r2',\n",
       "                                                                         verbose=True))]),\n",
       "                                                  ['a', 'delta_a', 'Tm',\n",
       "                                                   'sigma_Tm', 'Hmix',\n",
       "                                                   'sigma_Hmix', 'ideal_S',\n",
       "                                                   'elec_nega',\n",
       "                                                   'sigma_elec_nega', 'VEC',\n",
       "                                                   'sigma_VEC', 'bulk_modulus',\n",
       "                                                   'sigma_bulk_modulus']),\n",
       "                                                 ('fix', 'passthrough',\n",
       "                                                  ['Fe', 'Cr', 'Ni', 'Mo', 'W',\n",
       "                                                   'N', 'Nb', 'C', 'Si', 'Mn',\n",
       "                                                   'Cu', 'Al', 'V', 'Ta', 'Ti',\n",
       "                                                   'Co', 'Mg', 'Y', 'Zr',\n",
       "                                                   'Hf'])])),\n",
       "                ('reg',\n",
       "                 RandomForestRegressor(max_depth=8, max_features='sqrt',\n",
       "                                       min_samples_leaf=6, min_samples_split=5,\n",
       "                                       random_state=0))])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# let's deal with model_H first \n",
    "fixed_features_H = df_H_compo.columns.tolist()\n",
    "elimination_features_H = df_H_specific_features.columns.tolist()\n",
    "\n",
    "df_H_rfe = pd.concat([pd.DataFrame(X1_norm, columns=fixed_features_H), \n",
    "                      pd.DataFrame(Y1_norm, columns=elimination_features_H)], axis=1)\n",
    " \n",
    "# Define a pipeline for the elimination features\n",
    "elimination_pipeline = Pipeline(steps=[\n",
    "    ('rfecv', RFECV(estimator=RandomForestRegressor(**models_H_best_params, random_state=0), \n",
    "                     cv=6, scoring='r2', verbose=True))\n",
    "])\n",
    "\n",
    "# Define a preprocessor that applies the elimination pipeline to the elimination features,\n",
    "# and does nothing to the fixed features (since they are already preprocessed)\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('elim', elimination_pipeline, elimination_features_H),\n",
    "    ('fix', 'passthrough', fixed_features_H)\n",
    "])\n",
    "\n",
    "# Define the final pipeline that includes preprocessing and model training\n",
    "pipeline_H = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('reg', RandomForestRegressor(**models_H_best_params, random_state=0))\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the all data\n",
    "X_H_norm_rfe = df_H_rfe[fixed_features_H + elimination_features_H]\n",
    "H1_norm_rfe = H1_norm.ravel()\n",
    " \n",
    "pipeline_H.fit(X_H_norm_rfe, H1_norm_rfe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: ['a' 'delta_a' 'Hmix' 'sigma_Hmix' 'ideal_S' 'elec_nega' 'sigma_elec_nega'\n",
      " 'VEC' 'sigma_VEC' 'bulk_modulus']\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Access the mask of selected features from the RFECV step in the pipeline.\n",
    "# The `support_` attribute of RFECV is a boolean mask that indicates which features were selected.\n",
    "selected_features_mask_H = pipeline_H.named_steps['preprocessor'].transformers_[\n",
    "    0][1].named_steps['rfecv'].support_\n",
    "\n",
    "# Print the selected features. This is done by indexing the array of elimination_features_H with the boolean mask.\n",
    "print(\n",
    "    f'Selected features: {np.array(elimination_features_H)[selected_features_mask_H]}')\n",
    "\n",
    "# Create a range representing the number of features from 1 to the total number of features in the elimination set. \n",
    "# This will be used as the x-axis in the plot.\n",
    "num_features = range(1, len(pipeline_H.named_steps['preprocessor'].transformers_[\n",
    "                     0][1].named_steps['rfecv'].cv_results_['mean_test_score']) + 1)\n",
    "\n",
    "# Initiate a new figure for the plot.\n",
    "plt.figure()\n",
    "\n",
    "# Set labels for the x-axis and y-axis.\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (r2)\")\n",
    "\n",
    "# Create an errorbar plot. The mean cross-validation score and its standard deviation are obtained from the `cv_results_` attribute of RFECV.\n",
    "# This plot shows how the model performance (in terms of R^2 score) changes as we select a different number of features.\n",
    "plt.errorbar(num_features,\n",
    "             pipeline_H.named_steps['preprocessor'].transformers_[\n",
    "                 0][1].named_steps['rfecv'].cv_results_['mean_test_score'],\n",
    "             yerr=pipeline_H.named_steps['preprocessor'].transformers_[\n",
    "                 0][1].named_steps['rfecv'].cv_results_['std_test_score'],\n",
    "             fmt='o-', color='black', ecolor='lightgray', elinewidth=3, capsize=0)\n",
    "\n",
    "# Display the plot.\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------\n",
    "# to do list\n",
    "# ------\n",
    "\n",
    "# THE PROBLEM is the plotted R2 score is not from the full features - I feel it is only from part of the features\n",
    "\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance Calculation by Random Forest\n",
    "\n",
    "determines and visualizes the importance of features for two datasets (hardness and corrosion), using data from several models, by creating a bar plot with error bars that represent the standard deviation of the calculated importance values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize importance dataframes\n",
    "df_importance_H, df_importance_C = pd.DataFrame(\n",
    "    columns=feature_names_H), pd.DataFrame(columns=feature_names_C)\n",
    "\n",
    "df_permu_importance_train_H, df_permu_importance_train_C = pd.DataFrame(\n",
    "    columns=feature_names_H), pd.DataFrame(columns=feature_names_C)\n",
    "\n",
    "# Populate importance dataframes\n",
    "for i, (model_H, model_C, permu_importance_train_H, permu_importance_train_C) in enumerate(zip(models_H, models_C, permu_importances_train_H, permu_importances_train_C)):\n",
    "\n",
    "    df_importance_H.loc[i] = model_H.feature_importances_\n",
    "    df_importance_C.loc[i] = model_C.feature_importances_\n",
    "\n",
    "    # print(permu_importance_train_H)\n",
    "\n",
    "    df_permu_importance_train_H = pd.concat([df_permu_importance_train_H, pd.DataFrame(\n",
    "        permu_importance_train_H.importances_mean.reshape(1, -1), columns=feature_names_H)])\n",
    "    df_permu_importance_train_C = pd.concat([df_permu_importance_train_C, pd.DataFrame(\n",
    "        permu_importance_train_C.importances_mean.reshape(1, -1), columns=feature_names_C)])\n",
    "\n",
    "# display(df_permu_importance_train_H.shape)\n",
    "# display(df_permu_importance_train_C.shape)\n",
    "\n",
    "# Calculate mean and std for each feature importance\n",
    "df_importance_H.loc['mean'], df_importance_H.loc['std'] = df_importance_H.mean(\n",
    "), df_importance_H.std()\n",
    "df_importance_C.loc['mean'], df_importance_C.loc['std'] = df_importance_C.mean(\n",
    "), df_importance_C.std()\n",
    "df_permu_importance_train_H.loc['mean'], df_permu_importance_train_H.loc['std'] = df_permu_importance_train_H.mean(\n",
    "), df_permu_importance_train_H.std()\n",
    "df_permu_importance_train_C.loc['mean'], df_permu_importance_train_C.loc['std'] = df_permu_importance_train_C.mean(\n",
    "), df_permu_importance_train_C.std()\n",
    "\n",
    "# Select specific features\n",
    "df_importance_eng_H = df_importance_H[df_H_specific_features.columns]\n",
    "df_importance_eng_C = df_importance_C[df_C_specific_features.columns]\n",
    "df_permu_importance_train_eng_H = df_permu_importance_train_H[df_H_specific_features.columns]\n",
    "df_permu_importance_train_eng_C = df_permu_importance_train_C[df_C_specific_features.columns]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizes the feature importances derived from tree-based models for 'Hardness' and 'Corrosion' properties, using bar plots with error bars, to facilitate the comparison of feature relevance between the two models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importances from tree-based models\n",
    "\n",
    "# Synchronize feature names with importance data\n",
    "df_importance_H_full = pd.DataFrame(columns=feature_names_C)\n",
    "df_importance_H_full = pd.concat([df_importance_H_full, df_importance_H], axis=0)\n",
    "df_importance_H_full.index = df_importance_H.index\n",
    "\n",
    "# Prepare plot\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "bar_width = 0.35\n",
    "index = np.arange(len(df_importance_H_full.columns))\n",
    "\n",
    "# Data and labels\n",
    "data = [(df_importance_H_full, 'Hardness'), (df_importance_C, 'Corrosion')]\n",
    "\n",
    "# Plot data with error bars\n",
    "for i, (df, label) in enumerate(data):\n",
    "    ax.bar(index + (i-0.5)*bar_width, df.loc['mean'], bar_width, yerr=df.loc['std'], label=label)\n",
    "\n",
    "# Label and title\n",
    "ax.set_ylabel('Importance')\n",
    "ax.set_xlabel('Features')\n",
    "ax.set_title('Feature Importance by Tree-based Model')\n",
    "\n",
    "# Set x-axis labels with correct rotation\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels(df_importance_H_full.columns, rotation=45, ha='right')\n",
    "\n",
    "# Add legend\n",
    "ax.legend()\n",
    "\n",
    "# Tight layout and display\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importances\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "bar_width = 0.35\n",
    "index = np.arange(len(df_importance_eng_H.columns))\n",
    "\n",
    "# Define data and labels\n",
    "data = [(df_importance_eng_H, 'Hardness'),\n",
    "        (df_importance_eng_C, 'Corrosion')]\n",
    "\n",
    "# Add bars for each dataset\n",
    "for i, (df, label) in enumerate(data):\n",
    "    ax.bar(index + (i-0.5)*bar_width,\n",
    "           df.loc['mean'], bar_width, yerr=df.loc['std'], label=label)\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_ylabel('Importance')\n",
    "ax.set_xlabel('Features')\n",
    "ax.set_title('Feature Importance by tree-based model')\n",
    "ax.set_xticks(index)\n",
    "# set x-axis label rotation and alignment\n",
    "ax.set_xticklabels(df_importance_eng_H.columns, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "\n",
    "# Display plot\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation Feature Importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot permutation feature importances\n",
    "\n",
    "# match the feature importance with the feature names\n",
    "df_permu_importance_train_H_full = pd.DataFrame(columns=feature_names_C)\n",
    "df_permu_importance_train_H_full = pd.concat(\n",
    "    [df_permu_importance_train_H_full, df_permu_importance_train_H], axis=0)\n",
    "df_permu_importance_train_H_full.index = df_permu_importance_train_H.index\n",
    "\n",
    "# display(df_permu_importance_train_H_full)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "bar_width = 0.35\n",
    "index = np.arange(len(df_permu_importance_train_H_full.columns))\n",
    "\n",
    "# Define data and labels\n",
    "data = [(df_permu_importance_train_H_full, 'Hardness'),\n",
    "        (df_permu_importance_train_C, 'Corrosion')]\n",
    "\n",
    "# Add bars for each dataset\n",
    "for i, (df, label) in enumerate(data):\n",
    "    ax.bar(index + (i-0.5)*bar_width,\n",
    "           df.loc['mean'], bar_width, yerr=df.loc['std'], label=label)\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_ylabel('Importance')\n",
    "ax.set_xlabel('Features')\n",
    "ax.set_title('Feature Importance by permutation')\n",
    "ax.set_xticks(index)\n",
    "# set x-axis label rotation and alignment\n",
    "ax.set_xticklabels(df_permu_importance_train_H_full.columns, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "\n",
    "# Display plot\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot permutation feature importances\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "bar_width = 0.35\n",
    "index = np.arange(len(df_permu_importance_train_eng_H.columns))\n",
    "\n",
    "# Define data and labels\n",
    "data = [(df_permu_importance_train_eng_H, 'Hardness'),\n",
    "        (df_permu_importance_train_eng_C, 'Corrosion')]\n",
    "\n",
    "# Add bars for each dataset\n",
    "for i, (df, label) in enumerate(data):\n",
    "    ax.bar(index + (i-0.5)*bar_width,\n",
    "           df.loc['mean'], bar_width, yerr=df.loc['std'], label=label)\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_ylabel('Importance')\n",
    "ax.set_xlabel('Features')\n",
    "ax.set_title('Feature Importance by permutation')\n",
    "ax.set_xticks(index)\n",
    "# set x-axis label rotation and alignment\n",
    "ax.set_xticklabels(df_permu_importance_train_eng_H.columns, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "\n",
    "# Display plot\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top N features based on mean importance for both models\n",
    "N = 8\n",
    "top_features_H = df_permu_importance_train_eng_H.loc['mean'].nlargest(\n",
    "    N).index.tolist()\n",
    "top_features_C = df_permu_importance_train_eng_C.loc['mean'].nlargest(\n",
    "    N).index.tolist()\n",
    "\n",
    "# Find common features\n",
    "common_features = list(set(top_features_H) & set(top_features_C))\n",
    "\n",
    "print(f\"Top {N} features for 'H': {top_features_H}\")\n",
    "print(f\"Top {N} features for 'C': {top_features_C}\")\n",
    "print(f\"Common features: {common_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by combining the mean importance data from both models into a single DataFrame\n",
    "df_permu_importance_train_combined = pd.DataFrame(\n",
    "    index=['mean_H', 'mean_C'], columns=feature_names_C)\n",
    "\n",
    "# Assign mean importances to the DataFrame, filling NA values with zero\n",
    "df_permu_importance_train_combined.loc['mean_H'] = df_permu_importance_train_H_full.loc['mean'].fillna(\n",
    "    0)\n",
    "df_permu_importance_train_combined.loc['mean_C'] = df_permu_importance_train_C.loc['mean'].fillna(\n",
    "    0)\n",
    "\n",
    "# Normalize the importance scores so that the total sum for each model is 1\n",
    "df_permu_importance_train_combined.loc['mean_H'] /= df_permu_importance_train_combined.loc['mean_H'].sum()\n",
    "df_permu_importance_train_combined.loc['mean_C'] /= df_permu_importance_train_combined.loc['mean_C'].sum()\n",
    "\n",
    "# Create a 'consensus' score, which is the average of the importance scores from the two models\n",
    "df_permu_importance_train_combined.loc['consensus_score'] = (\n",
    "    df_permu_importance_train_combined.loc['mean_H'] + df_permu_importance_train_combined.loc['mean_C']) / 2\n",
    "\n",
    "# Reduce the DataFrame to only the features used in model H\n",
    "df_permu_importance_train_eng_combined = df_permu_importance_train_combined[df_H_specific_features.columns]\n",
    "\n",
    "# Sort the features according to the consensus score\n",
    "df_permu_importance_train_eng_combined_sorted = df_permu_importance_train_eng_combined.sort_values(\n",
    "    'consensus_score', ascending=False, axis=1)\n",
    "\n",
    "# Now, let's visualize the sorted feature importances\n",
    "\n",
    "# Define the figure and the axes\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# Set the bar width and calculate the positions of the bars\n",
    "bar_width = 0.25\n",
    "index = np.arange(len(df_permu_importance_train_eng_combined_sorted.columns))\n",
    "\n",
    "# Prepare the data for plotting, along with labels and colors\n",
    "data = [(df_permu_importance_train_eng_combined_sorted.loc['mean_H'], 'Hardness', 'cornflowerblue'),\n",
    "        (df_permu_importance_train_eng_combined_sorted.loc['mean_C'], 'Corrosion', 'darkseagreen'),\n",
    "        (df_permu_importance_train_eng_combined_sorted.loc['consensus_score'], 'Consensus', 'red')]\n",
    "\n",
    "# Plot each data series as a bar plot\n",
    "for i, (df, label, color) in enumerate(data):\n",
    "    ax.bar(index + i*bar_width, df, bar_width, label=label, color=color)\n",
    "\n",
    "# Set the labels and title for the plot\n",
    "ax.set_ylabel('Importance')\n",
    "ax.set_xlabel('Features')\n",
    "ax.set_title('Feature Importance by permutation (normalised)')\n",
    "\n",
    "# Add gridlines to make the plot easier to read\n",
    "ax.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Set up the x-axis labels and ensure they're readable\n",
    "ax.set_xticks(index + bar_width)\n",
    "ax.set_xticklabels(\n",
    "    df_permu_importance_train_eng_combined_sorted.columns, rotation=45, ha='right')\n",
    "\n",
    "# Position the legend in a good spot\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "# Make sure everything fits and then show the plot\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### need to do something about the correlation of features\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimination (RFE) - a customised version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import RFE\n",
    "\n",
    "# # Let's say these are your full features and you have a list defining the group of features\n",
    "# # you're interested in, like this:\n",
    "\n",
    "# all_features_C = np.array(df_C_compo.columns.to_list(\n",
    "# ) + df_C_specific_testing.columns.to_list() + df_C_specific_features.columns.to_list())  # all features\n",
    "# # the specific group of features\n",
    "# group_features_C = np.array(df_C_specific_features.columns.to_list())\n",
    "\n",
    "# # Get the indices of the group features in the full feature list\n",
    "# group_indices_C = np.where(np.isin(all_features_C, group_features_C))[0]\n",
    "\n",
    "# # Now extract the subset of X corresponding to group features\n",
    "# X_C_norm_subset = X_C_norm[:, group_indices_C]\n",
    "\n",
    "# # Now you can run RFE or any other feature selection method on this subset\n",
    "# rfe_C = RFE(estimator=forestmodel_C, n_features_to_select=5)\n",
    "# rfe_C = rfe_C.fit(X_C_norm_subset, C2_norm.ravel())\n",
    "# X_C_norm_subset_rfe = rfe_C.transform(X_C_norm_subset)\n",
    "\n",
    "# # Get a mask, or integer index, of the features selected\n",
    "# selected_features_C = rfe_C.support_\n",
    "\n",
    "# # Get a list of the feature names selected\n",
    "# selected_feature_names = group_features_C[selected_features_C]\n",
    "# print(selected_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.feature_selection import RFE\n",
    "# import numpy as np\n",
    "\n",
    "# # Assuming X_train is your feature matrix and y_train are your labels\n",
    "# # always_include is a list of indices for the features you always want to include\n",
    "\n",
    "# # replace these with the indices of your features\n",
    "# always_include_indices = [0, 2, 5]\n",
    "# include_X_train = X_train[:, always_include_indices]\n",
    "\n",
    "# # Remaining features for RFE\n",
    "# rfe_indices = [idx for idx in range(\n",
    "#     X_train.shape[1]) if idx not in always_include_indices]\n",
    "# rfe_X_train = X_train[:, rfe_indices]\n",
    "\n",
    "# # Set up a classifier to use with RFE\n",
    "# clf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# # Perform RFE on remaining features\n",
    "# selector = RFE(clf, n_features_to_select=10, step=1)  # choose your parameters\n",
    "# selector = selector.fit(rfe_X_train, y_train)\n",
    "\n",
    "# # Now, we join the always included features with the selected features from RFE\n",
    "# mask = selector.support_\n",
    "# selected_rfe_indices = np.array(rfe_indices)[mask]\n",
    "# selected_indices = np.concatenate(\n",
    "#     [always_include_indices, selected_rfe_indices])\n",
    "\n",
    "# # Now you can fit your final model on the selected features\n",
    "# final_X_train = X_train[:, selected_indices]\n",
    "# final_clf = RandomForestClassifier(n_estimators=100)\n",
    "# final_clf.fit(final_X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first let's use an example to understand how the RFE works\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate a dummy dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=6,\n",
    "                       noise=0.1, random_state=42)\n",
    "df = pd.DataFrame(X, columns=['A', 'B', 'C', 'D', 'E', 'F'])\n",
    "df['target'] = y\n",
    "\n",
    "# Define the fixed features and the features to be eliminated\n",
    "fixed_features = ['A', 'B']\n",
    "elimination_features = ['C', 'D', 'E', 'F']\n",
    "\n",
    "# Define a pipeline for the elimination features\n",
    "elimination_pipeline = Pipeline(steps=[\n",
    "    ('scale', StandardScaler()),\n",
    "    ('rfecv', RFECV(estimator=LinearRegression(), cv=5, scoring='r2'))\n",
    "])\n",
    "\n",
    "# Define a preprocessor that applies the elimination pipeline to the elimination features,\n",
    "# and applies scaling to the fixed features\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('elim', elimination_pipeline, elimination_features),\n",
    "    ('fix', StandardScaler(), fixed_features)\n",
    "])\n",
    "\n",
    "# Define the final pipeline that includes preprocessing and model training\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('pre', preprocessor),\n",
    "    ('reg', LinearRegression())\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the all data\n",
    "X_all = df[fixed_features + elimination_features]\n",
    "y_all = df['target']\n",
    "\n",
    "pipeline.fit(X_all, y_all)\n",
    "\n",
    "# Access the selected features\n",
    "selected_features_mask = pipeline.named_steps['pre'].transformers_[\n",
    "    0][1].named_steps['rfecv'].support_\n",
    "print(\n",
    "    f'Selected features: {np.array(elimination_features)[selected_features_mask]}')\n",
    "\n",
    "# Plot the R^2 score as a function of the number of selected features\n",
    "num_features = range(1, len(pipeline.named_steps['pre'].transformers_[\n",
    "                     0][1].named_steps['rfecv'].cv_results_['mean_test_score']) + 1)\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (r2)\")\n",
    "plt.errorbar(num_features,\n",
    "             pipeline.named_steps['pre'].transformers_[\n",
    "                 0][1].named_steps['rfecv'].cv_results_['mean_test_score'],\n",
    "             yerr=pipeline.named_steps['pre'].transformers_[\n",
    "                 0][1].named_steps['rfecv'].cv_results_['std_test_score'],\n",
    "             fmt='o-', color='black', ecolor='lightgray', elinewidth=3, capsize=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Use the same features and subset\n",
    "all_features_C = np.array(df_C_compo.columns.to_list(\n",
    ") + df_C_specific_testing.columns.to_list() + df_C_specific_features.columns.to_list())\n",
    "group_features_C = np.array(df_C_specific_features.columns.to_list())\n",
    "\n",
    "group_indices_C = np.where(np.isin(all_features_C, group_features_C))[0]\n",
    "X_C_norm_subset = X_C_norm[:, group_indices_C]\n",
    "\n",
    "# Create the RFECV object and compute a cross-validated score.\n",
    "# The \"accuracy\" scoring is proportional to the number of correct classifications\n",
    "rfecv_C = RFECV(estimator=forestmodel_C, step=1, cv=KFold(6),\n",
    "                scoring='neg_mean_squared_error')\n",
    "\n",
    "rfecv_C.fit(X_C_norm_subset, C2_norm.ravel())\n",
    "\n",
    "print(\"Optimal number of features : %d\" % rfecv_C.n_features_)\n",
    "\n",
    "# Get a mask, or integer index, of the features selected\n",
    "selected_features_C = rfecv_C.support_\n",
    "\n",
    "# Get a list of the feature names selected\n",
    "selected_feature_names = group_features_C[selected_features_C]\n",
    "print(selected_feature_names)\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (neg mean squared error)\")\n",
    "plt.plot(range(1, len(rfecv_C.grid_scores_) + 1), rfecv_C.grid_scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
