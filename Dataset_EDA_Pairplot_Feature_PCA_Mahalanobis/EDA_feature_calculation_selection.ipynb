{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis (EDA): feature engineering and feature selection\n",
    "\n",
    "**Author:** Y.X. Wu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.FeatureCalculator import FeatureCalculator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# display the current working directory\n",
    "display(\"Current working directory: {0}\".format(os.getcwd()))\n",
    "\n",
    "data_path = '../Dataset_Cleaned/'\n",
    "display(os.path.isfile(data_path+'LiteratureDataset_Corrosion_YW_v3.xlsx'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "### Feature Calculation for Alloy Components\n",
    "\n",
    "Prepares and processes data about alloy compositions, specifically it creates a `FeatureCalculator` object from the defined compositions, then calculates and prints the corresponding alloy features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of component elements and their corresponding fractions\n",
    "compo_elem = [\"Ni\", \"Cr\", \"Mo\", \"Ti\", \"Fe\"]\n",
    "ele_frac = np.array([43.8, 38.3, 2.44, 1.04, 0])\n",
    "\n",
    "# Create a dictionary mapping each element to its corresponding fraction,\n",
    "ele_frac_dict = {elem: frac for elem, frac in zip(compo_elem, ele_frac)}\n",
    "\n",
    "# Prepare data in the format required for FeatureCalculator - a list of tuples,\n",
    "# where each tuple contains a list of elements and their corresponding fractions\n",
    "compositions = [(list(ele_frac_dict.keys()), list(ele_frac_dict.values()))]\n",
    "\n",
    "print(compositions)\n",
    "\n",
    "# Create a FeatureCalculator object with the prepared compositions\n",
    "calculator = FeatureCalculator(compositions)\n",
    "\n",
    "# Calculate the features using the FeatureCalculator object\n",
    "features = calculator.calculate_features()\n",
    "\n",
    "\n",
    "feature_names = [\"a\", \"delta_a\", \"Tm\", \"sigma_Tm\", \"Hmix\", \"sigma_Hmix\", \"ideal_S\",\n",
    "                 \"elec_nega\", \"sigma_elec_nega\", \"VEC\", \"sigma_VEC\", \"bulk_modulus\", \"sigma_bulk_modulus\"]\n",
    "# tabulate the features the feature under feature_names\n",
    "df = pd.DataFrame(features, columns=feature_names)\n",
    "display(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading, Feature Calculation, and Extraction\n",
    "\n",
    "Reads various datasets from Excel files, calculates specific features for each material composition in these datasets using a custom `FeatureCalculator` class, extracts relevant data from the corrosion and hardness datasets, and displays the first rows of the extracted data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils.FeatureCalculator import FeatureCalculator\n",
    "\n",
    "# Initialization of the constants and the data to be loaded\n",
    "feature_names = [\"a\", \"delta_a\", \"Tm\", \"sigma_Tm\", \"Hmix\", \"sigma_Hmix\", \"ideal_S\",\n",
    "                 \"elec_nega\", \"sigma_elec_nega\", \"VEC\", \"sigma_VEC\", \"bulk_modulus\", \"sigma_bulk_modulus\"]\n",
    "\n",
    "# Lists of filenames, elements for each file, and the header rows for the data in each file\n",
    "data_file_names = [\"LiteratureDataset_Corrosion_YW_v3.xlsx\",\n",
    "                   \"LiteratureDataset_Hardness_YW_v3.xlsx\",\n",
    "                   \"MultiTaskModel_NiCrCoVFe_KW99_at_pct.xlsx\",\n",
    "                   \"MultiTaskModel_NiCrCoVFe_KW99_wt_pct.xlsx\",\n",
    "                   \"MultiTaskModel_NiCrMoTiFe_KW131_at_pct.xlsx\",\n",
    "                   \"MultiTaskModel_NiCrMoTiFe_KW131_wt_pct.xlsx\"]\n",
    "element_columns = [['Fe', 'Cr', 'Ni', 'Mo', 'W', 'N', 'Nb', 'C', 'Si', 'Mn',\n",
    "                    'Cu', 'Al', 'V', 'Ta', 'Ti', 'Co', 'Mg', 'Y'],\n",
    "                   ['Fe', 'Cr', 'Ni', 'Mo', 'W', 'N', 'Nb', 'C', 'Si', 'Mn',\n",
    "                    'Cu', 'Al', 'V', 'Ta', 'Ti', 'Co', 'Mg', 'Y', 'Zr', 'Hf'],\n",
    "                   ['Ni', 'Cr', 'Co', 'V', 'Fe'],\n",
    "                   ['Ni', 'Cr', 'Co', 'V', 'Fe'],\n",
    "                   ['Ni', 'Cr', 'Mo', 'Ti', 'Fe'],\n",
    "                   ['Ni', 'Cr', 'Mo', 'Ti', 'Fe']]\n",
    "\n",
    "df_header_list = [2, 2, 0, 0, 0, 0, 0, 0]\n",
    "df_compo = pd.DataFrame(columns=['Fe', 'Cr', 'Ni', 'Mo', 'W', 'N', 'Nb', 'C', 'Si', 'Mn',\n",
    "                                 'Cu', 'Al', 'V', 'Ta', 'Ti', 'Co', 'Mg', 'Y', 'Zr', 'Hf'])\n",
    "features_dfs = []\n",
    "\n",
    "# Processing each data file along with the corresponding elements\n",
    "for i in range(len(data_file_names)):\n",
    "    # Load and preprocess data from each excel file\n",
    "    data_df = pd.read_excel(\n",
    "        data_path + data_file_names[i], header=df_header_list[i])\n",
    "    # print(data_df.columns)\n",
    "    element_fractions = data_df[element_columns[i]].fillna(0)\n",
    "\n",
    "    # Calculate features for each composition\n",
    "    compositions = [(element_columns[i], element_fraction)\n",
    "                    for element_fraction in element_fractions.values]\n",
    "    feature_calculator = FeatureCalculator(compositions)\n",
    "    calculated_features = feature_calculator.calculate_features()\n",
    "\n",
    "    # Store the calculated features in a DataFrame\n",
    "    features_df = pd.DataFrame(calculated_features, columns=feature_names)\n",
    "    features_dfs.append(features_df)\n",
    "\n",
    "    # Extract and store specific data and features for the corrosion and hardness datasets\n",
    "    if i == 0:  # Corrosion dataset\n",
    "        df_C_compo, df_C_specific_testing, df_C_specific_features, df_C_output = element_fractions, data_df[[\n",
    "            'TestTemperature_C', 'ChlorideIonConcentration', 'pH', 'ScanRate_mVs']], features_df, data_df[['AvgPittingPotential_mV']]\n",
    "\n",
    "        # now I want to make df_C_compo have the same columns as df_compo\n",
    "        df_C_compo = pd.concat([df_C_compo, df_compo],\n",
    "                               axis=0, ignore_index=True).fillna(0)\n",
    "        display(df_C_compo.head(1))\n",
    "\n",
    "    if i == 1:  # Hardness dataset\n",
    "        df_H_compo, df_H_specific_features, df_H_output = element_fractions, features_df, data_df[[\n",
    "            'converted HV']]\n",
    "        df_H_compo = pd.concat([df_H_compo, df_compo],\n",
    "                               axis=0, ignore_index=True).fillna(0)\n",
    "        display(df_H_compo.head(1))\n",
    "\n",
    "\n",
    "# Display the first row of the specific data, features, and output for the corrosion and hardness datasets\n",
    "display(df_C_compo.head(1), df_C_specific_testing.head(1),\n",
    "        df_C_specific_features.head(1), df_C_output.head(1))\n",
    "display(df_H_compo.head(1), df_H_specific_features.head(1), df_H_output.head(1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Correlation Matrix and Correlation with the Target Variable\n",
    "\n",
    "Generates two visuals: a heatmap showing the correlation between all features in the data set, and a bar chart indicating the correlation of each feature with the target variable, 'AvgPittingPotential_mV'. These visuals help in identifying the relationships between different features, and how each one impacts the target variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Concatenate DataFrames\n",
    "df_C = pd.concat([df_C_compo, df_C_specific_testing,\n",
    "                 df_C_specific_features, df_C_output], axis=1)\n",
    "display(df_C.head(1))\n",
    "\n",
    "# Compute absolute correlation matrix\n",
    "corr_matrix_C = df_C.corr().abs()\n",
    "\n",
    "# Remove NaN columns and rows\n",
    "corr_matrix_C = corr_matrix_C.loc[:, ~corr_matrix_C.isna().all(axis=0)]\n",
    "corr_matrix_C = corr_matrix_C.loc[~corr_matrix_C.isna().all(axis=1), :]\n",
    "\n",
    "# Create a mask with True in all the cells. We'll only set the diagonal to False in the next step.\n",
    "mask = np.triu(np.ones_like(corr_matrix_C, dtype=bool))\n",
    "\n",
    "# Set the diagonal to False (these are the cells we want to keep)\n",
    "np.fill_diagonal(mask, False)\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(15, 15), dpi=150)\n",
    "sns.heatmap(corr_matrix_C, mask = mask, annot=True, fmt=\".1f\", cmap='RdGy_r',\n",
    "            cbar_kws={'label': 'Correlation Coefficient'}, annot_kws={\"size\": 8})\n",
    "plt.title(\"Correlation matrix of the features\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute and plot correlations with the target variable\n",
    "plt.figure(figsize=(15, 5), dpi=150)\n",
    "df_C.drop('AvgPittingPotential_mV', axis=1).apply(lambda x: x.corr(\n",
    "    df_C['AvgPittingPotential_mV'])).abs().plot(kind='bar')\n",
    "plt.title(\"Correlation of the features with the AvgPittingPotential_mV\")\n",
    "plt.ylabel(\"Correlation\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Concatenate DataFrames\n",
    "df_H = pd.concat([df_H_compo, df_H_specific_features, df_H_output], axis=1)\n",
    "\n",
    "# Compute absolute correlation matrix\n",
    "corr_matrix_H = df_H.corr().abs()\n",
    "\n",
    "# Remove NaN columns and rows\n",
    "corr_matrix_H = corr_matrix_H.loc[:, ~corr_matrix_H.isna().all(axis=0)]\n",
    "corr_matrix_H = corr_matrix_H.loc[~corr_matrix_H.isna().all(axis=1), :]\n",
    "\n",
    "# Create a mask with True in all the cells. We'll only set the diagonal to False in the next step.\n",
    "mask = np.triu(np.ones_like(corr_matrix_H, dtype=bool))\n",
    "\n",
    "# Set the diagonal to False (these are the cells we want to keep)\n",
    "np.fill_diagonal(mask, False)\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(15, 15), dpi=150)\n",
    "\n",
    "# Use the mask in the heatmap\n",
    "sns.heatmap(corr_matrix_H, mask=mask, annot=True, fmt=\".1f\", cmap='RdGy_r',\n",
    "            cbar_kws={'label': 'Correlation Coefficient'}, annot_kws={\"size\": 8})\n",
    "\n",
    "plt.title(\"Correlation matrix of the features\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute and plot correlations with the target variable\n",
    "plt.figure(figsize=(15, 5), dpi=150)\n",
    "df_H.drop('converted HV', axis=1).apply(lambda x: x.corr(\n",
    "    df_H['converted HV'])).abs().plot(kind='bar')\n",
    "plt.title(\"Correlation of the features with the converted HV\")\n",
    "plt.ylabel(\"Correlation\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection\n",
    "\n",
    "### Data Normalization\n",
    "\n",
    "performs MinMax scaling on multiple datasets to prepare them as inputs for a Neural Network, concatenates certain scaled datasets for model training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Prepare data for NN\n",
    "dfs = [df_H_compo, df_H_specific_features, df_H_output,\n",
    "       df_C_compo, df_C_specific_testing, df_C_specific_features, df_C_output]\n",
    "\n",
    "# Convert DataFrames to numpy arrays\n",
    "inputs_outputs = [np.asarray(df.values) for df in dfs]\n",
    "\n",
    "# Define each variable\n",
    "X1, Y1, H1, X2, Z2, W2, C2 = inputs_outputs\n",
    "\n",
    "# Initialize MinMaxScalers for each data set\n",
    "scalers = {\n",
    "    \"compo\": MinMaxScaler(),\n",
    "    \"H_specific_features\": MinMaxScaler(),\n",
    "    \"H_output\": MinMaxScaler(),\n",
    "    \"C_specific_testing\": MinMaxScaler(),\n",
    "    \"C_specific_features\": MinMaxScaler(),\n",
    "    \"C_output\": MinMaxScaler()\n",
    "}\n",
    "\n",
    "# Fit scalers to appropriate data\n",
    "scalers[\"compo\"].fit(np.concatenate((X1, X2)))\n",
    "scalers[\"H_specific_features\"].fit(Y1)\n",
    "scalers[\"H_output\"].fit(H1.reshape((-1, 1)))\n",
    "scalers[\"C_specific_testing\"].fit(Z2)\n",
    "scalers[\"C_specific_features\"].fit(W2)\n",
    "scalers[\"C_output\"].fit(C2.reshape((-1, 1)))\n",
    "\n",
    "# Apply transformations\n",
    "X1_norm = scalers[\"compo\"].transform(X1)\n",
    "Y1_norm = scalers[\"H_specific_features\"].transform(Y1)\n",
    "H1_norm = scalers[\"H_output\"].transform(H1.reshape((-1, 1)))\n",
    "\n",
    "X2_norm = scalers[\"compo\"].transform(X2)\n",
    "Z2_norm = scalers[\"C_specific_testing\"].transform(Z2)\n",
    "W2_norm = scalers[\"C_specific_features\"].transform(W2)\n",
    "C2_norm = scalers[\"C_output\"].transform(C2.reshape((-1, 1)))\n",
    "\n",
    "# Prepare final input data for model training\n",
    "X_H_norm = np.concatenate((X1_norm, Y1_norm), axis=1)\n",
    "X_C_norm = np.concatenate((X2_norm, Z2_norm, W2_norm), axis=1)\n",
    "\n",
    "# Plot distribution of target variables\n",
    "plt.figure(figsize=(15, 5), dpi=150)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(W2_norm[:, 2], bins=50)  # Distribution of one of the features\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(C2_norm, bins=50)  # Distribution of target variable\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and Evaluating Random Forest Regression Models\n",
    "\n",
    "trains and evaluates a Random Forest Regression model using 6-fold cross-validation on two sets of normalized data ('H' and 'C'), outputting the R^2 scores for each fold and their means.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "kf = KFold(n_splits=6, random_state=0, shuffle=True)\n",
    "\n",
    "def train_and_evaluate(X, y, model_name):\n",
    "    models, scores = [], []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test, y_train, y_test = X[train_index], X[test_index], y.ravel()[\n",
    "            train_index], y.ravel()[test_index]\n",
    "\n",
    "        model = RandomForestRegressor(random_state=0,\n",
    "                                      n_estimators=300,\n",
    "                                      max_features=20,\n",
    "                                      max_depth=10,\n",
    "                                      min_samples_split=2,\n",
    "                                      min_samples_leaf=4,\n",
    "                                      bootstrap=True)\n",
    "        model.fit(X_train, y_train)\n",
    "        models.append(model)\n",
    "        scores.append(r2_score(y_test, model.predict(X_test)))\n",
    "\n",
    "\n",
    "    # print the model performance mean and std\n",
    "    print(f\"{model_name} R^2 scores Mean: {np.mean(scores)} Std: {np.std(scores)}\")\n",
    "    return models\n",
    "\n",
    "\n",
    "models_H = train_and_evaluate(X_H_norm, H1_norm, 'H')\n",
    "models_C = train_and_evaluate(X_C_norm, C2_norm, 'C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(models_H[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Hyperparameter Tuning\n",
    "\n",
    "performs hyperparameter tuning and training of Random Forest Regressor models using K-fold cross-validation, evaluates model performance with R^2 scores, and calculates permutation feature importance for two target variables ('H' and 'C') with the same input features.\n",
    "\n",
    "for the small dataset and I will only split into train and test data based on cross-validation: model score + feature importance are evaluated based on the test data.\n",
    "\n",
    "be careful with overfitting: comparing the model score of the training data and the test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Set up the k-fold cross-validation\n",
    "kf = KFold(n_splits=6, random_state=0, shuffle=True)\n",
    "\n",
    "# Define a function to create, train, and evaluate a Random Forest model\n",
    "\n",
    "\n",
    "def hyperevaluate_train_model(X, y, model_name):\n",
    "    (models, scores_train, scores_test, \n",
    "     permu_importances_train, permu_importances_test) = [], [], [], [], []\n",
    "\n",
    "    param_distributions = {\n",
    "        'n_estimators': [50, 100, 150], # less trees to reduce overfitting\n",
    "        'max_features': [1, 'log2', 'sqrt'], \n",
    "        'max_depth': [4, 6, 8], # smaller to reduce overfitting\n",
    "        'min_samples_split': [5, 10, 15], # larger to reduce overfitting \n",
    "        'min_samples_leaf': [6, 8, 10], # larger to reduce overfitting\n",
    "        # 'bootstrap': [True, False]\n",
    "    }\n",
    "\n",
    "    rf = RandomForestRegressor(random_state=0)\n",
    "    random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_distributions,\n",
    "                                       n_iter=100, cv=kf, scoring='r2', verbose=0, random_state=0, n_jobs=-1)\n",
    "    # the model uses the KFold to estimate the average r2 score by testing dataset\n",
    "    random_search.fit(X, y.ravel())\n",
    "\n",
    "    print(f\"Best parameters for {model_name}:\", random_search.best_params_)\n",
    "    print(f\"Best score for {model_name}:\", random_search.best_score_)\n",
    "\n",
    "    # Now with best parameters, train and evaluate\n",
    "    for train_index, test_index in kf.split(X):\n",
    "\n",
    "        X_train, X_test, y_train, y_test = X[train_index], X[test_index], y.ravel()[\n",
    "            train_index], y.ravel()[test_index]\n",
    "\n",
    "        # Now build model with best parameters\n",
    "        model_best = RandomForestRegressor(\n",
    "            **random_search.best_params_, random_state=0)\n",
    "        model_best.fit(X_train, y_train)\n",
    "\n",
    "        models.append(model_best)\n",
    "        scores_train.append(r2_score(y_train, model_best.predict(X_train)))\n",
    "        scores_test.append(r2_score(y_test, model_best.predict(X_test)))\n",
    "\n",
    "        # Calculate permutation feature importance\n",
    "        permu_importance_train = permutation_importance(\n",
    "            model_best, X_train, y_train, n_repeats=50, random_state=42, n_jobs=-1)\n",
    "        permu_importances_train.append(permu_importance_train)\n",
    "        \n",
    "        permu_importance_test = permutation_importance(\n",
    "            model_best, X_test, y_test, n_repeats=50, random_state=42, n_jobs=-1)\n",
    "        permu_importances_test.append(permu_importance_test)\n",
    "\n",
    "\n",
    "    print(f\"{model_name} R^2 scores_train Mean: {np.mean(scores_train)}, Std: {np.std(scores_train)}\")\n",
    "    print(f\"{model_name} R^2 scores_test Mean: {np.mean(scores_test)}, Std: {np.std(scores_test)}\")\n",
    "    print(f\"--------------------------------------------------\")\n",
    "    return models, random_search.best_params_, permu_importances_train, permu_importances_test\n",
    "\n",
    "\n",
    "# Use the function to create, evaluate both models, and calculate importances\n",
    "(models_H, models_H_best_params, permu_importances_train_H, permu_importances_test_H) = hyperevaluate_train_model(X_H_norm, H1_norm, 'H')\n",
    "(models_C, models_C_best_params, permu_importances_train_C, permu_importances_test_C) = hyperevaluate_train_model(X_C_norm, C2_norm, 'C')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a set of four subplots displaying box plots overlapped with error bars, showing permutation feature importances across different datasets (Train H, Test H, Train C, Test C), each derived from 6-fold cross-validation.\n",
    "\n",
    "Compare the permutation feature importances between training data and testing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(10, 12), sharex=True)\n",
    "\n",
    "# create a color palette of 6 colors using seaborn\n",
    "colors = sns.color_palette(\"Set2\", 6)\n",
    "\n",
    "# Data for subplots\n",
    "datasets = [\n",
    "    (permu_importances_train_H, 'Train H'),\n",
    "    (permu_importances_test_H, 'Test H'),\n",
    "    (permu_importances_train_C, 'Train C'),\n",
    "    (permu_importances_test_C, 'Test C')\n",
    "]\n",
    "\n",
    "feature_names_list = [\n",
    "    df_H_compo.columns.append(df_H_specific_features.columns).tolist(),\n",
    "    df_H_compo.columns.append(df_H_specific_features.columns).tolist(),\n",
    "    df_C_compo.columns.append(df_C_specific_testing.columns).append(df_C_specific_features.columns).tolist(),\n",
    "    df_C_compo.columns.append(df_C_specific_testing.columns).append(df_C_specific_features.columns).tolist()\n",
    "]\n",
    "\n",
    "# Create each subplot\n",
    "for ax, (permu_importances, data_label), feature_names in zip(axs.ravel(), datasets, feature_names_list):\n",
    "    \n",
    "    importances_df_Kfold = pd.DataFrame()\n",
    "\n",
    "    for i in range(0, 6):\n",
    "        importances_df = pd.DataFrame(permu_importances[i].importances.T, columns=feature_names)\n",
    "        # display(importances_df)\n",
    "        \n",
    "        importances_df_Kfold = pd.concat([importances_df_Kfold, importances_df], axis=0)\n",
    "\n",
    "        means = importances_df.mean()\n",
    "        errors = importances_df.std()\n",
    "        for j, mean_val in enumerate(means):\n",
    "            ax.errorbar(mean_val, j, xerr=errors[j], marker='o', color=colors[i], zorder=2)\n",
    "    \n",
    "    # print(importances_df_Kfold.shape)\n",
    "    \n",
    "    # now overlap the boxplot from the importances_df_Kfold \n",
    "    ax.boxplot(importances_df_Kfold, vert=False, whis=[5, 95], positions=range(len(importances_df_Kfold.columns)), showfliers=False, widths=1, zorder=1)\n",
    "    ax.axvline(x=0, linestyle='--', color='grey', zorder=1)\n",
    "    ax.set_yticks(range(len(importances_df_Kfold.columns)))\n",
    "    ax.set_yticklabels(importances_df_Kfold.columns)\n",
    "    ax.set_xlabel('Importance')\n",
    "    ax.set_title(f'Permutation Importances on {data_label} Data')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimination to select features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;passthrough&#x27;, &#x27;passthrough&#x27;,\n",
       "                                                  [&#x27;Fe&#x27;, &#x27;Cr&#x27;, &#x27;Ni&#x27;, &#x27;Mo&#x27;, &#x27;W&#x27;,\n",
       "                                                   &#x27;N&#x27;, &#x27;Nb&#x27;, &#x27;C&#x27;, &#x27;Si&#x27;, &#x27;Mn&#x27;,\n",
       "                                                   &#x27;Cu&#x27;, &#x27;Al&#x27;, &#x27;V&#x27;, &#x27;Ta&#x27;, &#x27;Ti&#x27;,\n",
       "                                                   &#x27;Co&#x27;, &#x27;Mg&#x27;, &#x27;Y&#x27;, &#x27;Zr&#x27;,\n",
       "                                                   &#x27;Hf&#x27;]),\n",
       "                                                 (&#x27;pipeline&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;rfecv&#x27;,\n",
       "                                                                   RFECV(cv=6,\n",
       "                                                                         estimator=RandomForestRegressor(max_depth=8,\n",
       "                                                                                                         max_features=&#x27;sqrt&#x27;,\n",
       "                                                                                                         min_samples_leaf=6,\n",
       "                                                                                                         min_samples_split=5,\n",
       "                                                                                                         random_state=0),\n",
       "                                                                         scoring=&#x27;r2&#x27;,\n",
       "                                                                         verbose=True))]),\n",
       "                                                  [&#x27;a&#x27;, &#x27;delta_a&#x27;, &#x27;Tm&#x27;,\n",
       "                                                   &#x27;sigma_Tm&#x27;, &#x27;Hmix&#x27;,\n",
       "                                                   &#x27;sigma_Hmix&#x27;, &#x27;ideal_S&#x27;,\n",
       "                                                   &#x27;elec_nega&#x27;,\n",
       "                                                   &#x27;sigma_elec_nega&#x27;, &#x27;VEC&#x27;,\n",
       "                                                   &#x27;sigma_VEC&#x27;, &#x27;bulk_modulus&#x27;,\n",
       "                                                   &#x27;sigma_bulk_modulus&#x27;])])),\n",
       "                (&#x27;reg&#x27;,\n",
       "                 RandomForestRegressor(max_depth=8, max_features=&#x27;sqrt&#x27;,\n",
       "                                       min_samples_leaf=6, min_samples_split=5,\n",
       "                                       random_state=0))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;passthrough&#x27;, &#x27;passthrough&#x27;,\n",
       "                                                  [&#x27;Fe&#x27;, &#x27;Cr&#x27;, &#x27;Ni&#x27;, &#x27;Mo&#x27;, &#x27;W&#x27;,\n",
       "                                                   &#x27;N&#x27;, &#x27;Nb&#x27;, &#x27;C&#x27;, &#x27;Si&#x27;, &#x27;Mn&#x27;,\n",
       "                                                   &#x27;Cu&#x27;, &#x27;Al&#x27;, &#x27;V&#x27;, &#x27;Ta&#x27;, &#x27;Ti&#x27;,\n",
       "                                                   &#x27;Co&#x27;, &#x27;Mg&#x27;, &#x27;Y&#x27;, &#x27;Zr&#x27;,\n",
       "                                                   &#x27;Hf&#x27;]),\n",
       "                                                 (&#x27;pipeline&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;rfecv&#x27;,\n",
       "                                                                   RFECV(cv=6,\n",
       "                                                                         estimator=RandomForestRegressor(max_depth=8,\n",
       "                                                                                                         max_features=&#x27;sqrt&#x27;,\n",
       "                                                                                                         min_samples_leaf=6,\n",
       "                                                                                                         min_samples_split=5,\n",
       "                                                                                                         random_state=0),\n",
       "                                                                         scoring=&#x27;r2&#x27;,\n",
       "                                                                         verbose=True))]),\n",
       "                                                  [&#x27;a&#x27;, &#x27;delta_a&#x27;, &#x27;Tm&#x27;,\n",
       "                                                   &#x27;sigma_Tm&#x27;, &#x27;Hmix&#x27;,\n",
       "                                                   &#x27;sigma_Hmix&#x27;, &#x27;ideal_S&#x27;,\n",
       "                                                   &#x27;elec_nega&#x27;,\n",
       "                                                   &#x27;sigma_elec_nega&#x27;, &#x27;VEC&#x27;,\n",
       "                                                   &#x27;sigma_VEC&#x27;, &#x27;bulk_modulus&#x27;,\n",
       "                                                   &#x27;sigma_bulk_modulus&#x27;])])),\n",
       "                (&#x27;reg&#x27;,\n",
       "                 RandomForestRegressor(max_depth=8, max_features=&#x27;sqrt&#x27;,\n",
       "                                       min_samples_leaf=6, min_samples_split=5,\n",
       "                                       random_state=0))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocessor: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;passthrough&#x27;, &#x27;passthrough&#x27;,\n",
       "                                 [&#x27;Fe&#x27;, &#x27;Cr&#x27;, &#x27;Ni&#x27;, &#x27;Mo&#x27;, &#x27;W&#x27;, &#x27;N&#x27;, &#x27;Nb&#x27;, &#x27;C&#x27;,\n",
       "                                  &#x27;Si&#x27;, &#x27;Mn&#x27;, &#x27;Cu&#x27;, &#x27;Al&#x27;, &#x27;V&#x27;, &#x27;Ta&#x27;, &#x27;Ti&#x27;, &#x27;Co&#x27;,\n",
       "                                  &#x27;Mg&#x27;, &#x27;Y&#x27;, &#x27;Zr&#x27;, &#x27;Hf&#x27;]),\n",
       "                                (&#x27;pipeline&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;rfecv&#x27;,\n",
       "                                                  RFECV(cv=6,\n",
       "                                                        estimator=RandomForestRegressor(max_depth=8,\n",
       "                                                                                        max_features=&#x27;sqrt&#x27;,\n",
       "                                                                                        min_samples_leaf=6,\n",
       "                                                                                        min_samples_split=5,\n",
       "                                                                                        random_state=0),\n",
       "                                                        scoring=&#x27;r2&#x27;,\n",
       "                                                        verbose=True))]),\n",
       "                                 [&#x27;a&#x27;, &#x27;delta_a&#x27;, &#x27;Tm&#x27;, &#x27;sigma_Tm&#x27;, &#x27;Hmix&#x27;,\n",
       "                                  &#x27;sigma_Hmix&#x27;, &#x27;ideal_S&#x27;, &#x27;elec_nega&#x27;,\n",
       "                                  &#x27;sigma_elec_nega&#x27;, &#x27;VEC&#x27;, &#x27;sigma_VEC&#x27;,\n",
       "                                  &#x27;bulk_modulus&#x27;, &#x27;sigma_bulk_modulus&#x27;])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-21\" type=\"checkbox\" ><label for=\"sk-estimator-id-21\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">passthrough</label><div class=\"sk-toggleable__content\"><pre>[&#x27;Fe&#x27;, &#x27;Cr&#x27;, &#x27;Ni&#x27;, &#x27;Mo&#x27;, &#x27;W&#x27;, &#x27;N&#x27;, &#x27;Nb&#x27;, &#x27;C&#x27;, &#x27;Si&#x27;, &#x27;Mn&#x27;, &#x27;Cu&#x27;, &#x27;Al&#x27;, &#x27;V&#x27;, &#x27;Ta&#x27;, &#x27;Ti&#x27;, &#x27;Co&#x27;, &#x27;Mg&#x27;, &#x27;Y&#x27;, &#x27;Zr&#x27;, &#x27;Hf&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-22\" type=\"checkbox\" ><label for=\"sk-estimator-id-22\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">passthrough</label><div class=\"sk-toggleable__content\"><pre>passthrough</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-23\" type=\"checkbox\" ><label for=\"sk-estimator-id-23\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">pipeline</label><div class=\"sk-toggleable__content\"><pre>[&#x27;a&#x27;, &#x27;delta_a&#x27;, &#x27;Tm&#x27;, &#x27;sigma_Tm&#x27;, &#x27;Hmix&#x27;, &#x27;sigma_Hmix&#x27;, &#x27;ideal_S&#x27;, &#x27;elec_nega&#x27;, &#x27;sigma_elec_nega&#x27;, &#x27;VEC&#x27;, &#x27;sigma_VEC&#x27;, &#x27;bulk_modulus&#x27;, &#x27;sigma_bulk_modulus&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-24\" type=\"checkbox\" ><label for=\"sk-estimator-id-24\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">rfecv: RFECV</label><div class=\"sk-toggleable__content\"><pre>RFECV(cv=6,\n",
       "      estimator=RandomForestRegressor(max_depth=8, max_features=&#x27;sqrt&#x27;,\n",
       "                                      min_samples_leaf=6, min_samples_split=5,\n",
       "                                      random_state=0),\n",
       "      scoring=&#x27;r2&#x27;, verbose=True)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-25\" type=\"checkbox\" ><label for=\"sk-estimator-id-25\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=8, max_features=&#x27;sqrt&#x27;, min_samples_leaf=6,\n",
       "                      min_samples_split=5, random_state=0)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-26\" type=\"checkbox\" ><label for=\"sk-estimator-id-26\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=8, max_features=&#x27;sqrt&#x27;, min_samples_leaf=6,\n",
       "                      min_samples_split=5, random_state=0)</pre></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-27\" type=\"checkbox\" ><label for=\"sk-estimator-id-27\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=8, max_features=&#x27;sqrt&#x27;, min_samples_leaf=6,\n",
       "                      min_samples_split=5, random_state=0)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('passthrough', 'passthrough',\n",
       "                                                  ['Fe', 'Cr', 'Ni', 'Mo', 'W',\n",
       "                                                   'N', 'Nb', 'C', 'Si', 'Mn',\n",
       "                                                   'Cu', 'Al', 'V', 'Ta', 'Ti',\n",
       "                                                   'Co', 'Mg', 'Y', 'Zr',\n",
       "                                                   'Hf']),\n",
       "                                                 ('pipeline',\n",
       "                                                  Pipeline(steps=[('rfecv',\n",
       "                                                                   RFECV(cv=6,\n",
       "                                                                         estimator=RandomForestRegressor(max_depth=8,\n",
       "                                                                                                         max_features='sqrt',\n",
       "                                                                                                         min_samples_leaf=6,\n",
       "                                                                                                         min_samples_split=5,\n",
       "                                                                                                         random_state=0),\n",
       "                                                                         scoring='r2',\n",
       "                                                                         verbose=True))]),\n",
       "                                                  ['a', 'delta_a', 'Tm',\n",
       "                                                   'sigma_Tm', 'Hmix',\n",
       "                                                   'sigma_Hmix', 'ideal_S',\n",
       "                                                   'elec_nega',\n",
       "                                                   'sigma_elec_nega', 'VEC',\n",
       "                                                   'sigma_VEC', 'bulk_modulus',\n",
       "                                                   'sigma_bulk_modulus'])])),\n",
       "                ('reg',\n",
       "                 RandomForestRegressor(max_depth=8, max_features='sqrt',\n",
       "                                       min_samples_leaf=6, min_samples_split=5,\n",
       "                                       random_state=0))])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "# let's deal with model_H first \n",
    "fixed_features_H = df_H_compo.columns.tolist()\n",
    "elimination_features_H = df_H_specific_features.columns.tolist()\n",
    "\n",
    "df_H_rfe = pd.concat([pd.DataFrame(X1_norm, columns=fixed_features_H), \n",
    "                      pd.DataFrame(Y1_norm, columns=elimination_features_H)], axis=1)\n",
    "\n",
    "# Define a pipeline for the elimination features\n",
    "elimination_pipeline = Pipeline(steps=[\n",
    "    ('rfecv', RFECV(estimator=RandomForestRegressor(**models_H_best_params, random_state=0), \n",
    "                     cv=6, scoring='r2', verbose=True))\n",
    "])\n",
    "\n",
    "# Define a preprocessor that applies the elimination pipeline to the elimination features,\n",
    "# and does nothing to the fixed features (since they are already preprocessed)\n",
    "preprocessor = make_column_transformer(\n",
    "    ('passthrough', fixed_features_H),\n",
    "    (elimination_pipeline, elimination_features_H)\n",
    ")\n",
    "\n",
    "# Define the final pipeline that includes preprocessing and model training\n",
    "pipeline_H = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('reg', RandomForestRegressor(**models_H_best_params, random_state=0))\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the all data\n",
    "X_H_norm_rfe = df_H_rfe[fixed_features_H + elimination_features_H]\n",
    "H1_norm_rfe = H1_norm.ravel()\n",
    " \n",
    "pipeline_H.fit(X_H_norm_rfe, H1_norm_rfe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'named_steps'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[202], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Access the mask of selected features from the RFECV step in the pipeline.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# The `support_` attribute of RFECV is a boolean mask that indicates which features were selected.\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m selected_features_mask_H \u001b[39m=\u001b[39m pipeline_H\u001b[39m.\u001b[39;49mnamed_steps[\u001b[39m'\u001b[39;49m\u001b[39mpreprocessor\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mtransformers_[\n\u001b[1;32m      4\u001b[0m     \u001b[39m0\u001b[39;49m][\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49mnamed_steps[\u001b[39m'\u001b[39m\u001b[39mrfecv\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39msupport_\n\u001b[1;32m      6\u001b[0m \u001b[39m# Print the selected features. This is done by indexing the array of elimination_features_H with the boolean mask.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mprint\u001b[39m(\n\u001b[1;32m      8\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mSelected features: \u001b[39m\u001b[39m{\u001b[39;00mnp\u001b[39m.\u001b[39marray(elimination_features_H)[selected_features_mask_H]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'named_steps'"
     ]
    }
   ],
   "source": [
    "# Access the mask of selected features from the RFECV step in the pipeline.\n",
    "# The `support_` attribute of RFECV is a boolean mask that indicates which features were selected.\n",
    "selected_features_mask_H = pipeline_H.named_steps['preprocessor'].transformers_[\n",
    "    0][1].named_steps['rfecv'].support_\n",
    "\n",
    "# Print the selected features. This is done by indexing the array of elimination_features_H with the boolean mask.\n",
    "print(\n",
    "    f'Selected features: {np.array(elimination_features_H)[selected_features_mask_H]}')\n",
    "\n",
    "# Create a range representing the number of features from 1 to the total number of features in the elimination set. \n",
    "# This will be used as the x-axis in the plot.\n",
    "num_features = range(1, len(pipeline_H.named_steps['preprocessor'].transformers_[\n",
    "                     0][1].named_steps['rfecv'].cv_results_['mean_test_score']) + 1)\n",
    "\n",
    "# Initiate a new figure for the plot.\n",
    "plt.figure()\n",
    "\n",
    "# Set labels for the x-axis and y-axis.\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (r2)\")\n",
    "\n",
    "# Create an errorbar plot. The mean cross-validation score and its standard deviation are obtained from the `cv_results_` attribute of RFECV.\n",
    "# This plot shows how the model performance (in terms of R^2 score) changes as we select a different number of features.\n",
    "plt.errorbar(num_features,\n",
    "             pipeline_H.named_steps['preprocessor'].transformers_[\n",
    "                 0][1].named_steps['rfecv'].cv_results_['mean_test_score'],\n",
    "             yerr=pipeline_H.named_steps['preprocessor'].transformers_[\n",
    "                 0][1].named_steps['rfecv'].cv_results_['std_test_score'],\n",
    "             fmt='o-', color='black', ecolor='lightgray', elinewidth=3, capsize=0)\n",
    "\n",
    "# Display the plot.\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------\n",
    "# to do list\n",
    "# ------\n",
    "\n",
    "# THE PROBLEM is the plotted R2 score is not from the full features - I feel it is only from part of the features\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance Calculation by Random Forest\n",
    "\n",
    "determines and visualizes the importance of features for two datasets (hardness and corrosion), using data from several models, by creating a bar plot with error bars that represent the standard deviation of the calculated importance values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize importance dataframes\n",
    "df_importance_H, df_importance_C = pd.DataFrame(\n",
    "    columns=feature_names_H), pd.DataFrame(columns=feature_names_C)\n",
    "\n",
    "df_permu_importance_train_H, df_permu_importance_train_C = pd.DataFrame(\n",
    "    columns=feature_names_H), pd.DataFrame(columns=feature_names_C)\n",
    "\n",
    "# Populate importance dataframes\n",
    "for i, (model_H, model_C, permu_importance_train_H, permu_importance_train_C) in enumerate(zip(models_H, models_C, permu_importances_train_H, permu_importances_train_C)):\n",
    "\n",
    "    df_importance_H.loc[i] = model_H.feature_importances_\n",
    "    df_importance_C.loc[i] = model_C.feature_importances_\n",
    "\n",
    "    # print(permu_importance_train_H)\n",
    "\n",
    "    df_permu_importance_train_H = pd.concat([df_permu_importance_train_H, pd.DataFrame(\n",
    "        permu_importance_train_H.importances_mean.reshape(1, -1), columns=feature_names_H)])\n",
    "    df_permu_importance_train_C = pd.concat([df_permu_importance_train_C, pd.DataFrame(\n",
    "        permu_importance_train_C.importances_mean.reshape(1, -1), columns=feature_names_C)])\n",
    "\n",
    "# display(df_permu_importance_train_H.shape)\n",
    "# display(df_permu_importance_train_C.shape)\n",
    "\n",
    "# Calculate mean and std for each feature importance\n",
    "df_importance_H.loc['mean'], df_importance_H.loc['std'] = df_importance_H.mean(\n",
    "), df_importance_H.std()\n",
    "df_importance_C.loc['mean'], df_importance_C.loc['std'] = df_importance_C.mean(\n",
    "), df_importance_C.std()\n",
    "df_permu_importance_train_H.loc['mean'], df_permu_importance_train_H.loc['std'] = df_permu_importance_train_H.mean(\n",
    "), df_permu_importance_train_H.std()\n",
    "df_permu_importance_train_C.loc['mean'], df_permu_importance_train_C.loc['std'] = df_permu_importance_train_C.mean(\n",
    "), df_permu_importance_train_C.std()\n",
    "\n",
    "# Select specific features\n",
    "df_importance_eng_H = df_importance_H[df_H_specific_features.columns]\n",
    "df_importance_eng_C = df_importance_C[df_C_specific_features.columns]\n",
    "df_permu_importance_train_eng_H = df_permu_importance_train_H[df_H_specific_features.columns]\n",
    "df_permu_importance_train_eng_C = df_permu_importance_train_C[df_C_specific_features.columns]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizes the feature importances derived from tree-based models for 'Hardness' and 'Corrosion' properties, using bar plots with error bars, to facilitate the comparison of feature relevance between the two models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importances from tree-based models\n",
    "\n",
    "# Synchronize feature names with importance data\n",
    "df_importance_H_full = pd.DataFrame(columns=feature_names_C)\n",
    "df_importance_H_full = pd.concat([df_importance_H_full, df_importance_H], axis=0)\n",
    "df_importance_H_full.index = df_importance_H.index\n",
    "\n",
    "# Prepare plot\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "bar_width = 0.35\n",
    "index = np.arange(len(df_importance_H_full.columns))\n",
    "\n",
    "# Data and labels\n",
    "data = [(df_importance_H_full, 'Hardness'), (df_importance_C, 'Corrosion')]\n",
    "\n",
    "# Plot data with error bars\n",
    "for i, (df, label) in enumerate(data):\n",
    "    ax.bar(index + (i-0.5)*bar_width, df.loc['mean'], bar_width, yerr=df.loc['std'], label=label)\n",
    "\n",
    "# Label and title\n",
    "ax.set_ylabel('Importance')\n",
    "ax.set_xlabel('Features')\n",
    "ax.set_title('Feature Importance by Tree-based Model')\n",
    "\n",
    "# Set x-axis labels with correct rotation\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels(df_importance_H_full.columns, rotation=45, ha='right')\n",
    "\n",
    "# Add legend\n",
    "ax.legend()\n",
    "\n",
    "# Tight layout and display\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importances\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "bar_width = 0.35\n",
    "index = np.arange(len(df_importance_eng_H.columns))\n",
    "\n",
    "# Define data and labels\n",
    "data = [(df_importance_eng_H, 'Hardness'),\n",
    "        (df_importance_eng_C, 'Corrosion')]\n",
    "\n",
    "# Add bars for each dataset\n",
    "for i, (df, label) in enumerate(data):\n",
    "    ax.bar(index + (i-0.5)*bar_width,\n",
    "           df.loc['mean'], bar_width, yerr=df.loc['std'], label=label)\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_ylabel('Importance')\n",
    "ax.set_xlabel('Features')\n",
    "ax.set_title('Feature Importance by tree-based model')\n",
    "ax.set_xticks(index)\n",
    "# set x-axis label rotation and alignment\n",
    "ax.set_xticklabels(df_importance_eng_H.columns, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "\n",
    "# Display plot\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation Feature Importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot permutation feature importances\n",
    "\n",
    "# match the feature importance with the feature names\n",
    "df_permu_importance_train_H_full = pd.DataFrame(columns=feature_names_C)\n",
    "df_permu_importance_train_H_full = pd.concat(\n",
    "    [df_permu_importance_train_H_full, df_permu_importance_train_H], axis=0)\n",
    "df_permu_importance_train_H_full.index = df_permu_importance_train_H.index\n",
    "\n",
    "# display(df_permu_importance_train_H_full)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "bar_width = 0.35\n",
    "index = np.arange(len(df_permu_importance_train_H_full.columns))\n",
    "\n",
    "# Define data and labels\n",
    "data = [(df_permu_importance_train_H_full, 'Hardness'),\n",
    "        (df_permu_importance_train_C, 'Corrosion')]\n",
    "\n",
    "# Add bars for each dataset\n",
    "for i, (df, label) in enumerate(data):\n",
    "    ax.bar(index + (i-0.5)*bar_width,\n",
    "           df.loc['mean'], bar_width, yerr=df.loc['std'], label=label)\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_ylabel('Importance')\n",
    "ax.set_xlabel('Features')\n",
    "ax.set_title('Feature Importance by permutation')\n",
    "ax.set_xticks(index)\n",
    "# set x-axis label rotation and alignment\n",
    "ax.set_xticklabels(df_permu_importance_train_H_full.columns, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "\n",
    "# Display plot\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot permutation feature importances\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "bar_width = 0.35\n",
    "index = np.arange(len(df_permu_importance_train_eng_H.columns))\n",
    "\n",
    "# Define data and labels\n",
    "data = [(df_permu_importance_train_eng_H, 'Hardness'),\n",
    "        (df_permu_importance_train_eng_C, 'Corrosion')]\n",
    "\n",
    "# Add bars for each dataset\n",
    "for i, (df, label) in enumerate(data):\n",
    "    ax.bar(index + (i-0.5)*bar_width,\n",
    "           df.loc['mean'], bar_width, yerr=df.loc['std'], label=label)\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_ylabel('Importance')\n",
    "ax.set_xlabel('Features')\n",
    "ax.set_title('Feature Importance by permutation')\n",
    "ax.set_xticks(index)\n",
    "# set x-axis label rotation and alignment\n",
    "ax.set_xticklabels(df_permu_importance_train_eng_H.columns, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "\n",
    "# Display plot\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top N features based on mean importance for both models\n",
    "N = 8\n",
    "top_features_H = df_permu_importance_train_eng_H.loc['mean'].nlargest(\n",
    "    N).index.tolist()\n",
    "top_features_C = df_permu_importance_train_eng_C.loc['mean'].nlargest(\n",
    "    N).index.tolist()\n",
    "\n",
    "# Find common features\n",
    "common_features = list(set(top_features_H) & set(top_features_C))\n",
    "\n",
    "print(f\"Top {N} features for 'H': {top_features_H}\")\n",
    "print(f\"Top {N} features for 'C': {top_features_C}\")\n",
    "print(f\"Common features: {common_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by combining the mean importance data from both models into a single DataFrame\n",
    "df_permu_importance_train_combined = pd.DataFrame(\n",
    "    index=['mean_H', 'mean_C'], columns=feature_names_C)\n",
    "\n",
    "# Assign mean importances to the DataFrame, filling NA values with zero\n",
    "df_permu_importance_train_combined.loc['mean_H'] = df_permu_importance_train_H_full.loc['mean'].fillna(\n",
    "    0)\n",
    "df_permu_importance_train_combined.loc['mean_C'] = df_permu_importance_train_C.loc['mean'].fillna(\n",
    "    0)\n",
    "\n",
    "# Normalize the importance scores so that the total sum for each model is 1\n",
    "df_permu_importance_train_combined.loc['mean_H'] /= df_permu_importance_train_combined.loc['mean_H'].sum()\n",
    "df_permu_importance_train_combined.loc['mean_C'] /= df_permu_importance_train_combined.loc['mean_C'].sum()\n",
    "\n",
    "# Create a 'consensus' score, which is the average of the importance scores from the two models\n",
    "df_permu_importance_train_combined.loc['consensus_score'] = (\n",
    "    df_permu_importance_train_combined.loc['mean_H'] + df_permu_importance_train_combined.loc['mean_C']) / 2\n",
    "\n",
    "# Reduce the DataFrame to only the features used in model H\n",
    "df_permu_importance_train_eng_combined = df_permu_importance_train_combined[df_H_specific_features.columns]\n",
    "\n",
    "# Sort the features according to the consensus score\n",
    "df_permu_importance_train_eng_combined_sorted = df_permu_importance_train_eng_combined.sort_values(\n",
    "    'consensus_score', ascending=False, axis=1)\n",
    "\n",
    "# Now, let's visualize the sorted feature importances\n",
    "\n",
    "# Define the figure and the axes\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# Set the bar width and calculate the positions of the bars\n",
    "bar_width = 0.25\n",
    "index = np.arange(len(df_permu_importance_train_eng_combined_sorted.columns))\n",
    "\n",
    "# Prepare the data for plotting, along with labels and colors\n",
    "data = [(df_permu_importance_train_eng_combined_sorted.loc['mean_H'], 'Hardness', 'cornflowerblue'),\n",
    "        (df_permu_importance_train_eng_combined_sorted.loc['mean_C'], 'Corrosion', 'darkseagreen'),\n",
    "        (df_permu_importance_train_eng_combined_sorted.loc['consensus_score'], 'Consensus', 'red')]\n",
    "\n",
    "# Plot each data series as a bar plot\n",
    "for i, (df, label, color) in enumerate(data):\n",
    "    ax.bar(index + i*bar_width, df, bar_width, label=label, color=color)\n",
    "\n",
    "# Set the labels and title for the plot\n",
    "ax.set_ylabel('Importance')\n",
    "ax.set_xlabel('Features')\n",
    "ax.set_title('Feature Importance by permutation (normalised)')\n",
    "\n",
    "# Add gridlines to make the plot easier to read\n",
    "ax.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Set up the x-axis labels and ensure they're readable\n",
    "ax.set_xticks(index + bar_width)\n",
    "ax.set_xticklabels(\n",
    "    df_permu_importance_train_eng_combined_sorted.columns, rotation=45, ha='right')\n",
    "\n",
    "# Position the legend in a good spot\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "# Make sure everything fits and then show the plot\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### need to do something about the correlation of features\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimination (RFE) - a customised version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import RFE\n",
    "\n",
    "# # Let's say these are your full features and you have a list defining the group of features\n",
    "# # you're interested in, like this:\n",
    "\n",
    "# all_features_C = np.array(df_C_compo.columns.to_list(\n",
    "# ) + df_C_specific_testing.columns.to_list() + df_C_specific_features.columns.to_list())  # all features\n",
    "# # the specific group of features\n",
    "# group_features_C = np.array(df_C_specific_features.columns.to_list())\n",
    "\n",
    "# # Get the indices of the group features in the full feature list\n",
    "# group_indices_C = np.where(np.isin(all_features_C, group_features_C))[0]\n",
    "\n",
    "# # Now extract the subset of X corresponding to group features\n",
    "# X_C_norm_subset = X_C_norm[:, group_indices_C]\n",
    "\n",
    "# # Now you can run RFE or any other feature selection method on this subset\n",
    "# rfe_C = RFE(estimator=forestmodel_C, n_features_to_select=5)\n",
    "# rfe_C = rfe_C.fit(X_C_norm_subset, C2_norm.ravel())\n",
    "# X_C_norm_subset_rfe = rfe_C.transform(X_C_norm_subset)\n",
    "\n",
    "# # Get a mask, or integer index, of the features selected\n",
    "# selected_features_C = rfe_C.support_\n",
    "\n",
    "# # Get a list of the feature names selected\n",
    "# selected_feature_names = group_features_C[selected_features_C]\n",
    "# print(selected_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.feature_selection import RFE\n",
    "# import numpy as np\n",
    "\n",
    "# # Assuming X_train is your feature matrix and y_train are your labels\n",
    "# # always_include is a list of indices for the features you always want to include\n",
    "\n",
    "# # replace these with the indices of your features\n",
    "# always_include_indices = [0, 2, 5]\n",
    "# include_X_train = X_train[:, always_include_indices]\n",
    "\n",
    "# # Remaining features for RFE\n",
    "# rfe_indices = [idx for idx in range(\n",
    "#     X_train.shape[1]) if idx not in always_include_indices]\n",
    "# rfe_X_train = X_train[:, rfe_indices]\n",
    "\n",
    "# # Set up a classifier to use with RFE\n",
    "# clf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# # Perform RFE on remaining features\n",
    "# selector = RFE(clf, n_features_to_select=10, step=1)  # choose your parameters\n",
    "# selector = selector.fit(rfe_X_train, y_train)\n",
    "\n",
    "# # Now, we join the always included features with the selected features from RFE\n",
    "# mask = selector.support_\n",
    "# selected_rfe_indices = np.array(rfe_indices)[mask]\n",
    "# selected_indices = np.concatenate(\n",
    "#     [always_include_indices, selected_rfe_indices])\n",
    "\n",
    "# # Now you can fit your final model on the selected features\n",
    "# final_X_train = X_train[:, selected_indices]\n",
    "# final_clf = RandomForestClassifier(n_estimators=100)\n",
    "# final_clf.fit(final_X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first let's use an example to understand how the RFE works\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate a dummy dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=6,\n",
    "                       noise=0.1, random_state=42)\n",
    "df = pd.DataFrame(X, columns=['A', 'B', 'C', 'D', 'E', 'F'])\n",
    "df['target'] = y\n",
    "\n",
    "# Define the fixed features and the features to be eliminated\n",
    "fixed_features = ['A', 'B']\n",
    "elimination_features = ['C', 'D', 'E', 'F']\n",
    "\n",
    "# Define a pipeline for the elimination features\n",
    "elimination_pipeline = Pipeline(steps=[\n",
    "    ('scale', StandardScaler()),\n",
    "    ('rfecv', RFECV(estimator=LinearRegression(), cv=5, scoring='r2'))\n",
    "])\n",
    "\n",
    "# Define a preprocessor that applies the elimination pipeline to the elimination features,\n",
    "# and applies scaling to the fixed features\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('elim', elimination_pipeline, elimination_features),\n",
    "    ('fix', StandardScaler(), fixed_features)\n",
    "])\n",
    "\n",
    "# Define the final pipeline that includes preprocessing and model training\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('pre', preprocessor),\n",
    "    ('reg', LinearRegression())\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the all data\n",
    "X_all = df[fixed_features + elimination_features]\n",
    "y_all = df['target']\n",
    "\n",
    "pipeline.fit(X_all, y_all)\n",
    "\n",
    "# Access the selected features\n",
    "selected_features_mask = pipeline.named_steps['pre'].transformers_[\n",
    "    0][1].named_steps['rfecv'].support_\n",
    "print(\n",
    "    f'Selected features: {np.array(elimination_features)[selected_features_mask]}')\n",
    "\n",
    "# Plot the R^2 score as a function of the number of selected features\n",
    "num_features = range(1, len(pipeline.named_steps['pre'].transformers_[\n",
    "                     0][1].named_steps['rfecv'].cv_results_['mean_test_score']) + 1)\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (r2)\")\n",
    "plt.errorbar(num_features,\n",
    "             pipeline.named_steps['pre'].transformers_[\n",
    "                 0][1].named_steps['rfecv'].cv_results_['mean_test_score'],\n",
    "             yerr=pipeline.named_steps['pre'].transformers_[\n",
    "                 0][1].named_steps['rfecv'].cv_results_['std_test_score'],\n",
    "             fmt='o-', color='black', ecolor='lightgray', elinewidth=3, capsize=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Use the same features and subset\n",
    "all_features_C = np.array(df_C_compo.columns.to_list(\n",
    ") + df_C_specific_testing.columns.to_list() + df_C_specific_features.columns.to_list())\n",
    "group_features_C = np.array(df_C_specific_features.columns.to_list())\n",
    "\n",
    "group_indices_C = np.where(np.isin(all_features_C, group_features_C))[0]\n",
    "X_C_norm_subset = X_C_norm[:, group_indices_C]\n",
    "\n",
    "# Create the RFECV object and compute a cross-validated score.\n",
    "# The \"accuracy\" scoring is proportional to the number of correct classifications\n",
    "rfecv_C = RFECV(estimator=forestmodel_C, step=1, cv=KFold(6),\n",
    "                scoring='neg_mean_squared_error')\n",
    "\n",
    "rfecv_C.fit(X_C_norm_subset, C2_norm.ravel())\n",
    "\n",
    "print(\"Optimal number of features : %d\" % rfecv_C.n_features_)\n",
    "\n",
    "# Get a mask, or integer index, of the features selected\n",
    "selected_features_C = rfecv_C.support_\n",
    "\n",
    "# Get a list of the feature names selected\n",
    "selected_feature_names = group_features_C[selected_features_C]\n",
    "print(selected_feature_names)\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (neg mean squared error)\")\n",
    "plt.plot(range(1, len(rfecv_C.grid_scores_) + 1), rfecv_C.grid_scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
