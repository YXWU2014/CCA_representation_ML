{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82034ec8",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [4]</a>'.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3704069",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T16:46:29.092036Z",
     "iopub.status.busy": "2023-11-15T16:46:29.090884Z",
     "iopub.status.idle": "2023-11-15T16:46:29.236644Z",
     "shell.execute_reply": "2023-11-15T16:46:29.235219Z"
    },
    "papermill": {
     "duration": 0.205981,
     "end_time": "2023-11-15T16:46:29.240766",
     "exception": false,
     "start_time": "2023-11-15T16:46:29.034785",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "notebook_fname = \"NN_full_v3_BO_1\"\n",
    "data_path = \"../01_Dataset_Cleaned/\"\n",
    "model_path = \"../04_Model_Saved/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8644607e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T16:46:29.366068Z",
     "iopub.status.busy": "2023-11-15T16:46:29.364768Z",
     "iopub.status.idle": "2023-11-15T16:46:29.372917Z",
     "shell.execute_reply": "2023-11-15T16:46:29.371046Z"
    },
    "papermill": {
     "duration": 0.052556,
     "end_time": "2023-11-15T16:46:29.376218",
     "exception": false,
     "start_time": "2023-11-15T16:46:29.323662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Parameters\n",
    "# notebook_fname = \"NN_full_v3_BO_0\"\n",
    "# data_path = '../01_Dataset_Cleaned/'\n",
    "# model_path = '../04_Model_Saved/'\n",
    "\n",
    "# # Development mode: automatic reloading of modules\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca362c8",
   "metadata": {
    "papermill": {
     "duration": 0.089279,
     "end_time": "2023-11-15T16:46:29.605793",
     "exception": false,
     "start_time": "2023-11-15T16:46:29.516514",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# main notebook to train and evaluate the machine learning model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59ccd0ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T16:46:29.726627Z",
     "iopub.status.busy": "2023-11-15T16:46:29.726073Z",
     "iopub.status.idle": "2023-11-15T16:46:29.733956Z",
     "shell.execute_reply": "2023-11-15T16:46:29.732880Z"
    },
    "papermill": {
     "duration": 0.041533,
     "end_time": "2023-11-15T16:46:29.738970",
     "exception": false,
     "start_time": "2023-11-15T16:46:29.697437",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /nethome/home3/yuxiang.wu/CCA_CALPHAD_SSS_ML/CCA_representation_ML/05_Model_BO_batch\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "print(\"Current directory:\", current_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c71027",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb19a2ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T16:46:29.938475Z",
     "iopub.status.busy": "2023-11-15T16:46:29.935121Z",
     "iopub.status.idle": "2023-11-15T16:46:32.101341Z",
     "shell.execute_reply": "2023-11-15T16:46:32.100378Z"
    },
    "papermill": {
     "duration": 2.315982,
     "end_time": "2023-11-15T16:46:32.104684",
     "exception": true,
     "start_time": "2023-11-15T16:46:29.788702",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'GPyOpt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Parallel, delayed\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmultiprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pool\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mGPyOpt\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'GPyOpt'"
     ]
    }
   ],
   "source": [
    "# basic machine learning libaries/Bayesian Optimization/Parellisation\n",
    "import importlib\n",
    "from tabulate import tabulate\n",
    "import concurrent.futures\n",
    "from joblib import Parallel, delayed\n",
    "from multiprocessing import Pool\n",
    "import GPyOpt\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import warnings\n",
    "from sklearn.model_selection import RepeatedKFold, RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers import Dense, Concatenate, Dropout, BatchNormalization\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "# Data processing and plotting\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib\n",
    "matplotlib.rcParams['font.family'] = 'FreeSans'\n",
    "\n",
    "\n",
    "# Miscellaneous\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Turn off TensorFlow warnings\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "tf.autograph.set_verbosity(0)\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "print(f'cpu_count: {os.cpu_count()}')\n",
    "\n",
    "# Data Path\n",
    "if os.path.isfile(data_path+'LiteratureDataset_Corrosion_YW_v3_processed.xlsx'):\n",
    "    print(f\"Folder '{os.path.abspath(data_path)}' found.\")\n",
    "else:\n",
    "    print(f\"Warning: File '{data_path}' not found!\")\n",
    "\n",
    "# Setting up multiple path for parallel Bayesian Optimization\n",
    "num_str = notebook_fname.split(\"_\")[-1]  # get the last string after \"_\"\n",
    "\n",
    "try:\n",
    "    bo_ens_num = int(num_str)\n",
    "    print(f\"bo_ens_num: {bo_ens_num}\")\n",
    "except ValueError:\n",
    "    # if the string can't be converted to an integer, keep it as a string\n",
    "    bo_ens_num = int(123)\n",
    "    print(f\"book master - bo_ens_num: {bo_ens_num}\")\n",
    "\n",
    "# model path\n",
    "model_path_bo = f'{model_path}{notebook_fname}/'\n",
    "\n",
    "if not os.path.exists(model_path_bo):\n",
    "    os.makedirs(model_path_bo)\n",
    "    print(f\"Folder '{os.path.abspath(model_path_bo)}' created.\")\n",
    "else:\n",
    "    print(f\"Folder '{os.path.abspath(model_path_bo)}' already exists.\")\n",
    "\n",
    "# Cleaning up previous tensorflow sessions\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c983ee7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## functionality control\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42557176",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-24T09:03:34.588524Z",
     "iopub.status.busy": "2023-08-24T09:03:34.588145Z",
     "iopub.status.idle": "2023-08-24T09:03:34.591781Z",
     "shell.execute_reply": "2023-08-24T09:03:34.591333Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# functionality control of the notebook\n",
    "Flag_Preprocessing = True\n",
    "Flag_Training_try = False\n",
    "Flag_BO_search = True\n",
    "Flag_Training_BO_best = False\n",
    "Flag_Evaluation = False\n",
    "Flag_Prediction = False\n",
    "Flag_Explainer = False\n",
    "\n",
    "# Use GPU or not\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "print('not using GPU')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6b46096c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Import hardness and corrosion dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f16c0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-24T09:03:34.647107Z",
     "iopub.status.busy": "2023-08-24T09:03:34.646901Z",
     "iopub.status.idle": "2023-08-24T09:03:35.406220Z",
     "shell.execute_reply": "2023-08-24T09:03:35.405751Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Declare column names for the chemical composition dataframe, specific testing conditions, selected features, and output for Hardness and Corrosion datasets.\n",
    "compo_column = ['Fe', 'Cr', 'Ni', 'Mo', 'W', 'N', 'Nb', 'C', 'Si',\n",
    "                'Mn', 'Cu', 'Al', 'V', 'Ta', 'Ti', 'Co', 'Mg', 'Y', 'Zr', 'Hf']\n",
    "C_specific_testing_column = ['TestTemperature_C',\n",
    "                             'ChlorideIonConcentration', 'pH', 'ScanRate_mVs']\n",
    "specific_features_sel_column = ['delta_a', 'Tm', 'sigma_Tm',\n",
    "                                'Hmix', 'sigma_Hmix', 'sigma_elec_nega', 'VEC', 'sigma_VEC']\n",
    "H_output_column = ['converted HV']\n",
    "C_output_column = ['AvgPittingPotential_mV']\n",
    "\n",
    "# Load the Hardness and Corrosion datasets\n",
    "df_H = pd.read_excel(\n",
    "    data_path + 'LiteratureDataset_Hardness_YW_v3_processed.xlsx')\n",
    "df_C = pd.read_excel(\n",
    "    data_path + 'LiteratureDataset_Corrosion_YW_v3_processed.xlsx')\n",
    "\n",
    "# Partition the datasets into component composition, specific features, and output data\n",
    "df_H_compo, df_H_specific_testing, df_H_specific_features, df_H_output = df_H[compo_column], pd.DataFrame(), df_H[\n",
    "    specific_features_sel_column],  df_H[H_output_column]\n",
    "(df_C_compo, df_C_specific_testing, df_C_specific_features,\n",
    " df_C_output) = df_C[compo_column], df_C[C_specific_testing_column], df_C[specific_features_sel_column], df_C[C_output_column]\n",
    "\n",
    "df_H_compo_specific_features = pd.concat(\n",
    "    [df_H_compo, df_H_specific_features], axis=1)\n",
    "df_C_compo_specific_features = pd.concat(\n",
    "    [df_C_compo, df_C_specific_features], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e271add",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Data preprocessing\n",
    "\n",
    "**hardness network**\n",
    "\n",
    "- input (X1): df_H_compo\n",
    "- input (Y1): df_H_specific_testing\n",
    "- input (V1): df_H_specific_features\n",
    "- output(H1): df_H_output\n",
    "\n",
    "**corrosion network**\n",
    "\n",
    "- input (X2): df_C_compo\n",
    "- input (Z2): df_C_specific_testing\n",
    "- input (W2): df_C_specific_features\n",
    "- output(C2): df_C_output\n",
    "\n",
    "### Obtain the MinMaxScaler from normalisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68757d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-24T09:03:35.494010Z",
     "iopub.status.busy": "2023-08-24T09:03:35.493760Z",
     "iopub.status.idle": "2023-08-24T09:03:35.610020Z",
     "shell.execute_reply": "2023-08-24T09:03:35.609590Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if Flag_Preprocessing:\n",
    "\n",
    "    # Prepare data for NN\n",
    "    dfs = [df_H_compo, df_H_specific_testing, df_H_specific_features, df_H_output,\n",
    "           df_C_compo, df_C_specific_testing, df_C_specific_features, df_C_output]\n",
    "\n",
    "    # Convert DataFrames to numpy arrays\n",
    "    inputs_outputs = [np.asarray(df.values) for df in dfs]\n",
    "\n",
    "    # Define each variable\n",
    "    (X1, Y1, V1, H1,\n",
    "     X2, Z2, W2, C2) = inputs_outputs\n",
    "\n",
    "    # # Initialize MinMaxScalers for each data set\n",
    "    # scalers = {\n",
    "    #     \"compo\": MinMaxScaler(),\n",
    "    #     \"H_specific_testing\": MinMaxScaler(),\n",
    "    #     \"C_specific_testing\": MinMaxScaler(),\n",
    "    #     \"specific_features\": MinMaxScaler(),\n",
    "    #     \"H_output\": MinMaxScaler(),\n",
    "    #     \"C_output\": MinMaxScaler()\n",
    "    # }\n",
    "\n",
    "    # Initialize StandardScaler for each data set\n",
    "    scalers = {\n",
    "        \"compo\": StandardScaler(),\n",
    "        \"H_specific_testing\": StandardScaler(),\n",
    "        \"C_specific_testing\": StandardScaler(),\n",
    "        \"specific_features\": StandardScaler(),\n",
    "        \"H_output\": StandardScaler(),\n",
    "        \"C_output\": StandardScaler()\n",
    "    }\n",
    "\n",
    "    # Fit scalers to appropriate data\n",
    "    scalers[\"compo\"].fit(np.concatenate((X1, X2)))\n",
    "    if Y1.size != 0:  # if Y1 is empty\n",
    "        scalers[\"H_specific_testing\"].fit(Y1)\n",
    "    scalers[\"C_specific_testing\"].fit(Z2)\n",
    "    scalers[\"specific_features\"].fit(np.concatenate((V1, W2)))\n",
    "    scalers[\"H_output\"].fit(H1.reshape((-1, 1)))\n",
    "    scalers[\"C_output\"].fit(C2.reshape((-1, 1)))\n",
    "\n",
    "    # Save the scalers dictionary to a file using pickle\n",
    "    with open(data_path + 'scalers.pkl', 'wb') as f:\n",
    "        pickle.dump(scalers, f)\n",
    "\n",
    "    print(scalers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f3a6d3c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### RepeatedKFold train-test split\n",
    "\n",
    "- for hardness: `kfold_with_norm_H`\n",
    "\n",
    "  - `X1_train_KFold`, `X1_test_KFold`,\n",
    "  - (empty array)\n",
    "  - `V1_train_KFold`, `V1_test_KFold`,\n",
    "  - `H1_train_KFold`, `H1_test_KFold` as lists used in model training\n",
    "\n",
    "- for corrosion: `kfold_with_norm_C`\n",
    "  - `X2_train_KFold`, `X2_test_KFold`,\n",
    "  - `Z2_train_KFold`, `Z2_test_KFold`,\n",
    "  - `W2_train_KFold`, `W2_test_KFold`,\n",
    "  - `C2_train_KFold`, `C2_test_KFold` as lists used in model training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864212cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-24T09:03:35.716682Z",
     "iopub.status.busy": "2023-08-24T09:03:35.716316Z",
     "iopub.status.idle": "2023-08-24T09:03:35.744041Z",
     "shell.execute_reply": "2023-08-24T09:03:35.743622Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if Flag_Preprocessing:\n",
    "\n",
    "    from utils.preprocessing_kfold_norm import kfold_with_norm\n",
    "\n",
    "    n_splits, n_repeats, random_state = 6, 2, 42\n",
    "\n",
    "    # for hardness network\n",
    "    ([X1_train_KFold, Y1_train_KFold, V1_train_KFold, H1_train_KFold],\n",
    "     [X1_test_KFold, Y1_test_KFold, V1_test_KFold, H1_test_KFold],\n",
    "     [X1_train_norm_KFold, Y1_train_norm_KFold,\n",
    "         V1_train_norm_KFold, H1_train_norm_KFold],\n",
    "     [X1_test_norm_KFold, Y1_test_norm_KFold, V1_test_norm_KFold, H1_test_norm_KFold]) = kfold_with_norm(X1, Y1, V1, H1,\n",
    "                                                                                                         scalers[\"compo\"], scalers[\"H_specific_testing\"], scalers[\n",
    "                                                                                                             \"specific_features\"], scalers[\"H_output\"],\n",
    "                                                                                                         n_splits, n_repeats, random_state)\n",
    "\n",
    "    # for corrosion network\n",
    "    ([X2_train_KFold, Z2_train_KFold, W2_train_KFold, C2_train_KFold],\n",
    "     [X2_test_KFold, Z2_test_KFold, W2_test_KFold, C2_test_KFold],\n",
    "     [X2_train_norm_KFold, Z2_train_norm_KFold,\n",
    "         W2_train_norm_KFold, C2_train_norm_KFold],\n",
    "     [X2_test_norm_KFold, Z2_test_norm_KFold, W2_test_norm_KFold, C2_test_norm_KFold]) = kfold_with_norm(X2, Z2, W2, C2,\n",
    "                                                                                                         scalers[\"compo\"], scalers[\"C_specific_testing\"], scalers[\n",
    "                                                                                                             \"specific_features\"], scalers[\"C_output\"],\n",
    "                                                                                                         n_splits, n_repeats, random_state)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "90addeed",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Plot the histogram of train/test data for each split\n",
    "\n",
    "- Plotting hardness train/test datasets\n",
    "- Plotting corrosion train/test datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e51f4e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-24T09:03:35.861992Z",
     "iopub.status.busy": "2023-08-24T09:03:35.861766Z",
     "iopub.status.idle": "2023-08-24T09:03:35.864498Z",
     "shell.execute_reply": "2023-08-24T09:03:35.864090Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if Flag_Preprocessing:\n",
    "\n",
    "#     from utils.preprocessing_kfold_norm import plot_hist_kfold_with_norm\n",
    "\n",
    "#     # call the function to plot the histograms - hardness network\n",
    "#     plot_hist_kfold_with_norm(train_data=(X1_train_norm_KFold, Y1_train_norm_KFold, V1_train_norm_KFold, H1_train_norm_KFold),\n",
    "#                               test_data=(X1_test_norm_KFold, Y1_test_norm_KFold,\n",
    "#                                          V1_test_norm_KFold,  H1_test_norm_KFold),\n",
    "#                               # x_min=(0, 0, 0, 0),\n",
    "#                               # x_max=(0.2, 0, 1, 1),\n",
    "#                               x_min=(-3, -3, -3, -3),\n",
    "#                               x_max=(3,  3,  3,  3),\n",
    "#                               axs_title='Hardness network Train/Test Data',\n",
    "#                               n_splits=6, n_repeats=1, nrows=4)\n",
    "\n",
    "#     # call the function to plot the histograms - corrosion network\n",
    "#     plot_hist_kfold_with_norm(train_data=(X2_train_norm_KFold, Z2_train_norm_KFold, W2_train_norm_KFold, C2_train_norm_KFold),\n",
    "#                               test_data=(X2_test_norm_KFold,  Z2_test_norm_KFold,\n",
    "#                                          W2_test_norm_KFold, C2_test_norm_KFold),\n",
    "#                               # x_min=(0, 0, 0, 0),\n",
    "#                               # x_max=(0.2, 1, 1, 1),\n",
    "#                               x_min=(-3, -3, -3, -3),\n",
    "#                               x_max=(3,  3,  3,  3),\n",
    "#                               axs_title='Corrosion network Train/Test Data',\n",
    "#                               n_splits=6, n_repeats=1, nrows=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6dbe5b0e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## NN architecture, creation, evaluation:\n",
    "\n",
    "- shared feature NN: NNS\n",
    "- hardness NN: NNH\n",
    "- corrosion NN: NNC\n",
    "\n",
    "class: `MultiTaskNN`\n",
    "\n",
    "```\n",
    "         input2-->|\n",
    "                  |-->[NNH]-->output_H\n",
    "                  |\n",
    " input1-->[NNS]-->|\n",
    "                  |\n",
    "                  |-->[NNC]-->output_C\n",
    "         input3-->|\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa402a5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Using `MultiTaskNN` class for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fde7c0a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if Flag_Training_try:\n",
    "    from concurrent.futures import ThreadPoolExecutor\n",
    "    from utils.multitask_nn import MultiTaskNN, plot_R2_avg\n",
    "\n",
    "    def create_multitask_nn(shared_layers):\n",
    "        return MultiTaskNN(\n",
    "            NNS_num_nodes=128, NNS_num_layers=2,\n",
    "            NNH_num_nodes=64, NNH_num_layers=3,\n",
    "            NNC_num_nodes=64, NNC_num_layers=5,\n",
    "            mc_state=True, act='swish',\n",
    "            NNS_dropout=0.1, NNH_dropout=0.1, NNC_dropout=0.1,\n",
    "            loss_func=tf.keras.metrics.mean_squared_error,\n",
    "            learning_rate_H=0.0001, learning_rate_C=0.0001,\n",
    "            batch_size_H=32,\n",
    "            N_epochs_local=2,\n",
    "            total_epochs=100,\n",
    "            model_save_flag=False,\n",
    "            model_path_bo=model_path_bo,\n",
    "            share_initial_layers=shared_layers,\n",
    "            NNH_model_name='NNH_model_RepeatedKFold_{}',\n",
    "            NNC_model_name='NNC_model_RepeatedKFold_{}'\n",
    "        )\n",
    "\n",
    "    def evaluate_model(mt_nn, *args):\n",
    "        return mt_nn.evaluate_NN_full_model(*args)\n",
    "\n",
    "    # Create MultiTaskNN instances\n",
    "    mt_nn_shared = create_multitask_nn(shared_layers=True)\n",
    "    mt_nn_separate = create_multitask_nn(shared_layers=False)\n",
    "\n",
    "    # Inputs for `RepeatedKFold`\n",
    "    k_folds, n_CVrepeats = 6, 2\n",
    "\n",
    "    # Data for evaluation\n",
    "    data_args = (X1_train_norm_KFold, X1_test_norm_KFold, Y1_train_norm_KFold, Y1_test_norm_KFold,\n",
    "                 V1_train_norm_KFold, V1_test_norm_KFold, H1_train_norm_KFold, H1_test_norm_KFold,\n",
    "                 X2_train_norm_KFold, X2_test_norm_KFold, Z2_train_norm_KFold, Z2_test_norm_KFold,\n",
    "                 W2_train_norm_KFold, W2_test_norm_KFold, C2_train_norm_KFold, C2_test_norm_KFold)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Use ThreadPoolExecutor to parallelise\n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        future_shared = executor.submit(\n",
    "            evaluate_model, mt_nn_shared, *data_args, k_folds, n_CVrepeats, scalers)\n",
    "        future_separate = executor.submit(\n",
    "            evaluate_model, mt_nn_separate, *data_args, k_folds, n_CVrepeats, scalers)\n",
    "\n",
    "        results_shared = future_shared.result()\n",
    "        results_separate = future_separate.result()\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Time taken: {int(elapsed_time)} seconds\")\n",
    "\n",
    "    # Unpack results\n",
    "    (train_loss_H_shared, train_loss_C_shared,\n",
    "     val_loss_H_shared, val_loss_C_shared,\n",
    "     score_loss_H_shared, score_loss_C_shared,\n",
    "     score_r2_H_shared, score_r2_C_shared) = results_shared\n",
    "\n",
    "    (train_loss_H_separate, train_loss_C_separate,\n",
    "     val_loss_H_separate, val_loss_C_separate,\n",
    "     score_loss_H_separate, score_loss_C_separate,\n",
    "     score_r2_H_separate, score_r2_C_separate) = results_separate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2253909c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Training quality for NNH_model and NNC_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e078947f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-24T09:03:36.145553Z",
     "iopub.status.busy": "2023-08-24T09:03:36.145356Z",
     "iopub.status.idle": "2023-08-24T09:03:36.148384Z",
     "shell.execute_reply": "2023-08-24T09:03:36.147975Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if Flag_Training_try:\n",
    "    from utils.multitask_nn import plot_R2_avg\n",
    "    # training results by score_loss\n",
    "    print('-----')\n",
    "    # print([f'{x:.4f}' for x in score_loss_H_shared])\n",
    "    # print([f'{x:.4f}' for i in range(12) for x in val_loss_H_shared[i][-1]])\n",
    "    print(\n",
    "        f\"score_loss_H_shared: {np.mean(score_loss_H_shared):.4f} ({np.std(score_loss_H_shared):.4f})\")\n",
    "    print(\n",
    "        f\"score_loss_H_separate: {np.mean(score_loss_H_separate):.4f} ({np.std(score_loss_H_separate):.4f})\")\n",
    "\n",
    "    print('-----')\n",
    "    # print([f'{x:.4f}' for x in score_loss_C_shared])\n",
    "    # print([f'{x:.4f}' for i in range(12) for x in val_loss_C_shared[i][-1]])\n",
    "    print(\n",
    "        f\"score_loss_C_shared: {np.mean(score_loss_C_shared):.4f} ({np.std(score_loss_C_shared):.4f})\")\n",
    "    print(\n",
    "        f\"score_loss_C_separate: {np.mean(score_loss_C_separate):.4f} ({np.std(score_loss_C_separate):.4f})\")\n",
    "\n",
    "    # training results by score_r2\n",
    "    print('-----')\n",
    "    # print([f'{x:.4f}' for x in score_r2_H_shared])\n",
    "    print(\n",
    "        f\"score_r2_H_shared: {np.mean(score_r2_H_shared):.4f} ({np.std(score_r2_H_shared):.4f})\")\n",
    "    print(\n",
    "        f\"score_r2_H_separate: {np.mean(score_r2_H_separate):.4f} ({np.std(score_r2_H_separate):.4f})\")\n",
    "\n",
    "    print('-----')\n",
    "    # print([f'{x:.4f}' for x in score_r2_C_shared])\n",
    "    print(\n",
    "        f\"score_r2_C_shared: {np.mean(score_r2_C_shared):.4f} ({np.std(score_r2_C_shared):.4f})\")\n",
    "    print(\n",
    "        f\"score_r2_C_separate: {np.mean(score_r2_C_separate):.4f} ({np.std(score_r2_C_separate):.4f})\")\n",
    "\n",
    "    print('-----')\n",
    "    print(\n",
    "        f\"score_r2_HC_shared: {np.mean([score_r2_H_shared, score_r2_C_shared]):.4f} ({np.std([score_r2_H_shared, score_r2_C_shared]):.4f})\")\n",
    "    print(\n",
    "        f\"score_r2_HC_separate: {np.mean([score_r2_H_separate, score_r2_C_separate]):.4f} ({np.std([score_r2_H_separate, score_r2_C_separate]):.4f})\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f79553b1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Plot the loss history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0b8882",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-24T09:03:36.210777Z",
     "iopub.status.busy": "2023-08-24T09:03:36.210560Z",
     "iopub.status.idle": "2023-08-24T09:03:36.213139Z",
     "shell.execute_reply": "2023-08-24T09:03:36.212732Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if Flag_Training_try:\n",
    "    from utils.multitask_nn import plot_losses_avg\n",
    "    plot_losses_avg(model_path_bo, train_loss_H_shared, val_loss_H_shared,\n",
    "                    train_loss_C_shared, val_loss_C_shared,\n",
    "                    k_folds=6, n_CVrepeats=2, index=0, ymax=0.8, window_size=1,\n",
    "                    savefig=False, figname='NN_full_RepeatedKFold_loss_shared.pdf')\n",
    "    plot_losses_avg(model_path_bo, train_loss_H_separate, val_loss_H_separate,\n",
    "                    train_loss_C_separate, val_loss_C_separate,\n",
    "                    k_folds=6, n_CVrepeats=2, index=0, ymax=0.8, window_size=1,\n",
    "                    savefig=False, figname='NN_full_RepeatedKFold_loss_separate.pdf')\n",
    "\n",
    "    # plot the R2 comparison\n",
    "    H_list = [score_r2_H_shared, score_r2_H_separate]\n",
    "    C_list = [score_r2_C_shared, score_r2_C_separate]\n",
    "\n",
    "    plot_R2_avg(model_path_bo,\n",
    "                H_list, C_list,\n",
    "                ymin=0.5, ymax=0.85,\n",
    "                x_labels=['multi-task\\nlearning (MTL)', 'separate\\nlearning'],\n",
    "                savefig=False, figname='xNN_full_RepeatedKFold_R2_compare.pdf')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52ad6dfd",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Bayesian Hyperparameter Optimisation\n",
    "\n",
    "class: `BO_hyper_objective`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eced8c46",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Define the search domain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7652143",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-24T09:03:36.295674Z",
     "iopub.status.busy": "2023-08-24T09:03:36.295451Z",
     "iopub.status.idle": "2023-08-24T09:03:36.301365Z",
     "shell.execute_reply": "2023-08-24T09:03:36.300958Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if Flag_BO_search:\n",
    "    # Define the search space for hyperparameters\n",
    "\n",
    "    # loss_encoder == 0: mean_squared_error\n",
    "    # loss_encoder == 1: mean_absolute_error\n",
    "\n",
    "    # search_hyper_space = [{'name': 'NNS_num_nodes',   'type': 'discrete',  'domain': (64, 128)},  # 0: NNS_num_nodes\n",
    "    #                       {'name': 'NNS_num_layers',  'type': 'discrete',\n",
    "    #                        'domain': (1, 2, 3)},  # 1: NNS_num_layers\n",
    "    #                       {'name': 'NNH_num_nodes',   'type': 'discrete',\n",
    "    #                        'domain': (32, 64)},    # 2: NNH_num_nodes\n",
    "    #                       {'name': 'NNH_num_layers',  'type': 'discrete',\n",
    "    #                        'domain': (2, 3, 4, 5, 6)},        # 3: NNH_num_layers\n",
    "    #                       {'name': 'NNC_num_nodes',   'type': 'discrete',\n",
    "    #                        'domain': (32, 64)},    # 4: NNC_num_nodes\n",
    "    #                       {'name': 'NNC_num_layers',  'type': 'discrete',\n",
    "    #                        'domain': (2, 3, 4, 5, 6)},        # 5: NNC_num_layers\n",
    "    #                       {'name': 'learning_rate_H', 'type': 'discrete',\n",
    "    #                        'domain': (0.0001, 0.0003, 0.0005, 0.0007, 0.0009)},    # 9: learning_rate_H\n",
    "    #                       {'name': 'learning_rate_C', 'type': 'discrete',\n",
    "    #                        'domain': (0.0001, 0.0003, 0.0005, 0.0007, 0.0009)},    # 10: learning_rate_C\n",
    "    #                       {'name': 'batch_size_H',    'type': 'discrete',\n",
    "    #                        'domain': (16, 32, 64)},        # 11: batch_size_H\n",
    "    #                       {'name': 'N_epochs_local',  'type': 'discrete',\n",
    "    #                        'domain': (1, 2, 3, 4)}]  # 12: N_epochs_global\n",
    "\n",
    "    search_hyper_space = [{'name': 'NNS_num_nodes',   'type': 'continuous',\n",
    "                           'domain': (64, 128)},  # 0: NNS_num_nodes\n",
    "                          {'name': 'NNS_num_layers',  'type': 'continuous',\n",
    "                           'domain': (1, 3)},  # 1: NNS_num_layers\n",
    "                          {'name': 'NNH_num_nodes',   'type': 'continuous',\n",
    "                           'domain': (32, 64)},    # 2: NNH_num_nodes\n",
    "                          {'name': 'NNH_num_layers',  'type': 'continuous',\n",
    "                           'domain': (2, 6)},        # 3: NNH_num_layers\n",
    "                          {'name': 'NNC_num_nodes',   'type': 'continuous',\n",
    "                           'domain': (32, 64)},    # 4: NNC_num_nodes\n",
    "                          {'name': 'NNC_num_layers',  'type': 'continuous',\n",
    "                           'domain': (2, 6)},        # 5: NNC_num_layers\n",
    "                          {'name': 'learning_rate_H', 'type': 'continuous',\n",
    "                           'domain': (0.0001, 0.001)},    # 9: learning_rate_H\n",
    "                          {'name': 'learning_rate_C', 'type': 'continuous',\n",
    "                           'domain': (0.0001, 0.001)},    # 10: learning_rate_C\n",
    "                          {'name': 'batch_size_H',    'type': 'continuous',\n",
    "                           'domain': (16, 64)},        # 11: batch_size_H\n",
    "                          {'name': 'N_epochs_local',  'type': 'continuous',\n",
    "                           'domain': (1, 4)}]  # 12: N_epochs_global\n",
    "\n",
    "    fixed_hyper_space = [{'name': 'NNS_dropout',     'type': 'continuous',\n",
    "                          'domain': (0.1)},            # 6: NNS_dropout\n",
    "                         {'name': 'NNH_NNC_dropout', 'type': 'continuous',\n",
    "                          'domain': (0.1)},            # 7: NNH_NNC_dropout\n",
    "                         {'name': 'loss_encoder',      'type': 'discrete',\n",
    "                          'domain': (0)}]               # 8: loss_encoder\n",
    "\n",
    "    search_hyper_names = [entry['name'] for entry in search_hyper_space]\n",
    "    fixed_hyper_names = [entry['name'] for entry in fixed_hyper_space]\n",
    "    all_hyper_names = ['NNS_num_nodes', 'NNS_num_layers', 'NNH_num_nodes', 'NNH_num_layers', 'NNC_num_nodes', 'NNC_num_layers',\n",
    "                       'NNS_dropout', 'NNH_NNC_dropout',  'loss_encoder', 'learning_rate_H',  'learning_rate_C',   'batch_size_H', 'N_epochs_local']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067c51ea",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "round(37.500001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4575ca05",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### BO hyperparameter optimisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2b2917",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-24T09:03:36.348012Z",
     "iopub.status.busy": "2023-08-24T09:03:36.347790Z",
     "iopub.status.idle": "2023-08-24T09:03:36.354094Z",
     "shell.execute_reply": "2023-08-24T09:03:36.353690Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if Flag_BO_search:\n",
    "    from utils.BO_hyper_objective import BayesianOptimizationObjective\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Set up the optimizer\n",
    "    bo = None\n",
    "    bo_iteration = 0\n",
    "    np.random.seed(bo_ens_num)\n",
    "\n",
    "    # Configuration settings for the model being optimized\n",
    "    total_epochs = 500\n",
    "    n_initial_points, n_iterations = 1, 50\n",
    "\n",
    "    mc_state, act = True, 'relu'\n",
    "    model_save_flag = False\n",
    "    share_initial_layers = True\n",
    "    NNH_model_name, NNC_model_name = 'NNH_model_RepeatedKFold_BOsearch_{}', 'NNC_model_RepeatedKFold_BOsearch_{}'\n",
    "    k_folds, n_CVrepeats = 6, 2\n",
    "    (score_r2_HC_list, score_loss_HC_list,\n",
    "     score_r2_H_list, score_r2_C_list,\n",
    "     score_loss_H_list, score_loss_C_list) = [], [], [], [], [], []\n",
    "\n",
    "    # Instantiate the objective function class for Bayesian Optimization\n",
    "    bo_obj = BayesianOptimizationObjective(\n",
    "        bo_ens_num, model_path_bo, all_hyper_names, search_hyper_names)\n",
    "\n",
    "    # Set up and configure Bayesian Optimization using GPyOpt\n",
    "    bo = GPyOpt.methods.BayesianOptimization(f=lambda x: bo_obj.BO_NNS_NNH_NNC_objective(x, fixed_hyper_space, search_hyper_names, fixed_hyper_names,\n",
    "                                                                                         n_initial_points, n_iterations,\n",
    "                                                                                         mc_state, act,\n",
    "                                                                                         total_epochs,\n",
    "                                                                                         model_save_flag, model_path_bo,\n",
    "                                                                                         share_initial_layers,\n",
    "                                                                                         NNH_model_name, NNC_model_name,\n",
    "                                                                                         X1_train_norm_KFold, X1_test_norm_KFold, Y1_train_norm_KFold, Y1_test_norm_KFold, V1_train_norm_KFold, V1_test_norm_KFold, H1_train_norm_KFold, H1_test_norm_KFold,\n",
    "                                                                                         X2_train_norm_KFold, X2_test_norm_KFold, Z2_train_norm_KFold, Z2_test_norm_KFold, W2_train_norm_KFold, W2_test_norm_KFold, C2_train_norm_KFold, C2_test_norm_KFold,\n",
    "                                                                                         k_folds, n_CVrepeats, scalers,\n",
    "                                                                                         score_r2_HC_list, score_loss_HC_list,\n",
    "                                                                                         score_r2_H_list, score_r2_C_list,\n",
    "                                                                                         score_loss_H_list, score_loss_C_list)[0],  # Use only score_r2 as objective function\n",
    "                                             domain=search_hyper_space,\n",
    "                                             model_type='GP',\n",
    "                                             initial_design_numdata=n_initial_points,\n",
    "                                             init_design_type='random',\n",
    "                                             acquisition_type='EI',\n",
    "                                             acquisition_optimizer_type='lbfgs',\n",
    "                                             maximize=True)\n",
    "\n",
    "    # Start the optimization for the given number of iterations\n",
    "    bo.run_optimization(max_iter=n_iterations,\n",
    "                        # eps=0,\n",
    "                        )\n",
    "\n",
    "    # Print the best hyperparameters and their corresponding objective value\n",
    "    best_hyperparameters = \", \".join(\"{:.4f}\".format(h) for h in bo.x_opt)\n",
    "    print(\"Best hyperparameters: {}\".format(best_hyperparameters))\n",
    "    print(\"Best objective value:\", -bo.fx_opt)\n",
    "\n",
    "    # Calculate and display the elapsed time for the optimization\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(\"Time taken: \", int(elapsed_time), \"seconds\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81c282f9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Save the BO hypertable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81942f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-24T09:03:36.435217Z",
     "iopub.status.busy": "2023-08-24T09:03:36.435023Z",
     "iopub.status.idle": "2023-08-24T09:03:36.438358Z",
     "shell.execute_reply": "2023-08-24T09:03:36.437961Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if Flag_BO_search:\n",
    "\n",
    "    # update the hypertable\n",
    "    bo_obj.update_hypertable(fixed_hyper_space, bo,\n",
    "                             score_r2_HC_list, score_loss_HC_list,\n",
    "                             score_r2_H_list, score_r2_C_list, score_loss_H_list, score_loss_C_list)\n",
    "\n",
    "    # Save the sorted data to an Excel file\n",
    "    bo_obj.hypertable.to_excel(model_path_bo +\n",
    "                               f\"hypertable_{notebook_fname}.xlsx\", index=False)\n",
    "\n",
    "    # sort the list start from the best results\n",
    "    hypertable_sort = bo_obj.hypertable.sort_values(\n",
    "        by=['score_r2_HC'], ascending=False, ignore_index=True)\n",
    "\n",
    "    display(hypertable_sort.head())\n",
    "\n",
    "    # Save the sorted data to an Excel file\n",
    "    hypertable_sort.to_excel(model_path_bo +\n",
    "                             f\"hypertable_sort_{notebook_fname}.xlsx\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2b27e33",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Plotting the BO process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d13eda2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-24T09:03:36.514622Z",
     "iopub.status.busy": "2023-08-24T09:03:36.514404Z",
     "iopub.status.idle": "2023-08-24T09:03:36.516746Z",
     "shell.execute_reply": "2023-08-24T09:03:36.516359Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if Flag_BO_search:\n",
    "    # plot the convergence\n",
    "    bo.plot_convergence()\n",
    "\n",
    "    bo_obj.plot_best_r2_score()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8a68554",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Model training: using best BO hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4521ad46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-24T09:03:36.661145Z",
     "iopub.status.busy": "2023-08-24T09:03:36.660855Z",
     "iopub.status.idle": "2023-08-24T09:41:38.816072Z",
     "shell.execute_reply": "2023-08-24T09:41:38.815398Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if Flag_Training_BO_best:\n",
    "\n",
    "    import tensorflow as tf\n",
    "    import pandas as pd\n",
    "    from utils.multitask_nn import MultiTaskNN\n",
    "\n",
    "    def get_hyperparameter(param_name, hypertable):\n",
    "        return hypertable.loc[0, param_name]\n",
    "\n",
    "    def create_multi_task_nn(mc_state, act, total_epochs, share_initial_layers,\n",
    "                             NNH_model_name, NNC_model_name, hypertable):\n",
    "        # Define loss encoders\n",
    "        loss_encoders = {\n",
    "            0: tf.keras.metrics.mean_squared_error,\n",
    "            1: tf.keras.metrics.mean_absolute_error\n",
    "        }\n",
    "        loss_encoder = int(get_hyperparameter('loss_encoder', hypertable))\n",
    "        loss_func = loss_encoders.get(loss_encoder)\n",
    "        if not loss_func:\n",
    "            raise ValueError(f\"Invalid loss function '{loss_encoder}' \")\n",
    "\n",
    "        # Define parameters\n",
    "        params = {\n",
    "            'NNS_num_nodes': int(get_hyperparameter('NNS_num_nodes', hypertable)),\n",
    "            'NNS_num_layers': int(get_hyperparameter('NNS_num_layers', hypertable)),\n",
    "            'NNH_num_nodes': int(get_hyperparameter('NNH_num_nodes', hypertable)),\n",
    "            'NNH_num_layers': int(get_hyperparameter('NNH_num_layers', hypertable)),\n",
    "            'NNC_num_nodes': int(get_hyperparameter('NNC_num_nodes', hypertable)),\n",
    "            'NNC_num_layers': int(get_hyperparameter('NNC_num_layers', hypertable)),\n",
    "            'mc_state': mc_state,\n",
    "            'act': act,\n",
    "            'NNS_dropout': get_hyperparameter('NNS_dropout', hypertable),\n",
    "            'NNH_dropout': get_hyperparameter('NNH_NNC_dropout', hypertable),\n",
    "            'NNC_dropout': get_hyperparameter('NNH_NNC_dropout', hypertable),\n",
    "            'loss_func': loss_func,\n",
    "            'learning_rate_H': get_hyperparameter('learning_rate_H', hypertable),\n",
    "            'learning_rate_C': get_hyperparameter('learning_rate_C', hypertable),\n",
    "            'batch_size_H': int(get_hyperparameter('batch_size_H', hypertable)),\n",
    "            'N_epochs_local': int(get_hyperparameter('N_epochs_local', hypertable)),\n",
    "            'total_epochs': total_epochs,\n",
    "            'model_save_flag': True,\n",
    "            'model_path_bo': model_path_bo,\n",
    "            'share_initial_layers': share_initial_layers,\n",
    "            'NNH_model_name': NNH_model_name,\n",
    "            'NNC_model_name': NNC_model_name\n",
    "        }\n",
    "\n",
    "        return MultiTaskNN(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cdddbc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Parallelsed training for best BO hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74c726c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if Flag_Training_BO_best:\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # ---------- loading the best BO hyperparameters ----------\n",
    "    hypertable_sort = pd.read_excel(\n",
    "        model_path_bo + f\"hypertable_sort_{notebook_fname}.xlsx\")\n",
    "    hypertable_BObest = hypertable_sort.head(1)\n",
    "    display(hypertable_BObest)\n",
    "    k_folds, n_CVrepeats = 6, 2\n",
    "    total_epochs = 500\n",
    "\n",
    "    # ---------- creating models ----------\n",
    "    # BObest MODEL with MC dropout + shared layers for uncertainty quantification\n",
    "    mt_nn_BObest_mc_shared = create_multi_task_nn(mc_state=True, act='relu', total_epochs=total_epochs, share_initial_layers=True,\n",
    "                                                  NNH_model_name='NNH_model_RepeatedKFold_{}_mc_shared_{act}',\n",
    "                                                  NNC_model_name='NNC_model_RepeatedKFold_{}_mc_shared_{act}',\n",
    "                                                  hypertable=hypertable_BObest)\n",
    "\n",
    "    # BObest MODEL with MC dropout, but without shared layers to compare the architecture\n",
    "    mt_nn_BObest_mc_separate = create_multi_task_nn(mc_state=True, act='relu', total_epochs=total_epochs, share_initial_layers=False,\n",
    "                                                    NNH_model_name='NNH_model_RepeatedKFold_{}_mc_separate_{act}',\n",
    "                                                    NNC_model_name='NNC_model_RepeatedKFold_{}_mc_separate_{act}',\n",
    "                                                    hypertable=hypertable_BObest)\n",
    "\n",
    "    # BObest MODEL without MC dropout and remove engineered feature to show the performance degradation\n",
    "    mt_nn_BObest_mc_NoEng = create_multi_task_nn(mc_state=True, act='relu', total_epochs=total_epochs, share_initial_layers=True,\n",
    "                                                 NNH_model_name='NNH_model_RepeatedKFold_{}_mc_NoEng_{act}',\n",
    "                                                 NNC_model_name='NNC_model_RepeatedKFold_{}_mc_NoEng_{act}',\n",
    "                                                 hypertable=hypertable_BObest)\n",
    "\n",
    "    # Best model with only compositional input to simplify feature interaction analysis\n",
    "    mt_nn_BObest_compo_XAI = create_multi_task_nn(mc_state=False, act='swish', total_epochs=total_epochs, share_initial_layers=True,\n",
    "                                                  NNH_model_name='NNH_model_RepeatedKFold_{}_compo_XAI_{act}',\n",
    "                                                  NNC_model_name='NNC_model_RepeatedKFold_{}_compo_XAI_{act}',\n",
    "                                                  hypertable=hypertable_BObest)\n",
    "\n",
    "    # Best model with compo-engineered features to check the compo-engineered features interaction\n",
    "    mt_nn_BObest_compo_features_XAI = create_multi_task_nn(mc_state=False, act='swish', total_epochs=total_epochs, share_initial_layers=True,\n",
    "                                                           NNH_model_name='NNH_model_RepeatedKFold_{}_compo_features_XAI_{act}',\n",
    "                                                           NNC_model_name='NNC_model_RepeatedKFold_{}_compo_features_XAI_{act}',\n",
    "                                                           hypertable=hypertable_BObest)\n",
    "\n",
    "    # ---------- Helpful function to control inputs  ----------\n",
    "    # Function to create empty arrays for datasets\n",
    "    def create_empty_arrays(arr_list):\n",
    "        return [np.empty((arr.shape[0], 0)) for arr in arr_list]\n",
    "\n",
    "    # Helper function to evaluate the model\n",
    "    def evaluate_model(model):\n",
    "        return model.evaluate_NN_full_model(\n",
    "            X1_train_norm_KFold, X1_test_norm_KFold, Y1_train_norm_KFold, Y1_test_norm_KFold, V1_train_norm_KFold, V1_test_norm_KFold, H1_train_norm_KFold, H1_test_norm_KFold,\n",
    "            X2_train_norm_KFold, X2_test_norm_KFold, Z2_train_norm_KFold, Z2_test_norm_KFold, W2_train_norm_KFold, W2_test_norm_KFold, C2_train_norm_KFold, C2_test_norm_KFold,\n",
    "            k_folds, n_CVrepeats, scalers)\n",
    "\n",
    "    # Helper function to evaluate the model, but replace the testing and feature inputs to be empty\n",
    "\n",
    "    def evaluate_model_compo_testing(model):\n",
    "\n",
    "        datasets = [V1_train_norm_KFold, V1_test_norm_KFold,\n",
    "                    W2_train_norm_KFold, W2_test_norm_KFold]\n",
    "\n",
    "        V1_train_empty, V1_test_empty, \\\n",
    "            W2_train_empty, W2_test_empty = map(\n",
    "                create_empty_arrays, datasets)\n",
    "\n",
    "        return model.evaluate_NN_full_model(\n",
    "            X1_train_norm_KFold, X1_test_norm_KFold, Y1_train_norm_KFold, Y1_test_norm_KFold, V1_train_empty, V1_test_empty, H1_train_norm_KFold, H1_test_norm_KFold,\n",
    "            X2_train_norm_KFold, X2_test_norm_KFold, Z2_train_norm_KFold, Z2_test_norm_KFold, W2_train_empty, W2_test_empty, C2_train_norm_KFold, C2_test_norm_KFold,\n",
    "            k_folds, n_CVrepeats, scalers)\n",
    "\n",
    "    # Helper function to evaluate the model, but replace the testing and feature inputs to be empty\n",
    "    def evaluate_model_compoOnly(model):\n",
    "\n",
    "        datasets = [Y1_train_norm_KFold, Y1_test_norm_KFold, V1_train_norm_KFold, V1_test_norm_KFold,\n",
    "                    Z2_train_norm_KFold, Z2_test_norm_KFold, W2_train_norm_KFold, W2_test_norm_KFold]\n",
    "\n",
    "        Y1_train_empty, Y1_test_empty, V1_train_empty, V1_test_empty, \\\n",
    "            Z2_train_empty, Z2_test_empty, W2_train_empty, W2_test_empty = map(\n",
    "                create_empty_arrays, datasets)\n",
    "\n",
    "        return model.evaluate_NN_full_model(\n",
    "            X1_train_norm_KFold, X1_test_norm_KFold, Y1_train_empty, Y1_test_empty, V1_train_empty, V1_test_empty, H1_train_norm_KFold, H1_test_norm_KFold,\n",
    "            X2_train_norm_KFold, X2_test_norm_KFold, Z2_train_empty, Z2_test_empty, W2_train_empty, W2_test_empty, C2_train_norm_KFold, C2_test_norm_KFold,\n",
    "            k_folds, n_CVrepeats, scalers)\n",
    "\n",
    "    # Helper function to evaluate the model, but replace the testing and feature inputs to be empty\n",
    "\n",
    "    def evaluate_model_compo_features(model):\n",
    "\n",
    "        datasets = [Y1_train_norm_KFold, Y1_test_norm_KFold,\n",
    "                    Z2_train_norm_KFold, Z2_test_norm_KFold]\n",
    "\n",
    "        Y1_train_empty, Y1_test_empty, \\\n",
    "            Z2_train_empty, Z2_test_empty = map(\n",
    "                create_empty_arrays, datasets)\n",
    "\n",
    "        return model.evaluate_NN_full_model(\n",
    "            X1_train_norm_KFold, X1_test_norm_KFold, Y1_train_empty, Y1_test_empty, V1_train_norm_KFold, V1_test_norm_KFold, H1_train_norm_KFold, H1_test_norm_KFold,\n",
    "            X2_train_norm_KFold, X2_test_norm_KFold, Z2_train_empty, Z2_test_empty, W2_train_norm_KFold, W2_test_norm_KFold, C2_train_norm_KFold, C2_test_norm_KFold,\n",
    "            k_folds, n_CVrepeats, scalers)\n",
    "\n",
    "    # ---------- model training in parallel ----------\n",
    "    # Using ProcessPoolExecutor to run in parallel\n",
    "    futures_dict = {}\n",
    "    with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "        futures_dict['mc_shared'] = executor.submit(\n",
    "            evaluate_model, mt_nn_BObest_mc_shared)\n",
    "        futures_dict['mc_separate'] = executor.submit(\n",
    "            evaluate_model, mt_nn_BObest_mc_separate)\n",
    "        futures_dict['mc_NoEng'] = executor.submit(\n",
    "            evaluate_model_compo_testing, mt_nn_BObest_mc_NoEng)\n",
    "        futures_dict['compo_XAI'] = executor.submit(\n",
    "            evaluate_model_compoOnly, mt_nn_BObest_compo_XAI)\n",
    "        futures_dict['compo_features_XAI'] = executor.submit(\n",
    "            evaluate_model_compo_features, mt_nn_BObest_compo_features_XAI)\n",
    "\n",
    "    # Collecting results in the correct order\n",
    "    results = {}\n",
    "    for key in futures_dict:\n",
    "        results[key] = futures_dict[key].result()\n",
    "\n",
    "    # Unpack results as needed\n",
    "    (train_loss_H_mc_shared, train_loss_C_mc_shared,\n",
    "     val_loss_H_mc_shared, val_loss_C_mc_shared,\n",
    "     score_loss_H_mc_shared, score_loss_C_mc_shared,\n",
    "     score_r2_H_mc_shared, score_r2_C_mc_shared) = results['mc_shared']\n",
    "\n",
    "    (train_loss_H_mc_separate, train_loss_C_mc_separate,\n",
    "     val_loss_H_mc_separate, val_loss_C_mc_separate,\n",
    "     score_loss_H_mc_separate, score_loss_C_mc_separate,\n",
    "     score_r2_H_mc_separate, score_r2_C_mc_separate) = results['mc_separate']\n",
    "\n",
    "    (train_loss_H_mc_NoEng, train_loss_C_mc_NoEng,\n",
    "     val_loss_H_mc_NoEng, val_loss_C_mc_NoEng,\n",
    "     score_loss_H_mc_NoEng, score_loss_C_mc_NoEng,\n",
    "     score_r2_H_mc_NoEng, score_r2_C_mc_NoEng) = results['mc_NoEng']\n",
    "\n",
    "    (train_loss_H_compo_XAI, train_loss_C_compo_XAI,\n",
    "     val_loss_H_compo_XAI, val_loss_C_compo_XAI,\n",
    "     score_loss_H_compo_XAI, score_loss_C_compo_XAI,\n",
    "     score_r2_H_compo_XAI, score_r2_C_compo_XAI) = results['compo_XAI']\n",
    "\n",
    "    (train_loss_H_compo_features_XAI, train_loss_C_compo_features_XAI,\n",
    "     val_loss_H_compo_features_XAI, val_loss_C_compo_features_XAI,\n",
    "     score_loss_H_compo_features_XAI, score_loss_C_compo_features_XAI,\n",
    "     score_r2_H_compo_features_XAI, score_r2_C_compo_features_XAI) = results['compo_features_XAI']\n",
    "\n",
    "    # Save all the parameters as BestBO\n",
    "    # Save with pickle\n",
    "    with open(model_path_bo + 'BObest_training_results.pkl', 'wb') as file:\n",
    "        pickle.dump(results, file)\n",
    "\n",
    "    # Print elapsed time\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(\"Time taken: \", int(elapsed_time), \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e4f795",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-24T09:41:38.986102Z",
     "iopub.status.busy": "2023-08-24T09:41:38.985763Z",
     "iopub.status.idle": "2023-08-24T09:41:40.149490Z",
     "shell.execute_reply": "2023-08-24T09:41:40.148723Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if Flag_Training_BO_best:\n",
    "\n",
    "    from utils.multitask_nn import plot_losses_avg, plot_R2_avg\n",
    "    model_results = {\n",
    "        'mc_shared': (score_r2_H_mc_shared, score_r2_C_mc_shared),\n",
    "        'mc_separate': (score_r2_H_mc_separate, score_r2_C_mc_separate),\n",
    "        'mc_NoEng': (score_r2_H_mc_NoEng, score_r2_C_mc_NoEng),\n",
    "        'compo_XAI': (score_r2_H_compo_XAI, score_r2_C_compo_XAI),\n",
    "        'compo_features_XAI': (score_r2_H_compo_features_XAI, score_r2_C_compo_features_XAI)\n",
    "    }\n",
    "\n",
    "    for model_name, scores in model_results.items():\n",
    "        mean_score = np.mean(scores)\n",
    "        std_score = np.std(scores)\n",
    "        print(f\"score_r2_HC_{model_name}: {mean_score:.4f} ({std_score:.4f})\")\n",
    "\n",
    "    plot_losses_avg(model_path_bo, train_loss_H_mc_shared, val_loss_H_mc_shared,\n",
    "                    train_loss_C_mc_shared, val_loss_C_mc_shared,\n",
    "                    k_folds=6, n_CVrepeats=2, index=0, ymax=0.8, window_size=1,\n",
    "                    savefig=True, figname='NN_full_RepeatedKFold_loss_mc_shared.pdf')\n",
    "\n",
    "    plot_losses_avg(model_path_bo, train_loss_H_mc_separate, val_loss_H_mc_separate,\n",
    "                    train_loss_C_mc_separate, val_loss_C_mc_separate,\n",
    "                    k_folds=6, n_CVrepeats=2, index=0, ymax=0.8, window_size=1,\n",
    "                    savefig=True, figname='NN_full_RepeatedKFold_loss_Nomc_separate.pdf')\n",
    "\n",
    "    H_list = [score_r2_H_mc_shared,\n",
    "              score_r2_H_mc_separate, score_r2_H_mc_NoEng]\n",
    "    C_list = [score_r2_C_mc_shared,\n",
    "              score_r2_C_mc_separate, score_r2_C_mc_NoEng]\n",
    "\n",
    "    plot_R2_avg(model_path_bo,\n",
    "                H_list, C_list,\n",
    "                ymin=0.6, ymax=0.85,\n",
    "                x_labels=['multi-task\\nlearning (MTL)',\n",
    "                          'separate\\nlearning', 'MTL without\\nfeature\\nengineering'],\n",
    "                savefig=False, figname='xNN_full_RepeatedKFold_R2_compare.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4274e23",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if Flag_Training_BO_best:\n",
    "\n",
    "    plot_losses_avg(model_path_bo, train_loss_H_compo_XAI, val_loss_H_compo_XAI,\n",
    "                    train_loss_C_compo_XAI, val_loss_C_compo_XAI,\n",
    "                    k_folds=6, n_CVrepeats=2, index=0, ymax=0.8, window_size=1,\n",
    "                    savefig=False, figname='NA')\n",
    "\n",
    "    plot_losses_avg(model_path_bo, train_loss_H_compo_features_XAI, val_loss_H_compo_features_XAI,\n",
    "                    train_loss_C_compo_features_XAI, val_loss_C_compo_features_XAI,\n",
    "                    k_folds=6, n_CVrepeats=2, index=0, ymax=0.8, window_size=1,\n",
    "                    savefig=False, figname='NA')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "30f40cf0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Evaluate the training results\n",
    "\n",
    "- Show the model.h5 files in this directory\n",
    "- Load scalers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c176d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-24T09:41:40.370949Z",
     "iopub.status.busy": "2023-08-24T09:41:40.370588Z",
     "iopub.status.idle": "2023-08-24T09:41:40.973591Z",
     "shell.execute_reply": "2023-08-24T09:41:40.973081Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from utils.postprocessing_evalutation import display_saved_models\n",
    "\n",
    "print(os.path.abspath(model_path_bo))\n",
    "\n",
    "NNH_model_name = 'NNH_model_RepeatedKFold_{}_mc_shared_{act}.h5'\n",
    "NNC_model_name = 'NNC_model_RepeatedKFold_{}_mc_shared_{act}.h5'\n",
    "display_saved_models(model_path_bo, NNH_model_name,\n",
    "                     NNC_model_name,  act='relu')\n",
    "\n",
    "NNH_model_name = 'NNH_model_RepeatedKFold_{}_mc_separate_{act}.h5'\n",
    "NNC_model_name = 'NNC_model_RepeatedKFold_{}_mc_separate_{act}.h5'\n",
    "display_saved_models(model_path_bo, NNH_model_name,\n",
    "                     NNC_model_name,  act='relu')\n",
    "\n",
    "NNH_model_name = 'NNH_model_RepeatedKFold_{}_mc_NoEng_{act}.h5'\n",
    "NNC_model_name = 'NNC_model_RepeatedKFold_{}_mc_NoEng_{act}.h5'\n",
    "display_saved_models(model_path_bo, NNH_model_name,\n",
    "                     NNC_model_name,  act='relu')\n",
    "\n",
    "NNH_model_name = 'NNH_model_RepeatedKFold_{}_compo_XAI_{act}.h5'\n",
    "NNC_model_name = 'NNC_model_RepeatedKFold_{}_compo_XAI_{act}.h5'\n",
    "display_saved_models(model_path_bo, NNH_model_name,\n",
    "                     NNC_model_name,  act='swish')\n",
    "\n",
    "NNH_model_name = 'NNH_model_RepeatedKFold_{}_compo_features_XAI_{act}.h5'\n",
    "NNC_model_name = 'NNC_model_RepeatedKFold_{}_compo_features_XAI_{act}.h5'\n",
    "display_saved_models(model_path_bo, NNH_model_name,\n",
    "                     NNC_model_name,  act='swish')\n",
    "\n",
    "# Load the scalers dictionary from a file using pickle\n",
    "with open(data_path + 'scalers.pkl', 'rb') as f:\n",
    "    scalers = pickle.load(f)\n",
    "    print(\"\")\n",
    "print(scalers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d4067b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Plot true vs predictions (test data) for each RepeatedKFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc24efe6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if Flag_Evaluation:\n",
    "    from utils.postprocessing_evalutation import prepare_data_for_eval, predict_bootstrap, plot_test_true_vs_pred\n",
    "    import concurrent.futures\n",
    "\n",
    "    # Define a common function for the repeated processes\n",
    "\n",
    "    def evaluate_model(model_suffix, iscompo_testing, iscompoOnly, iscompo_features, mc_repeat=1, plot_flag=False):\n",
    "        NNH_model_name = f'NNH_model_RepeatedKFold_{{}}_{model_suffix}.h5'\n",
    "        NNC_model_name = f'NNC_model_RepeatedKFold_{{}}_{model_suffix}.h5'\n",
    "        k_folds, n_CVrepeats = 6, 2\n",
    "\n",
    "        # Prepare data for evaluation\n",
    "        (X1_test_list, Y1_test_list, V1_test_list,\n",
    "         X2_test_list, Z2_test_list, W2_test_list,\n",
    "         X1_list, Y1_list, V1_list,\n",
    "         X2_list, Z2_list, W2_list) = prepare_data_for_eval(\n",
    "            X1, Y1, V1, X2, Z2, W2,\n",
    "            X1_test_KFold, Y1_test_KFold, V1_test_KFold,\n",
    "            X2_test_KFold, Z2_test_KFold, W2_test_KFold,\n",
    "            k_folds, n_CVrepeats,\n",
    "            iscompo_testing, iscompoOnly, iscompo_features)\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            future1 = executor.submit(predict_bootstrap, model_path_bo, NNH_model_name,\n",
    "                                      X1_test_list, Y1_test_list, V1_test_list,\n",
    "                                      k_folds, n_CVrepeats, mc_repeat,\n",
    "                                      scalers[\"compo\"], scalers[\"H_specific_testing\"],\n",
    "                                      scalers[\"specific_features\"], scalers[\"H_output\"])\n",
    "\n",
    "            future2 = executor.submit(predict_bootstrap, model_path_bo, NNC_model_name,\n",
    "                                      X2_test_list, Z2_test_list, W2_test_list,\n",
    "                                      k_folds, n_CVrepeats, mc_repeat,\n",
    "                                      scalers[\"compo\"], scalers[\"C_specific_testing\"],\n",
    "                                      scalers[\"specific_features\"], scalers[\"C_output\"])\n",
    "\n",
    "        H1_test_pred_X1_stack, H1_test_pred_X1_mean, H1_test_pred_X1_std = future1.result()\n",
    "        C2_test_pred_X2_stack, C2_test_pred_X2_mean, C2_test_pred_X2_std = future2.result()\n",
    "\n",
    "        plt_score_r2_H = plot_test_true_vs_pred(k_folds, n_CVrepeats,\n",
    "                                                H1_test_KFold, H1_test_pred_X1_mean, H1_test_pred_X1_std,\n",
    "                                                [0, 1200], 'NNH', 'firebrick', model_path_bo, plot_flag,\n",
    "                                                figname=f'NNH_RepeatedKFold_True_Prediction_testdata_{model_suffix}')\n",
    "\n",
    "        plt_score_r2_C = plot_test_true_vs_pred(k_folds, n_CVrepeats,\n",
    "                                                C2_test_KFold, C2_test_pred_X2_mean, C2_test_pred_X2_std,\n",
    "                                                [-800, 1200], 'NNC', 'steelblue', model_path_bo, plot_flag,\n",
    "                                                figname=f'NNC_RepeatedKFold_True_Prediction_testdata_{model_suffix}')\n",
    "\n",
    "        return plt_score_r2_H, plt_score_r2_C\n",
    "\n",
    "    # Define different configurations for evaluation\n",
    "    configurations = [\n",
    "        (\"mc_shared_relu\", False, False, False, 50, False),\n",
    "        (\"mc_separate_relu\", False, False, False, 50, False),\n",
    "        (\"mc_NoEng_relu\", True, False, False, 50, False)\n",
    "    ]\n",
    "\n",
    "    # Execute evaluation for each configuration\n",
    "    results = {}\n",
    "    for model_suffix, iscomp_testing, iscompOnly, iscomp_features, mc_repeats, plot_flag in configurations:\n",
    "        results[f'plt_score_r2_H_{model_suffix}'], results[f'plt_score_r2_C_{model_suffix}'] = evaluate_model(\n",
    "            model_suffix, iscomp_testing, iscompOnly, iscomp_features, mc_repeats, plot_flag)\n",
    "\n",
    "    # results now contains all your plot variables like plt_score_r2_H_Nomc_separate etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9333ad",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if Flag_Evaluation:\n",
    "    from utils.multitask_nn import plot_R2_avg\n",
    "\n",
    "    H_list = [results['plt_score_r2_H_mc_shared_relu'],\n",
    "              results['plt_score_r2_H_mc_separate_relu'], results['plt_score_r2_H_mc_NoEng_relu']]\n",
    "    C_list = [results['plt_score_r2_C_mc_shared_relu'],\n",
    "              results['plt_score_r2_C_mc_separate_relu'], results['plt_score_r2_C_mc_NoEng_relu']]\n",
    "\n",
    "    plot_R2_avg(model_path_bo,\n",
    "                H_list, C_list,\n",
    "                ymin=0.6, ymax=0.85,\n",
    "                x_labels=['multi-task\\nlearning (MTL)',\n",
    "                          'separate\\nlearning', 'MTL without\\nfeature\\nengineering'],\n",
    "                savefig=True, figname='NN_full_RepeatedKFold_R2_compare.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac18dd98",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if Flag_Evaluation:\n",
    "    print([f'{x:.2f}' for x in score_r2_H_mc_shared])\n",
    "    print([f'{x:.2f}' for x in results['plt_score_r2_H_mc_shared_relu']])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4a33de6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Plot true vs predictions (full dataset) including all model ensembles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf8e6d5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if Flag_Evaluation:\n",
    "    from utils.postprocessing_evalutation import predict_bootstrap, plot_full_true_vs_pred\n",
    "\n",
    "    def batchplot_True_Prediction_full(model_suffix, iscompo_testing, iscompoOnly, iscompo_features):\n",
    "        NNH_model_name = f'NNH_model_RepeatedKFold_{{}}{model_suffix}.h5'\n",
    "        NNC_model_name = f'NNC_model_RepeatedKFold_{{}}{model_suffix}.h5'\n",
    "        k_folds, n_CVrepeats, mc_repeat = 6, 2, 50\n",
    "\n",
    "        # Unpack all lists into variables using a function we assume you have defined elsewhere\n",
    "        (X1_test_list, Y1_test_list, V1_test_list,\n",
    "         X2_test_list, Z2_test_list, W2_test_list,\n",
    "         X1_list, Y1_list, V1_list,\n",
    "         X2_list, Z2_list, W2_list) = prepare_data_for_eval(\n",
    "            X1, Y1, V1, X2, Z2, W2,\n",
    "            X1_test_KFold, Y1_test_KFold, V1_test_KFold,\n",
    "            X2_test_KFold, Z2_test_KFold, W2_test_KFold,\n",
    "            k_folds, n_CVrepeats,\n",
    "            iscompo_testing, iscompoOnly, iscompo_features)\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            future1 = executor.submit(predict_bootstrap, model_path_bo, NNH_model_name,\n",
    "                                      X1_list, Y1_list, V1_list,\n",
    "                                      k_folds, n_CVrepeats, mc_repeat,\n",
    "                                      scalers[\"compo\"], scalers[\"H_specific_testing\"], scalers[\"specific_features\"], scalers[\"H_output\"])\n",
    "            future2 = executor.submit(predict_bootstrap, model_path_bo, NNC_model_name,\n",
    "                                      X2_list, Z2_list, W2_list,\n",
    "                                      k_folds, n_CVrepeats, mc_repeat,\n",
    "                                      scalers[\"compo\"], scalers[\"C_specific_testing\"], scalers[\"specific_features\"], scalers[\"C_output\"])\n",
    "\n",
    "        H1_pred_X1_stack, H1_pred_X1_mean, H1_pred_X1_std = future1.result()\n",
    "        C2_pred_X2_stack, C2_pred_X2_mean, C2_pred_X2_std = future2.result()\n",
    "\n",
    "        figname = f'NN_full_RepeatedKFold_True_Prediction_full{model_suffix}'\n",
    "        plot_full_true_vs_pred([H1, C2], [H1_pred_X1_stack, C2_pred_X2_stack],\n",
    "                               model_path_bo, lims=[[0, 1200], [-800, 1200]], figname=figname)\n",
    "\n",
    "    model_suffix_config = {\n",
    "        '_mc_shared_relu': (False, False, False),\n",
    "        # '_mc_separate_relu': (False, False, False),\n",
    "        # '_mc_NoEng_relu': (True, False, False),\n",
    "        '_compo_XAI_swish': (False, True, False),\n",
    "        '_compo_features_XAI_swish': (False, False, True),\n",
    "    }\n",
    "\n",
    "    for suffix, config in model_suffix_config.items():\n",
    "        batchplot_True_Prediction_full(suffix, *config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4740143d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Predict based on new data inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24430d38",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Coordinates for PVD alloy representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac17bbd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-24T09:42:09.266493Z",
     "iopub.status.busy": "2023-08-24T09:42:09.265952Z",
     "iopub.status.idle": "2023-08-24T09:42:09.290678Z",
     "shell.execute_reply": "2023-08-24T09:42:09.290258Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if Flag_Prediction:\n",
    "    # load the wafer-coordinates\n",
    "    df_PVD_x_y = pd.read_excel(data_path + 'PVD_x_y.xlsx')\n",
    "    coord_x = df_PVD_x_y[\"x\"].to_numpy(dtype=float)\n",
    "    coord_y = df_PVD_x_y[\"y\"].to_numpy(dtype=float)\n",
    "    index_PVD_x_y = df_PVD_x_y.index.values+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bfa287",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Make predictions for new alloys\n",
    "\n",
    "- NiCrCoVFe_KW99\n",
    "- NiCrMoTiFe_KW131\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecb5717",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if Flag_Prediction:\n",
    "    from utils.postprocessing_prediction import prediction_new_composition\n",
    "    from utils.postprocessing_prediction import plot_prediction_uncertainty, plot_prediction_uncertainty_AVG\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    # Specify the fname and compo\n",
    "    compositions = {\n",
    "        'NiCrCoVFe_KW99': ['Ni', 'Cr', 'Co', 'V', 'Fe'],\n",
    "        'NiCrMoTiFe_KW131': ['Ni', 'Cr', 'Mo', 'Ti', 'Fe']\n",
    "    }\n",
    "\n",
    "    # Specify the model\n",
    "    model_suffix_config = {\n",
    "        '_mc_shared_relu': (False, False, False),\n",
    "        # '_mc_separate_relu': (False, False, False),\n",
    "        # '_mc_NoEng_relu': (True, False, False),\n",
    "        '_compo_XAI_swish': (False, True, False),\n",
    "        # '_compo_features_XAI_swish': (False, False, True),\n",
    "    }\n",
    "\n",
    "    for model_suffix, config in model_suffix_config.items():\n",
    "        NNH_model_name = f'NNH_model_RepeatedKFold_{{}}{model_suffix}.h5'\n",
    "        NNC_model_name = f'NNC_model_RepeatedKFold_{{}}{model_suffix}.h5'\n",
    "        iscompo_testing, iscompoOnly, iscompo_features = config\n",
    "\n",
    "        # NNH_model_name = 'NNH_model_RepeatedKFold_{}_mc_shared_relu.h5'\n",
    "        # NNC_model_name = 'NNC_model_RepeatedKFold_{}_mc_shared_relu.h5'\n",
    "        # iscompo_testing, iscompoOnly, iscompo_features = False, False, False\n",
    "\n",
    "        for fname, compo in tqdm(compositions.items(), desc='Processing', total=len(compositions)):\n",
    "\n",
    "            print(fname, compo)\n",
    "\n",
    "            # predict for new composition\n",
    "            (compo_data, H_testing_data, C_testing_data, HC_feature_data,\n",
    "             H1_new_pred_mean, H1_new_pred_std, C2_new_pred_mean, C2_new_pred_std,\n",
    "             H1_new_pred_KFold_mean, H1_new_pred_KFold_std,\n",
    "             C2_new_pred_KFold_mean, C2_new_pred_KFold_std) = prediction_new_composition(fname, compo, data_path, model_path_bo,\n",
    "                                                                                         NNH_model_name, NNC_model_name,\n",
    "                                                                                         iscompo_testing, iscompoOnly, iscompo_features,\n",
    "                                                                                         scalers,\n",
    "                                                                                         specific_features_sel_column=['delta_a', 'Tm', 'sigma_Tm',\n",
    "                                                                                                                       'Hmix', 'sigma_Hmix', 'sigma_elec_nega', 'VEC', 'sigma_VEC'],\n",
    "                                                                                         C_testing=np.array(\n",
    "                                                                                             [25, 1, 7, 0.333]),\n",
    "                                                                                         k_folds=6, n_CVrepeats=2, mc_repeat=50)\n",
    "\n",
    "            # # plot NNH predictions\n",
    "            # plot_prediction_uncertainty(\n",
    "            #     model_path_bo, coord_x, coord_y, index_PVD_x_y, H1_new_pred_mean, H1_new_pred_std,\n",
    "            #     pred_label='Hardness', unc_label='Hardness uncertainty',\n",
    "            #     title='NNH_RepeatedKFold_prediction_uncertainty_eachFold_' + fname)\n",
    "\n",
    "            # # NNC predictions\n",
    "            # plot_prediction_uncertainty(\n",
    "            #     model_path_bo, coord_x, coord_y, index_PVD_x_y, C2_new_pred_mean, C2_new_pred_std,\n",
    "            #     pred_label='Pitting potential (mV)', unc_label='Pitting potential uncertainty (mV)',\n",
    "            #     title='NNC_RepeatedKFold_prediction_uncertainty_eachFold_' + fname)\n",
    "\n",
    "            # NNH_NNC_AVG predictions\n",
    "\n",
    "            figname = f'NNH_NNC_RepeatedKFold_prediction_uncertainty_AVG{model_suffix}'\n",
    "\n",
    "            plot_prediction_uncertainty_AVG(\n",
    "                model_path_bo, coord_x, coord_y, index_PVD_x_y,\n",
    "                H1_new_pred_KFold_mean, H1_new_pred_KFold_std,\n",
    "                C2_new_pred_KFold_mean, C2_new_pred_KFold_std,\n",
    "                title=figname + '_' + fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d861d1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Local Explanation by SHAP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e353aa",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Data import and processing for new data\n",
    "\n",
    "reference data generated via tc-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3502ef",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if Flag_Explainer:\n",
    "    from tqdm import tqdm\n",
    "    from utils.postprocessing_prediction import prediction_new_composition\n",
    "\n",
    "    def process_base_shap_data(model_suffix_config, base_fname, shap_fname):\n",
    "        # Specify the fname and compo\n",
    "        compositions = {\n",
    "            'NiCrCoVFe_KW99': ['Ni', 'Cr', 'Co', 'V', 'Fe'],\n",
    "            'NiCrMoTiFe_KW131': ['Ni', 'Cr', 'Mo', 'Ti', 'Fe'],\n",
    "            'Ni_FeCrMoTi_TC': ['Ni', 'Cr', 'Mo', 'Ti', 'Fe'],\n",
    "            'NiFe_CrMoTi_TC': ['Ni', 'Cr', 'Mo', 'Ti', 'Fe'],\n",
    "            'NiCrFe_MoTi_TC': ['Ni', 'Cr', 'Mo', 'Ti', 'Fe'],\n",
    "            'NiCrFeMo_Ti_TC': ['Ni', 'Cr', 'Mo', 'Ti', 'Fe'],\n",
    "            'NiMoTiFe_Cr_TC': ['Ni', 'Cr', 'Mo', 'Ti', 'Fe'],\n",
    "            'NiCrTiFe_Mo_TC': ['Ni', 'Cr', 'Mo', 'Ti', 'Fe'],\n",
    "            'CrNiMnFe': ['Cr', 'Ni', 'Mn', 'Fe'],\n",
    "            'CrNiMnFe_exp': ['Cr', 'Ni', 'Mn', 'Fe'],\n",
    "            'CrNiMnFe_tensile': ['Cr', 'Ni', 'Mn', 'Fe'],\n",
    "            'SiTiFeCoNi': ['Si', 'Ti', 'Fe', 'Co', 'Ni'],\n",
    "            'NiCrCoMnFe': ['Ni', 'Cr', 'Co', 'Mn', 'Fe'],\n",
    "            'NiCrCoVFe': ['Ni', 'Cr', 'Co', 'V', 'Fe']\n",
    "        }\n",
    "\n",
    "        model_suffix, (iscompo_testing, iscompoOnly, iscompo_features) = next(\n",
    "            iter(model_suffix_config.items()))\n",
    "        NNH_model_name = f'NNH_model_RepeatedKFold_{{}}{model_suffix}.h5'\n",
    "        NNC_model_name = f'NNC_model_RepeatedKFold_{{}}{model_suffix}.h5'\n",
    "\n",
    "        print('Model used: ',  NNH_model_name)\n",
    "\n",
    "        for fname, compo in tqdm(compositions.items(), desc='Processing', total=len(compositions)):\n",
    "            if fname in [base_fname, shap_fname]:\n",
    "                print(f'{fname} :', fname, compo)\n",
    "\n",
    "                (compo_data, H_testing_data, C_testing_data, HC_feature_data,\n",
    "                 H1_new_pred_mean, H1_new_pred_std, C2_new_pred_mean, C2_new_pred_std,\n",
    "                 H1_new_pred_KFold_mean, H1_new_pred_KFold_std,\n",
    "                 C2_new_pred_KFold_mean, C2_new_pred_KFold_std) = prediction_new_composition(\n",
    "                    fname, compo, data_path, model_path_bo,\n",
    "                    NNH_model_name, NNC_model_name,\n",
    "                    iscompo_testing, iscompoOnly, iscompo_features,\n",
    "                    scalers,\n",
    "                    specific_features_sel_column=['delta_a', 'Tm', 'sigma_Tm',\n",
    "                                                  'Hmix', 'sigma_Hmix', 'sigma_elec_nega', 'VEC', 'sigma_VEC'],\n",
    "                    C_testing=np.array([25, 1, 7, 0.333]),\n",
    "                    k_folds=6, n_CVrepeats=2, mc_repeat=1)\n",
    "\n",
    "                if fname == base_fname:\n",
    "                    X1_base_data, Y1_base_data, V1_base_data = compo_data, H_testing_data, HC_feature_data\n",
    "                    X2_base_data, Z2_base_data, W2_base_data = compo_data, C_testing_data, HC_feature_data\n",
    "                else:\n",
    "                    X1_shap_data, Y1_shap_data, V1_shap_data = compo_data, H_testing_data, HC_feature_data\n",
    "                    X2_shap_data, Z2_shap_data, W2_shap_data = compo_data, C_testing_data, HC_feature_data\n",
    "\n",
    "        return (X1_base_data, Y1_base_data, V1_base_data,\n",
    "                X2_base_data, Z2_base_data, W2_base_data,\n",
    "                X1_shap_data, Y1_shap_data, V1_shap_data,\n",
    "                X2_shap_data, Z2_shap_data, W2_shap_data)\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # Specify the model\n",
    "    model_suffix_config = {\n",
    "        # '_mc_shared_relu': (False, False, False),\n",
    "        # '_mc_separate_relu': (False, False, False),\n",
    "        # '_mc_NoEng_relu': (True, False, False),\n",
    "        '_compo_XAI_swish': (False, True, False),\n",
    "        # '_compo_features_XAI_swish': (False, False, True),\n",
    "    }\n",
    "\n",
    "    # Specify the feature conditions\n",
    "    base_fname = 'NiFe_CrMoTi_TC'\n",
    "    shap_fname = 'NiCrMoTiFe_KW131'\n",
    "\n",
    "    (X1_base_data, Y1_base_data, V1_base_data,\n",
    "     X2_base_data, Z2_base_data, W2_base_data,\n",
    "     X1_shap_data, Y1_shap_data, V1_shap_data,\n",
    "     X2_shap_data, Z2_shap_data, W2_shap_data) = process_base_shap_data(model_suffix_config, base_fname, shap_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d83695d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if Flag_Explainer:\n",
    "    print(X1_base_data.shape, Y1_base_data.shape, V1_base_data.shape)\n",
    "    print(X2_base_data.shape, Z2_base_data.shape, W2_base_data.shape)\n",
    "    print(X1_shap_data.shape, Y1_shap_data.shape, V1_shap_data.shape)\n",
    "    print(X2_shap_data.shape, Z2_shap_data.shape, W2_shap_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60042b68",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Feature attribution/interaction: quick check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f374eb2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if Flag_Explainer:\n",
    "    from utils.postprocessing_explainer import compute_shap_attributions_interactions\n",
    "\n",
    "    i_KFold = 0\n",
    "\n",
    "    model_suffix, (iscompo_testing, iscompoOnly, iscompo_features) = next(\n",
    "        iter(model_suffix_config.items()))\n",
    "    NNH_model_name = f'NNH_model_RepeatedKFold_{{}}{model_suffix}.h5'\n",
    "    NNC_model_name = f'NNC_model_RepeatedKFold_{{}}{model_suffix}.h5'\n",
    "\n",
    "    model = keras.models.load_model(os.path.join(\n",
    "        model_path_bo, NNH_model_name.format(i_KFold+1)))\n",
    "\n",
    "    X1_base_normalized = scalers[\"compo\"].transform(\n",
    "        X2_base_data).astype(np.float32)\n",
    "    X1_shap_normalized = scalers[\"compo\"].transform(\n",
    "        X2_shap_data).astype(np.float32)\n",
    "\n",
    "    shap_values, attributions_values, interactions_values = compute_shap_attributions_interactions(\n",
    "        model, X1_base_normalized, X1_shap_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6588e561",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if Flag_Explainer:\n",
    "    from utils.postprocessing_explainer import plot_shap_attributions_interactions, plot_interactions_heatmap\n",
    "\n",
    "    sample_index = 0\n",
    "    plot_shap_attributions_interactions(\n",
    "        shap_values, attributions_values, interactions_values, sample_index, compo_column)\n",
    "\n",
    "    sample_index = [49, 51, 53, 55]\n",
    "    sample_index = [x-1 for x in sample_index]\n",
    "    plot_interactions_heatmap(\n",
    "        model_path_bo, interactions_values, sample_index, compo_column,\n",
    "        cmap='RdBu_r', vmin=-0.05, vmax=0.05, figsize=4,\n",
    "        save_flag=False, figname='compo_XAI_swish_'+shap_fname+'_interaction')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8df9ec0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Feature attribution/interaction: full ensemble using `ModelExplainer`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4c036e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if Flag_Explainer:\n",
    "    from utils.postprocessing_explainer import ModelExplainer\n",
    "\n",
    "    def run_model_explainers(model_path_bo, model_suffix, scalers, data_lists, k_folds=6, n_CVrepeats=2, mc_repeat=1):\n",
    "\n",
    "        model_suffix, (iscompo_testing, iscompoOnly, iscompo_features) = next(\n",
    "            iter(model_suffix_config.items()))\n",
    "        NNH_model_name = f'NNH_model_RepeatedKFold_{{}}{model_suffix}.h5'\n",
    "        NNC_model_name = f'NNC_model_RepeatedKFold_{{}}{model_suffix}.h5'\n",
    "        print('Model used: ',  NNH_model_name)\n",
    "\n",
    "        repeat_val = k_folds * n_CVrepeats\n",
    "\n",
    "        X1_base_list, Y1_base_list, V1_base_list, \\\n",
    "            X2_base_list, Z2_base_list, W2_base_list, \\\n",
    "            X1_shap_list, Y1_shap_list, V1_shap_list, \\\n",
    "            X2_shap_list, Z2_shap_list, W2_shap_list = [\n",
    "                [data]*repeat_val for data in data_lists]\n",
    "\n",
    "        # Create ModelExplainer instances\n",
    "        NNH_predictor = ModelExplainer(model_path_bo, NNH_model_name, k_folds, n_CVrepeats, mc_repeat,\n",
    "                                       scalers[\"compo\"], scalers[\"H_specific_testing\"], scalers[\"specific_features\"])\n",
    "\n",
    "        NNC_predictor = ModelExplainer(model_path_bo, NNC_model_name, k_folds, n_CVrepeats, mc_repeat,\n",
    "                                       scalers[\"compo\"], scalers[\"C_specific_testing\"], scalers[\"specific_features\"])\n",
    "\n",
    "        # Concurrently run ModelExplainer instances\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            future1 = executor.submit(NNH_predictor.predict_norm_shap_bootstrap,\n",
    "                                      X1_base_list, Y1_base_list, V1_base_list,\n",
    "                                      X1_shap_list, Y1_shap_list, V1_shap_list)\n",
    "\n",
    "            future2 = executor.submit(NNC_predictor.predict_norm_shap_bootstrap,\n",
    "                                      X2_base_list, Z2_base_list, W2_base_list,\n",
    "                                      X2_shap_list, Z2_shap_list, W2_shap_list)\n",
    "\n",
    "            NNH_explainer_results = future1.result()\n",
    "            NNC_explainer_results = future2.result()\n",
    "\n",
    "        return NNH_explainer_results, NNC_explainer_results\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    data_lists = [X1_base_data, Y1_base_data, V1_base_data,\n",
    "                  X2_base_data, Z2_base_data, W2_base_data,\n",
    "                  X1_shap_data, Y1_shap_data, V1_shap_data,\n",
    "                  X2_shap_data, Z2_shap_data, W2_shap_data]\n",
    "\n",
    "    NNH_explainer_results, NNC_explainer_results = run_model_explainers(model_path_bo, model_suffix, scalers,\n",
    "                                                                        data_lists, k_folds=6, n_CVrepeats=2, mc_repeat=1)\n",
    "\n",
    "    (H1_pred_norm_X1_base_stack, _, _,\n",
    "     H1_pred_norm_X1_shap_stack, _, _,\n",
    "     H1_shap_norm_X1_stack,\n",
    "     H1_attributions_norm_X1_stack,\n",
    "     H1_interactions_norm_X1_stack) = NNH_explainer_results\n",
    "\n",
    "    (C2_pred_norm_X2_base_stack, _, _,\n",
    "     C2_pred_norm_X2_shap_stack, _, _,\n",
    "     C2_shap_norm_X2_stack,\n",
    "     C2_attributions_norm_X2_stack,\n",
    "     C2_interactions_norm_X2_stack) = NNC_explainer_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f57e46",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if Flag_Explainer:\n",
    "    # checking the full dataset - before averaging\n",
    "    i_KFold, i_sample = 0, 0\n",
    "    plot_shap_attributions_interactions(\n",
    "        H1_shap_norm_X1_stack[i_KFold],\n",
    "        H1_attributions_norm_X1_stack[i_KFold],\n",
    "        H1_interactions_norm_X1_stack[i_KFold],\n",
    "        i_sample,\n",
    "        compo_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5565037b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "##### Inverse transfrom feature attribution/interaction index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7d9b52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-24T09:43:35.824545Z",
     "iopub.status.busy": "2023-08-24T09:43:35.824254Z",
     "iopub.status.idle": "2023-08-24T09:43:35.830185Z",
     "shell.execute_reply": "2023-08-24T09:43:35.829684Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if Flag_Explainer:\n",
    "    from utils.postprocessing_explainer import process_inverse_norm_explainer_data\n",
    "\n",
    "    (H1_pred_norm_X1_base_KFold_mean,\n",
    "     H1_pred_norm_X1_shap_KFold_mean,\n",
    "     H1_shap_norm_X1_KFold_mean,\n",
    "     H1_attributions_norm_X1_KFold_mean,\n",
    "     H1_interactions_norm_X1_KFold_mean,\n",
    "     H1_pred_X1_base_KFold_mean,\n",
    "     H1_pred_X1_shap_KFold_mean,\n",
    "     H1_shap_X1_KFold_mean,\n",
    "     H1_attributions_X1_KFold_mean,\n",
    "     H1_interactions_X1_KFold_mean) = process_inverse_norm_explainer_data(H1_pred_norm_X1_base_stack, H1_pred_norm_X1_shap_stack,\n",
    "                                                                          H1_shap_norm_X1_stack,\n",
    "                                                                          H1_attributions_norm_X1_stack,\n",
    "                                                                          H1_interactions_norm_X1_stack,\n",
    "                                                                          scalers[\"H_output\"])\n",
    "\n",
    "    (C2_pred_norm_X2_base_KFold_mean,\n",
    "     C2_pred_norm_X2_shap_KFold_mean,\n",
    "     C2_shap_norm_X2_KFold_mean,\n",
    "     C2_attributions_norm_X2_KFold_mean,\n",
    "     C2_interactions_norm_X2_KFold_mean,\n",
    "     C2_pred_X2_base_KFold_mean,\n",
    "     C2_pred_X2_shap_KFold_mean,\n",
    "     C2_shap_X2_KFold_mean,\n",
    "     C2_attributions_X2_KFold_mean,\n",
    "     C2_interactions_X2_KFold_mean) = process_inverse_norm_explainer_data(C2_pred_norm_X2_base_stack, C2_pred_norm_X2_shap_stack,\n",
    "                                                                          C2_shap_norm_X2_stack,\n",
    "                                                                          C2_attributions_norm_X2_stack,\n",
    "                                                                          C2_interactions_norm_X2_stack,\n",
    "                                                                          scalers[\"C_output\"])\n",
    "\n",
    "    # print(H1_pred_X1_shap_KFold_mean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55df278",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if Flag_Explainer:\n",
    "\n",
    "    sample_index = 0\n",
    "    plot_shap_attributions_interactions(\n",
    "        H1_shap_X1_KFold_mean,\n",
    "        H1_attributions_X1_KFold_mean,\n",
    "        H1_interactions_X1_KFold_mean,\n",
    "        sample_index, compo_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7197430f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### Force plot of feature Shapley attributions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3888acc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if Flag_Explainer:\n",
    "    import shap\n",
    "    from utils.postprocessing_explainer import data_for_shap_force\n",
    "    from IPython.display import display\n",
    "\n",
    "    shap.initjs()\n",
    "    sample_indices = [49, 51, 53, 55]\n",
    "    sample_indices = [x-1 for x in sample_indices]\n",
    "\n",
    "    for sample_index in sample_indices:\n",
    "        A_baseline, B_shap_values, C_column_names = data_for_shap_force(X1_shap_data, Y1_shap_data, V1_shap_data,\n",
    "                                                                        compo_column, C_specific_testing_column, specific_features_sel_column,\n",
    "                                                                        H1_pred_X1_base_KFold_mean, H1_shap_X1_KFold_mean,\n",
    "                                                                        sample_index=[sample_index])\n",
    "        # shap.initjs()\n",
    "        shap_html = shap.force_plot(\n",
    "            A_baseline,\n",
    "            B_shap_values,\n",
    "            C_column_names,\n",
    "            link='identity',\n",
    "            matplotlib=False,\n",
    "            figsize=(5, 2.6),\n",
    "            text_rotation=45,\n",
    "            contribution_threshold=0.1)\n",
    "\n",
    "        display(shap_html)  # Display the plot in th\n",
    "        shap.save_html(\n",
    "            model_path_bo + f\"shap_force_{shap_fname}_NNH_{sample_index+1}.html\", shap_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8bebe9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### Interaction plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b2891e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if Flag_Explainer:\n",
    "    sample_index = [49, 51, 53, 55]\n",
    "    sample_index = [x-1 for x in sample_index]\n",
    "    plot_interactions_heatmap(\n",
    "        model_path_bo, H1_interactions_X1_KFold_mean, sample_index, compo_column,\n",
    "        cmap='RdBu_r', vmin=-30, vmax=30, figsize=4,\n",
    "        save_flag=True, figname='compo_XAI_swish_'+shap_fname+'_NNH_interaction')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c414b97c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Now use both compositional and engineered features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20ce6f6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if Flag_Explainer:\n",
    "#     # Specify the model\n",
    "#     model_suffix_config = {\n",
    "#         # '_mc_shared_relu': (False, False, False),\n",
    "#         # '_mc_separate_relu': (False, False, False),\n",
    "#         # '_mc_NoEng_relu': (True, False, False),\n",
    "#         # '_compo_XAI_swish': (False, True, False),\n",
    "#         '_compo_features_XAI_swish': (False, False, True),\n",
    "#     }\n",
    "\n",
    "#     # Specify the feature conditions\n",
    "#     base_fname = 'NiFe_CrMoTi_TC'\n",
    "#     shap_fname = 'NiCrMoTiFe_KW131'\n",
    "\n",
    "#     (X1_base_data, Y1_base_data, V1_base_data,\n",
    "#      X2_base_data, Z2_base_data, W2_base_data,\n",
    "#      X1_shap_data, Y1_shap_data, V1_shap_data,\n",
    "#      X2_shap_data, Z2_shap_data, W2_shap_data) = process_base_shap_data(model_suffix_config, base_fname, shap_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ec9a39",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if Flag_Explainer:\n",
    "#     data_lists = [X1_base_data, Y1_base_data, V1_base_data,\n",
    "#                   X2_base_data, Z2_base_data, W2_base_data,\n",
    "#                   X1_shap_data, Y1_shap_data, V1_shap_data,\n",
    "#                   X2_shap_data, Z2_shap_data, W2_shap_data]\n",
    "\n",
    "#     NNH_explainer_results, NNC_explainer_results = run_model_explainers(model_path_bo, model_suffix, scalers,\n",
    "#                                                                         data_lists, k_folds=6, n_CVrepeats=2, mc_repeat=1)\n",
    "\n",
    "#     (H1_pred_norm_X1_base_stack, _, _,\n",
    "#      H1_pred_norm_X1_shap_stack, _, _,\n",
    "#      H1_shap_norm_X1_stack,\n",
    "#      H1_attributions_norm_X1_stack,\n",
    "#      H1_interactions_norm_X1_stack) = NNH_explainer_results\n",
    "\n",
    "#     (C2_pred_norm_X2_base_stack, _, _,\n",
    "#      C2_pred_norm_X2_shap_stack, _, _,\n",
    "#      C2_shap_norm_X2_stack,\n",
    "#      C2_attributions_norm_X2_stack,\n",
    "#      C2_interactions_norm_X2_stack) = NNC_explainer_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7889d11",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if Flag_Explainer:\n",
    "#     (H1_pred_norm_X1_base_KFold_mean,\n",
    "#      H1_pred_norm_X1_shap_KFold_mean,\n",
    "#      H1_shap_norm_X1_KFold_mean,\n",
    "#      H1_attributions_norm_X1_KFold_mean,\n",
    "#      H1_interactions_norm_X1_KFold_mean,\n",
    "#      H1_pred_X1_base_KFold_mean,\n",
    "#      H1_pred_X1_shap_KFold_mean,\n",
    "#      H1_shap_X1_KFold_mean,\n",
    "#      H1_attributions_X1_KFold_mean,\n",
    "#      H1_interactions_X1_KFold_mean) = process_inverse_norm_explainer_data(H1_pred_norm_X1_base_stack, H1_pred_norm_X1_shap_stack,\n",
    "#                                                                           H1_shap_norm_X1_stack,\n",
    "#                                                                           H1_attributions_norm_X1_stack,\n",
    "#                                                                           H1_interactions_norm_X1_stack,\n",
    "#                                                                           scalers[\"H_output\"])\n",
    "\n",
    "#     (C2_pred_norm_X2_base_KFold_mean,\n",
    "#      C2_pred_norm_X2_shap_KFold_mean,\n",
    "#      C2_shap_norm_X2_KFold_mean,\n",
    "#      C2_attributions_norm_X2_KFold_mean,\n",
    "#      C2_interactions_norm_X2_KFold_mean,\n",
    "#      C2_pred_X2_base_KFold_mean,\n",
    "#      C2_pred_X2_shap_KFold_mean,\n",
    "#      C2_shap_X2_KFold_mean,\n",
    "#      C2_attributions_X2_KFold_mean,\n",
    "#      C2_interactions_X2_KFold_mean) = process_inverse_norm_explainer_data(C2_pred_norm_X2_base_stack, C2_pred_norm_X2_shap_stack,\n",
    "#                                                                           C2_shap_norm_X2_stack,\n",
    "#                                                                           C2_attributions_norm_X2_stack,\n",
    "#                                                                           C2_interactions_norm_X2_stack,\n",
    "#                                                                           scalers[\"C_output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9098492",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if Flag_Explainer:\n",
    "#     # sample_index = [49, 51, 53, 55]\n",
    "#     sample_index = [55]\n",
    "#     sample_index = [x-1 for x in sample_index]\n",
    "#     plot_interactions_heatmap(\n",
    "#         model_path_bo, H1_interactions_X1_KFold_mean, sample_index, compo_column +\n",
    "#         specific_features_sel_column,\n",
    "#         cmap='RdBu_r', vmin=-5, vmax=5, figsize=10,\n",
    "#         save_flag=False, figname='compo_XAI_swish_'+shap_fname+'_NNH_interaction')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 14.961962,
   "end_time": "2023-11-15T16:46:32.576912",
   "environment_variables": {},
   "exception": true,
   "input_path": "../03_Model_Train_Evaluate_Predict/NN_full_v3_BO_Train_Eval_Pred_master.ipynb",
   "output_path": "../04_Model_Saved/NN_full_v3_BO_1/NN_full_v3_BO_1.ipynb",
   "parameters": {
    "data_path": "../01_Dataset_Cleaned/",
    "model_path": "../04_Model_Saved/",
    "notebook_fname": "NN_full_v3_BO_1"
   },
   "start_time": "2023-11-15T16:46:17.614950",
   "version": "2.3.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "9bb1e37c70b1d2984796c5ff5b7458aa397f7ffd0c82b990a87aa0f6b2cdb3f9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}